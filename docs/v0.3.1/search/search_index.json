{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pravega Overview Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. A Pravega stream is a durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. Read Pravega Concepts for more details. Key Features Exactly-Once Semantics - Ensure that each event is delivered and processed exactly once, with exact ordering guarantees, despite failures in clients, servers or the network. Auto Scaling - Unlike systems with static partitioning, Pravega can automatically scale individual data streams to accommodate changes in data ingestion rate. Distributed Computing Primitive - Pravega is great for distributed computing; it can be used as a data storage mechanism, for messaging between processes and for other distributed computing services such as leader election. Write Efficiency - Pravega shrinks write latency to milliseconds, and seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications. Unlimited Retention - Ingest, process and retain data in streams forever. Use same paradigm to access both real-time and historical events stored in Pravega. Storage Efficiency - Use Pravega to build pipelines of data processing, combining batch, real-time and other applications without duplicating data for every step of the pipeline. Durability - Don't compromise between performance, durability and consistency. Pravega persists and protects data before the write operation is acknowledged to the client. Transaction Support - A developer uses a Pravega Transaction to ensure that a set of events are written to a stream atomically. Releases The latest pravega releases can be found on the Github Release project page. Quick Start Read Getting Started page for more information, and also visit sample-apps repo for more applications. Frequently Asked Questions You can find a list of frequently asked questions here . Running Pravega Pravega can be installed locally or in a distributed environment. The installation and deployment of Pravega is covered in the Running Pravega guide. Support Don\u2019t hesitate to ask! Contact the developers and community on the mailing lists or on slack if you need any help. Open an issue if you found a bug on Github Issues . Contributing Become one of the contributors! We thrive to build a welcoming and open community for anyone who wants to use the system or contribute to it. Here we describe how to contribute to Pravega! You can see the roadmap document here . About Pravega is 100% open source and community-driven. All components are available under Apache 2 License on GitHub.","title":"Overview"},{"location":"#pravega-overview","text":"Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. A Pravega stream is a durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. Read Pravega Concepts for more details.","title":"Pravega Overview"},{"location":"#key-features","text":"Exactly-Once Semantics - Ensure that each event is delivered and processed exactly once, with exact ordering guarantees, despite failures in clients, servers or the network. Auto Scaling - Unlike systems with static partitioning, Pravega can automatically scale individual data streams to accommodate changes in data ingestion rate. Distributed Computing Primitive - Pravega is great for distributed computing; it can be used as a data storage mechanism, for messaging between processes and for other distributed computing services such as leader election. Write Efficiency - Pravega shrinks write latency to milliseconds, and seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications. Unlimited Retention - Ingest, process and retain data in streams forever. Use same paradigm to access both real-time and historical events stored in Pravega. Storage Efficiency - Use Pravega to build pipelines of data processing, combining batch, real-time and other applications without duplicating data for every step of the pipeline. Durability - Don't compromise between performance, durability and consistency. Pravega persists and protects data before the write operation is acknowledged to the client. Transaction Support - A developer uses a Pravega Transaction to ensure that a set of events are written to a stream atomically.","title":"Key Features"},{"location":"#releases","text":"The latest pravega releases can be found on the Github Release project page.","title":"Releases"},{"location":"#quick-start","text":"Read Getting Started page for more information, and also visit sample-apps repo for more applications.","title":"Quick Start"},{"location":"#frequently-asked-questions","text":"You can find a list of frequently asked questions here .","title":"Frequently Asked Questions"},{"location":"#running-pravega","text":"Pravega can be installed locally or in a distributed environment. The installation and deployment of Pravega is covered in the Running Pravega guide.","title":"Running Pravega"},{"location":"#support","text":"Don\u2019t hesitate to ask! Contact the developers and community on the mailing lists or on slack if you need any help. Open an issue if you found a bug on Github Issues .","title":"Support"},{"location":"#contributing","text":"Become one of the contributors! We thrive to build a welcoming and open community for anyone who wants to use the system or contribute to it. Here we describe how to contribute to Pravega! You can see the roadmap document here .","title":"Contributing"},{"location":"#about","text":"Pravega is 100% open source and community-driven. All components are available under Apache 2 License on GitHub.","title":"About"},{"location":"basic-reader-and-writer/","text":"Working with Pravega: Basic Reader and Writer Lets examine how to build simple Pravega applications. The simplest kind of Pravega application uses a Pravega Reader to read from a Pravega Stream or a Pravega Writer that writes to a Pravega Stream. A simple example of both can be found in the Pravega Samples \"hello world\" app. These sample applications provide a very basic example of how a Java application could use the Pravega Java Client Library to access Pravega functionality. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts) before continuing reading this page. HelloWorldWriter The HelloWorldWriter application is a simple demonstration of using the EventStreamWriter to write an Event to Pravega. Taking a look first at the HelloWorldWriter example application, the key part of the code is in the run() method: public void run ( String routingKey , String message ) { StreamManager streamManager = StreamManager . create ( controllerURI ); final boolean scopeCreation = streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder () . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); final boolean streamCreation = streamManager . createStream ( scope , streamName , streamConfig ); try ( ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamWriter String writer = clientFactory . createEventWriter ( streamName , new JavaSerializer String (), EventWriterConfig . builder (). build ())) { System . out . format ( Writing message: %s with routing-key: %s to stream %s / %s %n , message , routingKey , scope , streamName ); final CompletableFuture Void writeFuture = writer . writeEvent ( routingKey , message ); } } The purpose of the run() method is to create a Stream (lines 2-9) and output the given Event to that Stream (lines 10-18). Creating a Stream and the StreamManager Interface A Stream is created in the context of a Scope; the Scope acts as a namespace mechanism so that different sets of Streams can be categorized for some purpose. For example, I might have a separate scope for each application. I might choose to create a set of Scopes, one for each department in my organization. In a multi-tenant environment, I might have a separate Scope per tenant. As a developer, I can choose whatever categorization scheme I need and use the Scope concept for organizing my Streams along that categorization scheme. Scopes and Streams are created and manipulated via the StreamManager Interface to the Pravega Controller. You need to have a URI to any of the Pravega Controller instances in your cluster in order to create a StreamManager object. This is shown in line 2. In the setup for the HelloWorld sample applications, the controllerURI is configured as a command line parameter when the sample application is launched. For the \"single node\" deployment of Pravega, the Controller is listening on localhost, port 9090. The StreamManager provides access to various control plane functions in Pravega related to Scopes and Streams: Method Parameters Discussion (static) create (URI controller) Given a URI to one of the Pravega Controller instances in the Pravega Cluster, create a Stream Manager object. createScope (String scopeName) Creates a Scope with the given name. Returns true if the Scope is created, returns false if the Scope already exists. You can call this method even if the Scope already exists, it won't harm anything. deleteScope (String scopeName) Deletes a Scope with the given name. Returns true if the scope was deleted, false otherwise. Note, if the Scope contains Streams, the deleteScope operation will fail with an exception. If you delete a nonexistent Scope, the method will succeed and return false. createStream (String scopeName, String streamName, StreamConfiguration config) Create a Stream within a given Scope. Note that both scope name and stream name are limited by the following pattern: [a-zA-Z0-9]+ (i.e. letters and numbers only, no punctuation) Note also: the Scope must exist, an exception is thrown if you create a Stream in a nonexistent scope. A StreamConfiguration is built using a builder pattern Returns true if the Stream is created, returns false if the Stream already exists. You can call this method even if the Stream already exists, it won't harm anything. updateStream (String scopeName, String streamName, StreamConfiguration config) Swap out the Stream's configuration. Note the Stream must already exist, an exception is thrown if you update a nonexistent stream. Returns true if the Stream was changed sealStream (String scopeName, String streamName) Prevent any further writes to a Stream Note the Stream must already exist, an exception is thrown if you seal a nonexistent stream. Returns true if the Stream is successfully sealed deleteStream (String scopeName, String streamName) Remove the Stream from Pravega and recover any resources used by that Stream Note the Stream must already exist, an exception is thrown if you delete a nonexistent stream. Returns true if the stream was deleted. After line 3 in the code is finished, we have established that the Scope exists, we can then go on and create the Stream in lines 5-8. The StreamManager needs 3 things to create a Stream, the Scope's name, the Stream's name and a StreamConfiguration. The most interesting task is to create the StreamConfiguration. Like many objects in Pravega, a Stream takes a configuration object that allows a developer to control various behaviors of the Stream. All configuration objects in Pravega use a builder pattern for construction. There are really two important configuration items related to streams: Retention Policy and Scaling Policy. Retention Policy allows the developer to control how long data is kept in a Stream before it is deleted. S/he can specify data should be kept for a certain period of time (ideal for situations like regulatory compliance that mandate certain retention periods) or to retain data until a certain number of bytes have been consumed. At the moment, Retention Policy is not completely implemented. By default, the RetentionPolicy is set as \"unlimited\" meaning, data will not be removed from the Stream. Scaling Policy is the way developers configure a Stream to take advantage Pravega's auto-scaling feature. In line 6, we use a fixed policy, meaning the Stream is configured with the given number of Stream Segments and that won't change. The other options are to scale by a given number of Events per second or a given number of Kilobytes per second. In these two policies, the developer specifies a target rate, a scaling factor and a minimum number of Segments. The target rate is straight forward, if ingest rate exceeds a certain number of Events or Kilobytes of data for a sustained period of time, Pravega will attempt to add new Stream Segments to the Stream. If the rate drops below that threshold for a sustained period of time, Pravega will attempt to merge adjacent Stream Segments. The scaling factor is a setting on the Scaling Policy that determines how many Stream Segments should be added when the target rate (of Events or Kilobytes) is exceeded. The minimum number of Segments is a factor that sets the minimum degree of read parallelism to be maintained; if this value is set at 3, for example, there will always be 3 Stream Segments available on the Stream. Currently, this property is effective only when the stream is created; at some point in the future, update stream will allow this factor to be used to change the minimum degree of read parallelism on an existing Stream. Once the StreamConfiguration object is created, creating the Stream is straight forward (line 8). After the Stream is created, we are all set to start writing Event(s) to the Stream. Writing an Event using EventWriter Applications use an EventStreamWriter object to write Events to a Stream. The key object to creating the EventStreamWriter is the ClientFactory. The ClientFactory is used to create Readers, Writers and other types of Pravega Client objects such as the State Synchronizer (see Working with Pravega: State Synchronizer ). Line 10 shows the creation of a ClientFactory. A ClientFactory is created in the context of a Scope, since all Readers, Writers and other Clients created by the ClientFactory are created in the context of that Scope. The ClientFactory also needs a URI to one of the Pravega Controllers, just like StreamManager. Because ClientFactory and the objects it creates consumes resources from Pravega, it is a good practice to create these objects in a try-with-resources statement. Because ClientFactory and the objects it creates all implement Autocloseable, the try-with-resources approach makes sure that regardless of how your application ends, the Pravega resources will be properly closed in the right order. Now that we have a ClientFactory, we can use it to create a Writer. There are several things a developer needs to know before s/he creates a Writer: What is the name of the Stream to write to? Note: the Scope has already been determined when the ClientFactory was created What Type of Event objects will be written to the Stream? What serializer will be used to convert an Event object to bytes? Recall that Pravega only knows about sequences of bytes, it does not really know anything about Java objects. Does the Writer need to be configured with any special behavior? In our example, lines 11-13 show all these decisions. This Writer writes to the Stream specified in the configuration of the HelloWorldWriter object itself (by default the stream is named \"helloStream\" in the \"examples\" Scope). The Writer processes Java String objects as Events and uses the built in Java serializer for Strings. The EventWriterConfig allows the developer to specify things like the number of attempts to retry a request before giving up and associated exponential back settings. Pravega takes care to retry requests in the case where connection failures or Pravega component outages may temporarily prevent a request from succeeding, so application logic doesn't need to be complicated with dealing with intermittent cluster failures. In our case, we took the default settings for EventWriterConfig in line 13. Now we can write the Event to the Stream as shown in line 17. EventStreamWriter provides a writeEvent() operation that writes the given non-null Event object to the Stream using a given routing key to determine which Stream Segment it should appear on. Many operations in Pravega, such as writeEvent(), are asynchronous and return some sort of Future object. If the application needed to make sure the Event was durably written to Pravega and available for Readers, it could wait on the Future before proceeding. In the case of our simple \"hello world\" example, we don't bother waiting. EventStreamWriter can also be used to begin a Transaction. We cover Transactions in more detail elsewhere ( Working with Pravega: Transactions ). That's it for writing Events. Now lets take a look at how to read Events using Pravega. HelloWorldReader The HelloWorldReader is a simple demonstration of using the EventStreamReader. The application simply reads Events from the given Stream and prints a string representation of those Events onto the console. Just like the HelloWorldWriter example, the key part of the HelloWorldReader app is in the run() method: public void run () { StreamManager streamManager = StreamManager . create ( controllerURI ); final boolean scopeIsNew = streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder () . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); final boolean streamIsNew = streamManager . createStream ( scope , streamName , streamConfig ); final String readerGroup = UUID . randomUUID (). toString (). replace ( - , ); final ReaderGroupConfig readerGroupConfig = ReaderGroupConfig . builder () . stream ( Stream . of ( scope , streamName )) . build (); try ( ReaderGroupManager readerGroupManager = ReaderGroupManager . withScope ( scope , controllerURI )) { readerGroupManager . createReaderGroup ( readerGroup , readerGroupConfig ); } try ( ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamReader String reader = clientFactory . createReader ( reader , readerGroup , new JavaSerializer String (), ReaderConfig . builder (). build ())) { System . out . format ( Reading all the events from %s/%s%n , scope , streamName ); EventRead String event = null ; do { try { event = reader . readNextEvent ( READER_TIMEOUT_MS ); if ( event . getEvent () != null ) { System . out . format ( Read event %s %n , event . getEvent ()); } } catch ( ReinitializationRequiredException e ) { //There are certain circumstances where the reader needs to be reinitialized e . printStackTrace (); } } while ( event . getEvent () != null ); System . out . format ( No more events from %s/%s%n , scope , streamName ); } Lines 2-8 set up the Scope and Stream just like in the HelloWorldWriter application. Lines 10-15 set up the ReaderGroup as the prerequisite to creating the EventStreamReader and using it to read Events from the Stream (lines 17-36). ReaderGroup Basics Any Reader in Pravega belongs to some ReaderGroup. A ReaderGroup is a grouping of one or more Readers that consume from a Stream in parallel. Before we create a Reader, we need to either create a ReaderGroup (or be aware of the name of an existing ReaderGroup). This application only uses the basics from ReaderGroup. Lines 10-15 show basic ReaderGroup creation. ReaderGroup objects are created from a ReaderGroupManager object. The ReaderGroupManager object, in turn, is created on a given Scope with a URI to one of the Pravega Controllers, very much like a ClientFactory is created. A ReaderGroupManager object is created on line 14. Note the creation is also in a try-with-resources statement to make sure the ReaderGroupManager is properly cleaned up. The ReaderGroupManager allows a developer to create, delete and retrieve ReaderGroup objects by name. To create a ReaderGroup, the developer needs a name for the ReaderGroup, a configuration with a set of 1 or more Streams to read from. The ReaderGroup's name might be meaningful to the application, like \"WebClickStreamReaders\". In our case, on line 10, we have a simple UUID as the name (note the modification of the UUID string to remove the \"-\" character because ReaderGroup names can only have letters and numbers). In cases where you will have multiple Readers reading in parallel and each Reader in a separate process, it is helpful to have a human readable name for the ReaderGroup. In our case, we have one Reader, reading in isolation, so a UUID is a safe way to name the ReaderGroup. Since the ReaderGroup is created via the ReaderGroupManager and since the ReaderGroupManager is created within the context of a Scope, we can safely conclude that ReaderGroup names are namespaced by that Scope. The ReaderGroupConfig right now doesn't have much behavior. The developer specifies the Stream which should be part of the ReaderGroup and its lower and upper bounds. In our case, on line 11, we start at the beginning of the Stream. Other configuration items, such as specifying checkpointing etc. are options that will be available through the ReaderGroupConfig. But for now, we keep it simple. The fact that a ReaderGroup can be configured to read from multiple Streams is kind of cool. Imagine a situation where I have a collection of Stream of sensor data coming from a factory floor, each machine has its own Stream of sensor data. I can build applications that use a ReaderGroup per Stream so that the app reasons about data from exactly one machine. I can build other apps that use a ReaderGroup configured to read from all of the Streams. In our case, on line 14, the ReaderGroup only reads from one Stream. You can call createReaderGroup() with the same parameters multiple times, it doesn't hurt anything, and the same ReaderGroup will be returned each time after it is initially created. Note that in other cases, if the developer knows the name of the ReaderGroup to use and knows it has already been created, s/he can use getReaderGroup() on ReaderGroupManager to retrieve the ReaderGroup object by name. So at this point in the code, we have the Scope and Stream set up, we have the ReaderGroup created and now we need to create a Reader and start reading Events. Reading Event using an EventStreamReader Lines 17-36 show an example of setting up an EventStreamReader and reading Events using that EventStreamReader. First, we create a ClientFactory on line 17, in the same way we did it in the HelloWorldWriter app. Then we use the ClientFactory to create an EventStreamReader object. There are four things the developer needs to create a Reader: a name for the reader, the readerGroup it should be part of, the type of object expected on the Stream, the serializer to use to convert from the bytes stored in Pravega into the Event objects and a ReaderConfig. Lines 18-21 show the creation of an EventStreamReader. The name of the Reader can be any valid Pravega name (numbers and letters). Of course, the name of the reader is namespaced within the Scope. We talked about the creation of the ReaderGroup in the previous section. Just like with the EventStreamWriter, EventStreamReader uses Java generic types to allow a developer to specify a type safe Reader. In our case, we read Strings from the stream and use the standard Java String Serializer to convert the bytes read from the stream into String objects. Finally, the ReaderConfig is created, but at the moment, there are no configuration items associated with a Reader, so the empty ReaderConfig is just a place holder as Pravega evolves to include configuration items on Readers. Note that you cannot create the same Reader multiple times. Basically overtime you call createReader() it tries to add the Reader to the ReaderGroup. If the ReaderGroup already contains a Reader with that name, an exception is thrown. Now that we have an EventStreamReader created, we can start using it to read Events from the stream. This is done on line 26. The readNextEvent() operation returns the next Event available on the Stream, or if there is no such Event, blocks for a specified timeout period. If, after the timeout period has expired and no Event is available for reading, null is returned. That is why there is a null check on line 27 (to avoid printing out a spurious \"null\" event message to the console). It is also used as the termination of the loop on line 34. Note that the Event itself is wrapped in an EventRead object. It is worth noting that readNextEvent() may throw an exception (handled in lines 30-33). This exception would be handled in cases where the Readers in the ReaderGroup need to be reset to a checkpoint or the ReaderGroup itself has been altered and the set of Streams being read has therefore been changed. So that's it. The simple HelloWorldReader loops, reading Events from a Stream until there are no more Events, and then the application terminates. Experimental batch reader For applications that want to perform batch reads of historical stream data, the BatchClient provides a way to do this. It allows for listing all of the segments in a stream, and reading their data. When the data is read this way, rather than joining a reader group which automatically partitions the data, the underlying structure of the stream is exposed and it is up to the application to decide how to process it. So events read in this way need not be read in order. Obviously this API is not for every application, the main advantage is that it allows for low level integration with batch processing frameworks such as MapReduce. As an example to iterate over all the segments in the stream: //Passing null to fromStreamCut and toStreamCut will result in using the current start of stream and the current end of stream respectively. Iterator SegmentRange segments = client . listSegments ( stream , null , null ). getIterator (); SegmentRange segmentInfo = segments . next (); Or to read the events from a segment: SegmentIterator T events = client . readSegment ( segmentInfo , deserializer ); while ( events . hasNext ()) { processEvent ( events . next ()); }","title":"Working with Reader and Writer"},{"location":"basic-reader-and-writer/#working-with-pravega-basic-reader-and-writer","text":"Lets examine how to build simple Pravega applications. The simplest kind of Pravega application uses a Pravega Reader to read from a Pravega Stream or a Pravega Writer that writes to a Pravega Stream. A simple example of both can be found in the Pravega Samples \"hello world\" app. These sample applications provide a very basic example of how a Java application could use the Pravega Java Client Library to access Pravega functionality. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts) before continuing reading this page.","title":"Working with Pravega: Basic Reader and Writer"},{"location":"basic-reader-and-writer/#helloworldwriter","text":"The HelloWorldWriter application is a simple demonstration of using the EventStreamWriter to write an Event to Pravega. Taking a look first at the HelloWorldWriter example application, the key part of the code is in the run() method: public void run ( String routingKey , String message ) { StreamManager streamManager = StreamManager . create ( controllerURI ); final boolean scopeCreation = streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder () . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); final boolean streamCreation = streamManager . createStream ( scope , streamName , streamConfig ); try ( ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamWriter String writer = clientFactory . createEventWriter ( streamName , new JavaSerializer String (), EventWriterConfig . builder (). build ())) { System . out . format ( Writing message: %s with routing-key: %s to stream %s / %s %n , message , routingKey , scope , streamName ); final CompletableFuture Void writeFuture = writer . writeEvent ( routingKey , message ); } } The purpose of the run() method is to create a Stream (lines 2-9) and output the given Event to that Stream (lines 10-18).","title":"HelloWorldWriter"},{"location":"basic-reader-and-writer/#creating-a-stream-and-the-streammanager-interface","text":"A Stream is created in the context of a Scope; the Scope acts as a namespace mechanism so that different sets of Streams can be categorized for some purpose. For example, I might have a separate scope for each application. I might choose to create a set of Scopes, one for each department in my organization. In a multi-tenant environment, I might have a separate Scope per tenant. As a developer, I can choose whatever categorization scheme I need and use the Scope concept for organizing my Streams along that categorization scheme. Scopes and Streams are created and manipulated via the StreamManager Interface to the Pravega Controller. You need to have a URI to any of the Pravega Controller instances in your cluster in order to create a StreamManager object. This is shown in line 2. In the setup for the HelloWorld sample applications, the controllerURI is configured as a command line parameter when the sample application is launched. For the \"single node\" deployment of Pravega, the Controller is listening on localhost, port 9090. The StreamManager provides access to various control plane functions in Pravega related to Scopes and Streams: Method Parameters Discussion (static) create (URI controller) Given a URI to one of the Pravega Controller instances in the Pravega Cluster, create a Stream Manager object. createScope (String scopeName) Creates a Scope with the given name. Returns true if the Scope is created, returns false if the Scope already exists. You can call this method even if the Scope already exists, it won't harm anything. deleteScope (String scopeName) Deletes a Scope with the given name. Returns true if the scope was deleted, false otherwise. Note, if the Scope contains Streams, the deleteScope operation will fail with an exception. If you delete a nonexistent Scope, the method will succeed and return false. createStream (String scopeName, String streamName, StreamConfiguration config) Create a Stream within a given Scope. Note that both scope name and stream name are limited by the following pattern: [a-zA-Z0-9]+ (i.e. letters and numbers only, no punctuation) Note also: the Scope must exist, an exception is thrown if you create a Stream in a nonexistent scope. A StreamConfiguration is built using a builder pattern Returns true if the Stream is created, returns false if the Stream already exists. You can call this method even if the Stream already exists, it won't harm anything. updateStream (String scopeName, String streamName, StreamConfiguration config) Swap out the Stream's configuration. Note the Stream must already exist, an exception is thrown if you update a nonexistent stream. Returns true if the Stream was changed sealStream (String scopeName, String streamName) Prevent any further writes to a Stream Note the Stream must already exist, an exception is thrown if you seal a nonexistent stream. Returns true if the Stream is successfully sealed deleteStream (String scopeName, String streamName) Remove the Stream from Pravega and recover any resources used by that Stream Note the Stream must already exist, an exception is thrown if you delete a nonexistent stream. Returns true if the stream was deleted. After line 3 in the code is finished, we have established that the Scope exists, we can then go on and create the Stream in lines 5-8. The StreamManager needs 3 things to create a Stream, the Scope's name, the Stream's name and a StreamConfiguration. The most interesting task is to create the StreamConfiguration. Like many objects in Pravega, a Stream takes a configuration object that allows a developer to control various behaviors of the Stream. All configuration objects in Pravega use a builder pattern for construction. There are really two important configuration items related to streams: Retention Policy and Scaling Policy. Retention Policy allows the developer to control how long data is kept in a Stream before it is deleted. S/he can specify data should be kept for a certain period of time (ideal for situations like regulatory compliance that mandate certain retention periods) or to retain data until a certain number of bytes have been consumed. At the moment, Retention Policy is not completely implemented. By default, the RetentionPolicy is set as \"unlimited\" meaning, data will not be removed from the Stream. Scaling Policy is the way developers configure a Stream to take advantage Pravega's auto-scaling feature. In line 6, we use a fixed policy, meaning the Stream is configured with the given number of Stream Segments and that won't change. The other options are to scale by a given number of Events per second or a given number of Kilobytes per second. In these two policies, the developer specifies a target rate, a scaling factor and a minimum number of Segments. The target rate is straight forward, if ingest rate exceeds a certain number of Events or Kilobytes of data for a sustained period of time, Pravega will attempt to add new Stream Segments to the Stream. If the rate drops below that threshold for a sustained period of time, Pravega will attempt to merge adjacent Stream Segments. The scaling factor is a setting on the Scaling Policy that determines how many Stream Segments should be added when the target rate (of Events or Kilobytes) is exceeded. The minimum number of Segments is a factor that sets the minimum degree of read parallelism to be maintained; if this value is set at 3, for example, there will always be 3 Stream Segments available on the Stream. Currently, this property is effective only when the stream is created; at some point in the future, update stream will allow this factor to be used to change the minimum degree of read parallelism on an existing Stream. Once the StreamConfiguration object is created, creating the Stream is straight forward (line 8). After the Stream is created, we are all set to start writing Event(s) to the Stream.","title":"Creating a Stream and the StreamManager Interface"},{"location":"basic-reader-and-writer/#writing-an-event-using-eventwriter","text":"Applications use an EventStreamWriter object to write Events to a Stream. The key object to creating the EventStreamWriter is the ClientFactory. The ClientFactory is used to create Readers, Writers and other types of Pravega Client objects such as the State Synchronizer (see Working with Pravega: State Synchronizer ). Line 10 shows the creation of a ClientFactory. A ClientFactory is created in the context of a Scope, since all Readers, Writers and other Clients created by the ClientFactory are created in the context of that Scope. The ClientFactory also needs a URI to one of the Pravega Controllers, just like StreamManager. Because ClientFactory and the objects it creates consumes resources from Pravega, it is a good practice to create these objects in a try-with-resources statement. Because ClientFactory and the objects it creates all implement Autocloseable, the try-with-resources approach makes sure that regardless of how your application ends, the Pravega resources will be properly closed in the right order. Now that we have a ClientFactory, we can use it to create a Writer. There are several things a developer needs to know before s/he creates a Writer: What is the name of the Stream to write to? Note: the Scope has already been determined when the ClientFactory was created What Type of Event objects will be written to the Stream? What serializer will be used to convert an Event object to bytes? Recall that Pravega only knows about sequences of bytes, it does not really know anything about Java objects. Does the Writer need to be configured with any special behavior? In our example, lines 11-13 show all these decisions. This Writer writes to the Stream specified in the configuration of the HelloWorldWriter object itself (by default the stream is named \"helloStream\" in the \"examples\" Scope). The Writer processes Java String objects as Events and uses the built in Java serializer for Strings. The EventWriterConfig allows the developer to specify things like the number of attempts to retry a request before giving up and associated exponential back settings. Pravega takes care to retry requests in the case where connection failures or Pravega component outages may temporarily prevent a request from succeeding, so application logic doesn't need to be complicated with dealing with intermittent cluster failures. In our case, we took the default settings for EventWriterConfig in line 13. Now we can write the Event to the Stream as shown in line 17. EventStreamWriter provides a writeEvent() operation that writes the given non-null Event object to the Stream using a given routing key to determine which Stream Segment it should appear on. Many operations in Pravega, such as writeEvent(), are asynchronous and return some sort of Future object. If the application needed to make sure the Event was durably written to Pravega and available for Readers, it could wait on the Future before proceeding. In the case of our simple \"hello world\" example, we don't bother waiting. EventStreamWriter can also be used to begin a Transaction. We cover Transactions in more detail elsewhere ( Working with Pravega: Transactions ). That's it for writing Events. Now lets take a look at how to read Events using Pravega.","title":"Writing an Event using EventWriter"},{"location":"basic-reader-and-writer/#helloworldreader","text":"The HelloWorldReader is a simple demonstration of using the EventStreamReader. The application simply reads Events from the given Stream and prints a string representation of those Events onto the console. Just like the HelloWorldWriter example, the key part of the HelloWorldReader app is in the run() method: public void run () { StreamManager streamManager = StreamManager . create ( controllerURI ); final boolean scopeIsNew = streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder () . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); final boolean streamIsNew = streamManager . createStream ( scope , streamName , streamConfig ); final String readerGroup = UUID . randomUUID (). toString (). replace ( - , ); final ReaderGroupConfig readerGroupConfig = ReaderGroupConfig . builder () . stream ( Stream . of ( scope , streamName )) . build (); try ( ReaderGroupManager readerGroupManager = ReaderGroupManager . withScope ( scope , controllerURI )) { readerGroupManager . createReaderGroup ( readerGroup , readerGroupConfig ); } try ( ClientFactory clientFactory = ClientFactory . withScope ( scope , controllerURI ); EventStreamReader String reader = clientFactory . createReader ( reader , readerGroup , new JavaSerializer String (), ReaderConfig . builder (). build ())) { System . out . format ( Reading all the events from %s/%s%n , scope , streamName ); EventRead String event = null ; do { try { event = reader . readNextEvent ( READER_TIMEOUT_MS ); if ( event . getEvent () != null ) { System . out . format ( Read event %s %n , event . getEvent ()); } } catch ( ReinitializationRequiredException e ) { //There are certain circumstances where the reader needs to be reinitialized e . printStackTrace (); } } while ( event . getEvent () != null ); System . out . format ( No more events from %s/%s%n , scope , streamName ); } Lines 2-8 set up the Scope and Stream just like in the HelloWorldWriter application. Lines 10-15 set up the ReaderGroup as the prerequisite to creating the EventStreamReader and using it to read Events from the Stream (lines 17-36).","title":"HelloWorldReader"},{"location":"basic-reader-and-writer/#readergroup-basics","text":"Any Reader in Pravega belongs to some ReaderGroup. A ReaderGroup is a grouping of one or more Readers that consume from a Stream in parallel. Before we create a Reader, we need to either create a ReaderGroup (or be aware of the name of an existing ReaderGroup). This application only uses the basics from ReaderGroup. Lines 10-15 show basic ReaderGroup creation. ReaderGroup objects are created from a ReaderGroupManager object. The ReaderGroupManager object, in turn, is created on a given Scope with a URI to one of the Pravega Controllers, very much like a ClientFactory is created. A ReaderGroupManager object is created on line 14. Note the creation is also in a try-with-resources statement to make sure the ReaderGroupManager is properly cleaned up. The ReaderGroupManager allows a developer to create, delete and retrieve ReaderGroup objects by name. To create a ReaderGroup, the developer needs a name for the ReaderGroup, a configuration with a set of 1 or more Streams to read from. The ReaderGroup's name might be meaningful to the application, like \"WebClickStreamReaders\". In our case, on line 10, we have a simple UUID as the name (note the modification of the UUID string to remove the \"-\" character because ReaderGroup names can only have letters and numbers). In cases where you will have multiple Readers reading in parallel and each Reader in a separate process, it is helpful to have a human readable name for the ReaderGroup. In our case, we have one Reader, reading in isolation, so a UUID is a safe way to name the ReaderGroup. Since the ReaderGroup is created via the ReaderGroupManager and since the ReaderGroupManager is created within the context of a Scope, we can safely conclude that ReaderGroup names are namespaced by that Scope. The ReaderGroupConfig right now doesn't have much behavior. The developer specifies the Stream which should be part of the ReaderGroup and its lower and upper bounds. In our case, on line 11, we start at the beginning of the Stream. Other configuration items, such as specifying checkpointing etc. are options that will be available through the ReaderGroupConfig. But for now, we keep it simple. The fact that a ReaderGroup can be configured to read from multiple Streams is kind of cool. Imagine a situation where I have a collection of Stream of sensor data coming from a factory floor, each machine has its own Stream of sensor data. I can build applications that use a ReaderGroup per Stream so that the app reasons about data from exactly one machine. I can build other apps that use a ReaderGroup configured to read from all of the Streams. In our case, on line 14, the ReaderGroup only reads from one Stream. You can call createReaderGroup() with the same parameters multiple times, it doesn't hurt anything, and the same ReaderGroup will be returned each time after it is initially created. Note that in other cases, if the developer knows the name of the ReaderGroup to use and knows it has already been created, s/he can use getReaderGroup() on ReaderGroupManager to retrieve the ReaderGroup object by name. So at this point in the code, we have the Scope and Stream set up, we have the ReaderGroup created and now we need to create a Reader and start reading Events.","title":"ReaderGroup Basics"},{"location":"basic-reader-and-writer/#reading-event-using-an-eventstreamreader","text":"Lines 17-36 show an example of setting up an EventStreamReader and reading Events using that EventStreamReader. First, we create a ClientFactory on line 17, in the same way we did it in the HelloWorldWriter app. Then we use the ClientFactory to create an EventStreamReader object. There are four things the developer needs to create a Reader: a name for the reader, the readerGroup it should be part of, the type of object expected on the Stream, the serializer to use to convert from the bytes stored in Pravega into the Event objects and a ReaderConfig. Lines 18-21 show the creation of an EventStreamReader. The name of the Reader can be any valid Pravega name (numbers and letters). Of course, the name of the reader is namespaced within the Scope. We talked about the creation of the ReaderGroup in the previous section. Just like with the EventStreamWriter, EventStreamReader uses Java generic types to allow a developer to specify a type safe Reader. In our case, we read Strings from the stream and use the standard Java String Serializer to convert the bytes read from the stream into String objects. Finally, the ReaderConfig is created, but at the moment, there are no configuration items associated with a Reader, so the empty ReaderConfig is just a place holder as Pravega evolves to include configuration items on Readers. Note that you cannot create the same Reader multiple times. Basically overtime you call createReader() it tries to add the Reader to the ReaderGroup. If the ReaderGroup already contains a Reader with that name, an exception is thrown. Now that we have an EventStreamReader created, we can start using it to read Events from the stream. This is done on line 26. The readNextEvent() operation returns the next Event available on the Stream, or if there is no such Event, blocks for a specified timeout period. If, after the timeout period has expired and no Event is available for reading, null is returned. That is why there is a null check on line 27 (to avoid printing out a spurious \"null\" event message to the console). It is also used as the termination of the loop on line 34. Note that the Event itself is wrapped in an EventRead object. It is worth noting that readNextEvent() may throw an exception (handled in lines 30-33). This exception would be handled in cases where the Readers in the ReaderGroup need to be reset to a checkpoint or the ReaderGroup itself has been altered and the set of Streams being read has therefore been changed. So that's it. The simple HelloWorldReader loops, reading Events from a Stream until there are no more Events, and then the application terminates.","title":"Reading Event using an EventStreamReader"},{"location":"basic-reader-and-writer/#experimental-batch-reader","text":"For applications that want to perform batch reads of historical stream data, the BatchClient provides a way to do this. It allows for listing all of the segments in a stream, and reading their data. When the data is read this way, rather than joining a reader group which automatically partitions the data, the underlying structure of the stream is exposed and it is up to the application to decide how to process it. So events read in this way need not be read in order. Obviously this API is not for every application, the main advantage is that it allows for low level integration with batch processing frameworks such as MapReduce. As an example to iterate over all the segments in the stream: //Passing null to fromStreamCut and toStreamCut will result in using the current start of stream and the current end of stream respectively. Iterator SegmentRange segments = client . listSegments ( stream , null , null ). getIterator (); SegmentRange segmentInfo = segments . next (); Or to read the events from a segment: SegmentIterator T events = client . readSegment ( segmentInfo , deserializer ); while ( events . hasNext ()) { processEvent ( events . next ()); }","title":"Experimental batch reader"},{"location":"connectors/","text":"Pravega Connectors Connectors allow integrating Pravega with different data sources and sinks. Flink Connector The initial connector supported is Flink which enables building end-to-end stream processing pipelines with Pravega. This also allows reading and writing data to external data sources and sinks via Flink Streaming Connectors. In Experimental Stage Logstash Hadoop Connector","title":"Pravega Connectors"},{"location":"connectors/#pravega-connectors","text":"Connectors allow integrating Pravega with different data sources and sinks. Flink Connector The initial connector supported is Flink which enables building end-to-end stream processing pipelines with Pravega. This also allows reading and writing data to external data sources and sinks via Flink Streaming Connectors.","title":"Pravega Connectors"},{"location":"connectors/#in-experimental-stage","text":"Logstash Hadoop Connector","title":"In Experimental Stage"},{"location":"contributing/","text":"Contributing to Pravega Contributions guidelines Issue triaging and labeling Review process Happy hacking!","title":"Coding guildelines"},{"location":"contributing/#contributing-to-pravega","text":"Contributions guidelines Issue triaging and labeling Review process Happy hacking!","title":"Contributing to Pravega"},{"location":"controller-service/","text":"Pravega Controller Service Introduction Architecture Stream Management Cluster Management System Diagram Components Service Endpoints Controller Service Stream Metadata Store Stream Metadata Stream Store Caching Stream Buckets Controller Cluster Listener Host Store Background workers Roles and Responsibilities Stream Operations Stream State Create Stream Update Stream Scale Stream Truncate Stream Seal Stream Delete Stream Stream Policy Manager Scaling infrastructure Retention infrastructure Transaction Manager Create Transaction Commit Transaction Abort Transaction Ping Transaction Transaction Timeout Management Segment Container to Host Mapping Resources Introduction The controller service is a core component of Pravega that implements the control plane. It acts as the central coordinator and manager for various operations performed in the cluster, mainly divided into two categories: 1) stream management 2) cluster management. The controller service, referred to simply as controller henceforth, is responsible for providing the abstraction of a stream , which is the main abstraction that Pravega exposes to applications. A stream comprises one or more segments . Each segment is an append-only data structure that stores a sequence of bytes. A segment on its own is agnostic to presence of other segments and is not aware of its logical relationship with its peer segments. The segment store, which owns and manages these segments, does not have any notion of a stream. A stream is a logical view conceptualized by Controller by composing a dynamically changing set of segments that satisfy a predefined set of logical invariants. The controller provides the stream abstraction and orchestrates all lifecycle operations on a stream while ensuring that the abstraction stays consistent. The controller plays a central role in the lifecycle of a stream: creation, modification, scaling , and deletion. It does these by maintaining metadata per stream and performs requisite operations on segments as and when necessary. For example, as part of stream\u2019s lifecycle, new segments can be created and existing segments sealed. The controller decides when to perform these operation such that streams continue to be available and consistent to the clients accessing them. Architecture The Controller Service is made up of one or more instances of stateless worker nodes. Each new controller instance can be brought up independently and to become part of pravega cluster it merely needs to point to the same Apache Zookeeper . For high availability it is advised to have more than one instance of controller service per cluster. Each controller instance is capable of working independently and uses a shared persistent store as the source of truth for all state owned and managed by controller service. We currently use Apache ZooKeeper as the store for persisting all metadata consistently. Each instance comprises various subsystems which are responsible for performing specific operations on different categories of metadata. These subsystems include different API endpoints, metadata store handles, policy managers and background workers. The controller exposes two endpoints which can be used to interact with a controller service. The first port is for providing programmatic access for pravega clients and is implemented as an RPC using gRPC. The other endpoint is for administrative operations and is implemented as a REST endpoint. Stream Management The controller owns and manages the concept of stream and is responsible for maintaining metadata and lifecycle for each stream. Specifically, it is responsible for creating, updating, scaling, truncating, sealing and deleting streams. The Stream Management can be broadly divided into three categories: 1. Stream Abstraction A stream can be viewed as a series of dynamically changing segment sets where the stream transitions from one set of consistent segments to the next. Controller is the place for creating and managing this stream abstraction. Controller decides when and how a stream transitions from one state to another and is responsible for performing these transitions while keeping the state of the stream consistent and available. These transitions are governed user-defined policies that the controller enforces. Consequently, as part of stream management, the controller also performs roles of Policy Manager for policies like retention and scale. Automated policy Management Controller is responsible for storing and enforcing user-defined Stream policies by actively monitoring the state of the stream. Presently we have two policies that users can define, namely Scaling Policy and Retention Policy . Scaling policy describes if and under what circumstances a stream should automatically scale its number of segments. Retention policy describes a policy about how much data to retain within a stream. Transaction Management Implementing transactions requires the manipulation of segments. With each transaction, Pravega creates a set of transaction segments, which are later merged onto the stream segments upon commit or discarded upon aborts. The controller performs the role of transaction manager and is responsible for creating and committing transactions on a given stream. Upon creating transactions, controller also tracks transaction timeouts and aborts transactions whose timeouts have elapsed. Details of transaction management can be found later in the document. Cluster Management The controller is responsible for managing segment store cluster. Controller manages life cycle of segment store nodes as they are added to/removed from the cluster and performs redistribution of segment containers across available segment store nodes. System diagram The following diagram shows the main components of a controller process. We discuss the elements of the diagram in detail next. Controller Process Diagram Components Service Endpoints There are two ports exposed by controller: client-controller APIs and administration APIs. The client controller communication is implemented as RPC which exposes APIs to perform all stream related control plane operations. Apart from this controller also exposes an administrative API set implemented as REST. Each endpoint performs appropriate call to the Controller Service backend subsystem which has the actual implementation for various create, read, update and delete (CRUD) operations on entities owned and managed by controller. ##### gRPC Client Controller communication endpoint is implemented as a gRPC interface. The complete list of APIs can be found here . This exposes APIs used by Pravega clients (readers, writers and stream manager) and enables stream management. Requests enabled by this API include creating, modifying, and deleting streams. The underlying gRPC framework provides both synchronous and asynchronous programming models. We use the asynchronous model in our client controller interactions so that the client thread does not block on the response from the server. To be able to append to and read data from streams, writers and readers query controller to get active segment sets, successor and predecessor segments while working with a stream. For transactions, the client uses specific API calls to request controller to create and commit transactions. ##### REST For administration, the controller implements and exposes a REST interface. This includes API calls for stream management as well as other administration API primarily dealing with creation and deletion of scopes. We use swagger to describe our REST APIs. The swagger yaml file can be found here . Controller Service This is the backend layer behind the controller endpoints (gRPC and REST). All the business logic required to serve controller API calls are implemented here. This layer contains handles to all other subsystems like the various store implementations (stream store, host store and checkpoint store) and background processing frameworks (task framework, event processor framework). Stores are interfaces that provide access to various types of metadata managed by Controller. Background processing frameworks are used to perform asynchronous processing that typically implement workflows involving metadata updates and requests to segment store. Stream Metadata Store A stream is dynamically-changing sequence of segments, where regions of the routing key space map to open segments. As the set of segments of a stream changes, so does the mapping of the routing key space to segment. A set of segments is consistent if 1) union of key space regions mapping to segments in the set covers the entire key space; and 2) There is no overlap between key space regions. For example, suppose a set S = { S 1 , S 2 , S 3 }, such that: - Region [0, 0.3) maps to segment S 1 - Region [0.3, 0.6) maps to segment S 2 - Region [0.6, 1.0) maps to segment S 3 S is a consistent segment set. A stream goes through transformations as it scales over time. A stream starts with an initial set of segments that is determined by the stream configuration when created and it transitions to new sets of segments as scale operations are performed on the stream. Each generation of segments that constitute stream at any given point in time are considered to belong to an epoch. So a stream starts with initial epoch which is epoch 0 and upon each transition, it moves ahead in its epochs to describe the change in generation of segments in the stream. The controller maintains the stream store the information about all epochs that constitute a given stream and how they transition. The store is designed to optimally store and query information pertaining to segments and their inter-relationships. Apart from the epoch information, it keeps some additional metadata, such as state and its policies and ongoing transactions on the stream. Various sub-components of controller access the stored metadata for each stream via a well-defined interface . We currently have two concrete implementations of the stream store interface: in-memory and zookeeper backed stores. Both share a common base implementation that relies on stream objects for providing store-type specific implementations for all stream-specific metadata. The base implementation of stream store creates and caches these stream objects. The stream objects implement a store/stream interface. The concrete stream implementation is specific to the store type and is responsible for implementation of store specific methods to provide consistency and correctness. We have a common base implementation of all store types that provide optimistic concurrency. This base class encapsulates the logic for queries against stream store for all concrete stores that support Compare and Swap (CAS). We currently use zookeeper as our underlying store which also supports CAS. We store all stream metadata in a hierarchical fashion under stream specific znodes (ZooKeeper data nodes). For the ZooKeeper based store, we structure our metadata into different groups to support a variety of queries against this metadata. All stream specific metadata is stored under a scoped/stream root node. Queries against this metadata include, but not limited to, querying segment sets that form the stream at different points in time, segment specific information, segment predecessors and successors. Refer to stream metadata interface for details about APIs exposed by stream metadata store. Stream Metadata Clients need information about what segments constitute a stream to start their processing and they obtain it from the epoch information the controller stores in the stream store. A reader client typically starts from the head of the stream, but it might also choose to access the stream starting from any arbitrarily interesting position. Writers on the other hand always append to the tail of the stream. Clients need ability to query and find segments at any of the three cases efficiently. To enable such queries, the stream store provides API calls to get Initial set of Segments , get Segments At Specific Time and get Current set of Segments . As mentioned earlier, a stream can transition from one set of segments (epoch) to another set of segments that constitute the stream. A stream moves from one epoch to another if there is at least one segment that is sealed and that is replaced by one or more set of segments that cover precisely the key space of the sealed segments. As clients work on streams, they may encounter the end of sealed segments and consequently need to find new segments to be able to move forward. To enable the clients to query for the next segments, the stream store exposes via the controller service efficient queries for finding immediate successors and predecessors for any arbitrary segment. To enable serving queries like those mentioned above, we need to efficiently store a time series of these segment transitions and index them against time. We store this information about the current and historical state of a stream-segments in a set of tables which are designed to optimize on aforementioned queries. Apart from segment specific metadata record, the current state of stream comprises of other metadata types that are described henceforth. Tables To efficiently store and query the segment information, we have split segment data into three append only tables, namely, segment-table, history-table and index-table. Segment Table segment-info: \u27e8segmentid, time, keySpace-start, keySpace-end\u27e9 The controller stores the segment table in an append-only table with i- th row corresponding to metadata for segment id i . It is important to note that each row in the segment table is of fixed size. As new segments are added, they are assigned new segment ids in a strictly increasing order. So this table is very efficient in creating new segments and querying segment information response with O(1) processing for both these operations. History Table epoch: \u27e8time, list-of-segments-in-epoch\u27e9 The History Table stores a series of active segments as they transition from one epoch to another. Each row in the history table stores an epoch which captures a logically consistent (as defined earlier) set of segments that form the stream and are valid through the lifespan of the epoch. This table is designed to optimize queries to find set of segments that form the stream at any arbitrary time. There are three most commonly used scenarios where we want to efficiently know the set of segments that form the stream - initial set of segments, current set of segments and segments at any arbitrary time. First two queries are very efficiently answered in O(1) time because they correspond to first and last rows in the table. Since rows in the table are sorted by increasing order of time and capture time series of streams segment set changes, so we could easily perform binary search to find row which corresponds to segment sets at any arbitrary time. Index Table index: \u27e8time, offset-in-history-table\u27e9 Since history rows are of variable length, we index history rows for timestamps in the index table. This enables us to navigate the history table and perform binary search to efficiently answer queries to get segment set at any arbitrary time. We also perform binary searches on history table to determine successors of any given segment. Stream Configuration Znode under which stream configuration is serialized and persisted. A stream configuration contains stream policies that need to be enforced. Scaling policy and Retention policy are supplied by the application at the time of stream creation and enforced by controller by monitoring the rate and size of data in the stream. Scaling policy describes if and when to automatically scale based on incoming traffic conditions into the stream. The policy supports two flavours - traffic as rate of events per second and traffic as rate of bytes per second. The application specifies their desired traffic rates into each segment by means of scaling policy and the supplied value is chosen to compute thresholds that determine when to scale a given stream. Retention Policy describes the amount of data that needs to be retained into pravega cluster for this stream. We support a time based and a size based retention policy where applications can choose whether they want to retain data in the stream by size or by time by choosing the appropriate policy and supplying their desired values. Stream State Znode which captures the state of the stream. It is an enum with values from Creating, Active, Updating, Scaling, Truncating, Sealing, and Sealed*.* Once Active, a stream transitions between performing a specific operation and active until it is sealed. A transition map is defined in the State class which allows and prohibits various state transitions. Stream state describes the current state of the stream. It transitions from ACTIVE to respective action based on the action being performed on the stream. For example, during scaling the state of the stream transitions from ACTIVE to SCALING and once scaling completes, it transitions back to ACTIVE. Stream state is used as a barrier to ensure only one type of operation is being performed on a given stream at any point in time. Only certain state transitions are allowed and are described in the State Transition object. Only legitimate state transitions are allowed and any attempt for disallowed transition results in appropriate exception. Truncation Record This corresponds to the stream cut which was last used to truncate the given stream. All stream segment queries superimpose the truncation record and return segments that are strictly greater than or equal to the stream cut in truncation record. Sealed Segments Record Since the segment table is append only, any additional information that we need to persist when a segment is sealed is stored in sealed segments record. Presently, it simple contains a map of segment number to its sealed size. Transaction Related metadata records: Active Transactions Each new transaction is created under this Znode. This stores metadata corresponding to each transaction as ActiveTransactionRecord . Once a transaction is completed, a new node is created under the global Completed Transaction Znode and removed from under the stream specific active transaction node. Completed Transactions All completed transactions for all streams are moved under this single znode upon completion (via either commit or abort paths). We can subsequently garbage collect these values periodically following any collection scheme we deem fit. We have not implemented any scheme at this point though. Stream Store Caching Since there could be multiple concurrent requests for a given stream being processed by same controller instance, it is suboptimal to read the value by querying zookeeper every time. So we have introduced an in-memory cache that each stream store maintains. It caches retrieved metadata per stream so that there is maximum one copy of the data per stream in the cache. We have two in-memory caches \u2013 a) a cache of multiple stream objects in the store, b) cache properties of a stream in the stream object. We have introduced a concept of operation context and at the start of any new operation a new operation context is created. The creation of a new operation context invalidates the cached entities for a stream and each entity is lazily retrieved from the store whenever requested. If a value is updated during the course of the operation, it is again invalidated in the cache so that other concurrent read/update operations on the stream get the new value for their subsequent steps. Stream Buckets To enable some scenarios, we may need our background workers to periodically work on each of the streams in our cluster to perform some specific action on them. We bring in a notion of a bucket to distribute this periodic background work across all available controller instances. For this we hash each stream into one of the predefined buckets and then distribute buckets across available controller instances. Number of buckets for a cluster is a fixed (configurable) value for the lifetime of a cluster. Controller instances map all available streams in the system into buckets and distribute buckets amongst themselves so that all long running background work can be uniformly distributed across multiple controller instances. Each bucket corresponds to a unique Znode in zookeeper. Fully qualified scoped stream name is used to compute a hashed to assign the stream to a bucket. All controller instances, upon startup, attempt to take ownership of buckets. Upon failover, ownerships are transferred as surviving nodes compete to acquire ownership of orphaned buckets. The controller instance which owns a bucket is responsible for all long running scheduled background work corresponding to all nodes under the bucket. Presently this entails running periodic workflows to capture stream-cuts (called Retention-Set) for each stream at desired frequencies. Retention Set One retention set per stream is stored under the corresponding bucket/stream Znode. As we compute stream-cuts periodically, we keep preserving them under this Znode. As some automatic truncation is performed, the stream-cuts that are no longer valid are purged from this set. Controller Cluster Listener Each node in Pravega Cluster registers itself under a cluster Znode as an ephemeral node. This includes both controller and segment store nodes. Each controller instance registers a watch on the cluster Znode to listen for cluster change notifications. These notifications are of the kind node-added and node-removed. One controller instance assumes leadership amongst all controller instances. This leader controller instance is responsible for handling segment store node change notifications. Based on change in topology, controller instance periodically rebalances segment containers to segment store node mapping. All controller instances listen for controller node change notifications. Each controller instance has multiple sub components that implement the failover-sweeper interface. Presently there are three components that implement failover sweeper interface namely, TaskSweeper, EventProcessors and TransactionSweeper . Whenever a controller instance is identified to have been removed from the cluster, the cluster listener invokes all registered failover sweepers to optimistically try to sweep all the orphaned work previously owned by the failed controller host. Host Store Host store interface is used to store Segment Container to Segment Store node mapping. It exposes APIs like getHostForSegment where it computes consistent hash of segment id to compute the owner Segment Container. Then based on the container-host mapping, it returns the appropriate Uri to the caller. Background workers Controller process has two different mechanisms/frameworks for processing background work. These background works typically entail multiple steps and updates to metadata under a specific metadata root entity and potential interactions with one or more segment stores. We initially started with a simple task framework that allows ability to run tasks that take exclusive rights over a given resource (typically a stream) and allow for tasks to failover from one controller instance to another. However, this model was limiting in its scope and locking semantics and had no inherent notion of ordering of tasks as multiple tasks could race to acquire working rights (lock) on a resource concurrently and any one of them could succeed. To overcome this limitation we came up with a new infrastructure called Event Processor. Event processor is the classic eat-your-own-dog-food. It is built using pravega streams. This provides us a neat mechanism to ensure mutually exclusive and ordered processing. Task Framework Task framework is designed to run exclusive background processing per resource such that in case of controller instance failure, the work can easily failover to another controller instance and brought to completion. The framework, on its own, does not guarantee idempotent processing and the author of a task has to handle it if required. The model of tasks is defined to work on a given resource exclusively, which means no other task can run concurrently on the same resource. This is implemented by way of a persisted distributed lock implemented on zookeeper. The failover of a task is achieved by following a scheme of indexing the work a given process is performing. So if a process fails, another process will sweep all outstanding work and attempt to transfer ownership to itself. Note: upon failure of a controller process, multiple surviving controller processes can concurrently attempt sweeping of orphaned tasks. Each of them will index the task in their host-index but exactly one of them will be able to successfully acquire the lock on the resource and hence permission to process the task. The parameters for executing a task are serialized and stored under the resource. Currently we use task framework only for create stream tasks. All the other background processing is done using event processor framework. Event Processor Framework Event processors framework is a background worker sub system which reads events from an internal stream and processes it, hence the name event processor. All event processors in our system provide at least once processing guarantee. And in its basic flavor, the framework also provides strong ordering guarantees. But we also have different subtypes of event processors that allow concurrent processing. We create different event processors for different kinds of work. Presently we have three different event processors in our system for committing transaction, aborting transactions and processing stream specific requests like scale update seal etc. Each controller instance has one event processor of each type. The event processor framework allows for multiple readers to be created per event processor. All readers for a specific event processor across controller instances share the same reader group, which guarantees mutually exclusive distribution of work across controller instances. Each reader gets a dedicated thread where it reads the event, calls for its processing and upon completion of processing, updates its checkpoint. Events are posted in the event processor specific stream and are routed to specific segments based on using scoped stream name as the routing key. We have two flavors of event processors, one that performs serial processing, which essentially means it reads an event and initiates its processing and waits on it to complete before moving on to next event. This provides strong ordering guarantees in processing. And it checkpoints after processing each event. Commit transaction is implemented using this base flavor of event processor. The degree of parallelism for processing these events is upper bounded by number of segments in the internal stream and lower bounded by number of readers. Multiple events from across different streams could land up in the same segment and since we perform serial processing, serial processing has the drawback that processing stalls or flooding of events from one stream could adversely impact latencies for unrelated streams. To overcome these drawbacks we designed Concurrent Event Processor as an overlay on Serial Event processor. Concurrent event processor, as name implies, allows us to process multiple events concurrently. Here the reader thread reads an event, schedules it\u2019s asynchronous processing and returns to read the next event. There is a ceiling on number of events that are concurrently processed at any point in time and as processing of some event completes, newer events are allowed to be fetched. The checkpoint scheme here becomes slightly more involved because we want to guarantee at least once processing. However, with concurrent processing the ordering guarantees get broken. However, it is important to note that we only need ordering guarantees for processing events from a stream and not across streams. In order to satisfy ordering guarantee, we overlay Concurrent Event Processor with Serialized Request Handler, which queues up events from the same stream in an in-memory queue and processes them in order. Commit Transaction processing is implemented on a dedicated Serial Event Processor because we want strong Commit ordering while ensuring that commit does not interfere with processing of other kinds of requests on the stream. Abort Transaction processing is implemented on a dedicated Concurrent Event Processor which performs abort processing on transactions from across streams concurrently. All other requests for streams is implemented on a Serialized Request Handler which ensures exactly one request per stream is being processed at any given time and there is ordering guarantee within request processing. However, it allows for concurrent requests from across streams to go on concurrently. Workflows like scale, truncation, seal, update and delete stream are implemented for processing on the Request Event processor. Roles and Responsibilities Stream Operations Controller is the store of truth for all stream related metadata. Pravega clients (EventStreamReaders and EventStreamWriters), in conjunction with the controller, ensure that stream invariants are satisfied and honored as they work on streams. The controller maintains the metadata of streams, including the entire history of segments. Client accessing a stream need to contact the controller to obtain information about segments. Clients query controller in order to know how to navigate streams. For this purpose controller exposes appropriate APIs to get active segments, successors, predecessors and segment information and Uris. These queries are served using metadata stored and accessed via stream store interface. Controller also provides workflows to modify state and behavior of the stream. These workflows include create, scale, truncation, update, seal, and delete. These workflows are invoked both via direct APIs and in some cases as applicable via background policy manager (auto-scale and retention). Request Processing Flow Create Stream Create stream is implemented as a task on Task Framework. Create stream workflow first creates initial stream metadata with stream state set to CREATING*.* Following this, it identifies segment containers that should own and create segments for this stream and calls create-segment concurrently. Once all create segments complete, the create stream task completes thus moving the stream to ACTIVE state. All failures are retried few times with exponential backoffs. However, if it is unable to complete any step, the stream is left dangling in CREATING state. * * Update Stream Update stream is implemented as a task on Serialized Request Handler/Concurrent Event Processor framework. Update stream is invoked by an explicit API call into controller. It first posts an Update Request event into request-stream. Following that it tries to create a temporary update property. If it fails to create the temporary update property, the request is failed and the caller is notified of the failure to update a stream due to conflict with another ongoing update. The event is picked by Request Event processor. When the processing starts, the update stream task expects to find the temporary update stream property to be present. If it does not find the property, the update processing is delayed by pushing event the back in the in-memory queue until it deems the event expired. If it finds the property to be updated during this period, before the expiry, the event is processed and update stream operation is performed. Now that the processing starts, it first sets the state to UPDATING. Following this the stream configuration is updated in the metadata store followed by notifying segment stores for all active segments of the stream about change in policy. Now the state is reset to ACTIVE. Scale Stream Scale can be invoked either by explicit API call (referred to as manual scale) or performed automatically based on scale policy (referred to as auto-scale). We first write the event followed by updating the segment table by creating new entries for desired segments to be created. This step is idempotent and ensures that if an existing ongoing scale operation is in progress, then this attempt to start a new scale fails. The start of processing is similar to mechanism followed in update stream. If metadata is updated, the event processes and proceeds with executing the task. If the metadata is not updated within the desired time frame, the event is discarded. Once scale processing starts, it first sets the stream state SCALING. This is followed by creating new segments in segment stores. After successfully creating new segments, it updates the history table with a partial record corresponding to new epoch which contains list of segments as they would appear post scale. Each new epoch creation also creates a new root epoch node under which metadata for all transactions from that epoch reside. So as the scale is performed, there would be a node corresponding to old epoch and now there will also be a root node for new epoch. Any transaction creation from this point on will be done against new epoch. Now the workflow attempts to complete scale by opportunistically attempting to delete the old epoch. Old epoch can be deleted if and only if there are no transactions under its tree. Once we are sure there are no transactions on old epoch, we can proceed with sealing old segments and completing the scale. After old segments are sealed successfully, the partial record in history table is now completed whereby completing the scale workflow. The state is now reset to ACTIVE. Truncate Stream Truncate follows similar mechanism to update and has a temporary stream-property for truncation that is used to supply input for truncate stream. Once truncate workflow identifies that it can proceed, it first sets the state to TRUNCATING. Truncate workflow then looks at the requested stream-cut, and checks if it is greater than or equal to the existing truncation point, only then is it a valid input for truncation and the workflow commences. The truncation workflow takes the requested stream-cut and computes all segments that are to be deleted as part of this truncation request. It then calls into respective segment stores to delete identified segments. Post deletion, we call truncate on segments that are described in the stream cut at the offsets as described in the stream cut. Following this the truncation record is updated with the new truncation point and deleted segments. The state is reset to ACTIVE. Seal Stream Seal stream can be requested via an explicit API call into controller. It first posts a seal-stream event into request stream followed by attempts to set the state of stream to SEALING . If the event is picked and does not find the stream to be in desired state, it postpones the seal stream processing by reposting it at the back of in-memory queue. Once the stream is set to sealing state, all active segments for the stream are sealed by calling into segment store. After this the stream is marked as SEALED in the stream metadata. Delete Stream Delete stream can be requested via an explicit API call into controller. The request first verifies if the stream is in SEALED state. Only sealed streams can be deleted and an event to this effect is posted in request stream. When the event is picked for processing, it verifies the stream state again and then proceeds to delete all segments that belong to this stream from its inception by calling into segment store. Once all segments are deleted successfully, the stream metadata corresponding to this stream is cleaned up. Stream Policy Manager As described earlier, there are two types of user defined policies that controller is responsible for enforcing, namely Automatic Scaling and Automatic Retention. Controller is not just the store for stream policy but it actively enforces those user-defined policies for their streams. Scaling infrastructure Scaling infrastructure is built in conjunction with segment stores. As controller creates new segments in segment stores, it passes user defined scaling policies to segment stores. The Segment store then monitors traffic for the said segment and reports to controller if some thresholds, as determined from policy, are breached. Controller receives these notifications via events posted in dedicated internal streams. There are two types of traffic reports that can be received for segments. First type identifies if a segment should be scaled up (split) and second type identifies if a segment should be scaled down. For segments eligible for scale up, controller immediately posts request for segment scale up in the request stream for Request Event Processor to process. However, for scale down, controller needs to wait for at least two neighboring segments to become eligible for scale down. For this purpose it simply marks the segment as cold in the metadata store. And if and when there are neighboring segments that are marked as cold, controller consolidates them and posts a scale down request for them. The scale requests processing is then performed asynchronously on the Request Event processor. Retention infrastructure The retention policy defines how much data should be retained for a given stream. This can be defined as time based or size based. To apply this policy, controller periodically collects stream-cuts for the stream and opportunistically performs truncation on previously collected stream cuts if policy dictates it. Since this is a periodic background work that needs to be performed for all streams that have a retention policy defined, there is an imperative need to fairly distribute this workload across all available controller instances. To achieve this we rely on bucketing streams into predefined sets and distributing these sets across controller instances. This is done by using zookeeper to store this distribution. Each controller instance, during bootstrap, attempts to acquire ownership of buckets. All streams under a bucket are monitored for retention opportunities by the owning controller. At each period, controller collects a new stream cut and adds it to a retention-set for the said stream. Post this it looks for candidate stream-cuts stored in retention-set which are eligible for truncation based on the defined retention policy. For example, in time based retention, the latest stream-cut older than specified retention period is chosen as the truncation point. Transaction Manager Another important role played by controller is that of transaction manager. It is responsible for creation and committing and abortion of transactions. Since controller is the central brain and agency in our cluster, and is the holder of truth about stream, the writers request controller to perform all control plane actions with respect to transactions. Controller plays active roles in providing guarantees for transactions from the time since they are created till the time they are committed or aborted. Controller tracks each transaction for their specified timeouts and if the timeout exceeds, it automatically aborts the transaction. Controller is responsible for ensuring that the transaction and a potential concurrent scale operation play well with each other and ensure all promises made with respect to either are honored and enforced. Transaction Management Diagram Client calls into controller process to create, ping commit or abort transactions. Each of these requests is received on controller and handled by the Transaction Utility module which implements the business logic for processing each request. Create transaction Writers interact with Controller to create new transactions. Controller Service passes the create transaction request to Transaction Utility module. The create transaction function in the module performs follows steps in order to create a transaction: 1. Generates a unique UUID for the transaction. 2. It fetches current active set of segments for the stream from metadata store and its corresponding epoch identifier from the history. 3. It creates a new transaction record in the zookeeper using the metadata store interface. 4. It then requests segment store to create special transaction segments that are inherently linked to the parent active segments. While creating transactions, controller ensures that parent segments are not sealed as we attempt to create corresponding transaction segments. And during the lifespan of a transaction, should a scale commence, it should wait for transactions on older epoch to finish before the scale proceeds to seal segments from old epoch. Commit Transaction Upon receiving request to commit a transaction, Controller Service passes the request to Transaction Utility module. This module first tries to mark the transaction for commit in the transaction specific metadata record via metadata store. Following this, it posts a commit event in the internal Commit Stream. Commit transaction workflow is implemented on commit event processor and thereby processed asynchronously. The commit transaction workflow checks for eligibility of transaction to be committed, and if true, it performs the commit workflow with indefinite retries until it succeeds. If the transaction is not eligible for commit, which typically happens if transaction is created on a new epoch while the old epoch is still active, then such events are reposted into the internal stream to be picked later. Once a transaction is committed successfully, the record for the transaction is removed from under its epoch root. Then if there is an ongoing scale, then it calls to attempt to complete the ongoing scale. Trying to complete scale hinges on ability to delete old epoch which can be deleted if and only if there are no outstanding active transactions against the said epoch (refer to scale workflow for more details). Abort Transaction Abort, like commit, can be requested explicitly by the application. However, abort can also be initiated automatically if the transaction\u2019s timeout elapses. Controller tracks the timeout for each and every transaction in the system and whenever timeout elapses, or upon explicit user request, transaction utility module marks the transaction for abort in its respective metadata. Post this, the event is picked for processing by abort event processor and transactions abort is immediately attempted. There is no ordering requirement for abort transaction and hence it is performed concurrently and across streams. Like commit, once the transaction is aborted, its node is deleted from its epoch root and if there is an ongoing scale, it complete scale flow is attempted. Ping Transaction Since controller has no visibility into data path with respect to data being written to segments in a transaction, Controller is unaware if a transaction is being actively worked upon or not and if the timeout elapses it may attempt to abort the transaction. To enable applications to control the destiny of a transaction, controller exposes an API to allow applications to renew transaction timeout period. This mechanism is called ping and whenever application pings a transaction, controller resets its timer for respective transaction. Transaction Timeout Management Controllers track each transaction for their timeouts. This is implemented as timer wheel service. Each transaction, upon creation gets registered into the timer service on the controller where it is created. Subsequent pings for the transaction could be received on different controller instances and timer management is transferred to latest controller instance based on ownership mechanism implemented via zookeeper. Upon timeout expiry, an automatic abort is attempted and if it is able to successfully set transaction status to abort, the abort workflow is initiated. Each transaction that a controller is monitoring for timeouts is added to this processes index. If such a controller instance fails or crashes, other controller instances will receive node failed notification and attempt to sweep all outstanding transactions from the failed instance and monitor their timeouts from that point onward. Segment Container to Host Mapping Controller is also responsible for assignment of segment containers to segment store nodes. The responsibility of maintaining this mapping befalls a single controller instance that is chosen via a leader election using zookeeper. This leader controller monitors lifecycle of segment store nodes as they are added to/removed from the cluster and performs redistribution of segment containers across available segment store nodes. This distribution mapping is stored in a dedicated ZNode. Each segment store periodically polls this znode to look for changes and if changes are found, it shuts down and relinquishes containers it no longer owns and attempts to acquire ownership of containers that are assigned to it. The details about implementation, esp with respect to how the metadata is stored and managed is already discussed here . Resources Pravega Code Other Documents","title":"Controller Service"},{"location":"controller-service/#pravega-controller-service","text":"Introduction Architecture Stream Management Cluster Management System Diagram Components Service Endpoints Controller Service Stream Metadata Store Stream Metadata Stream Store Caching Stream Buckets Controller Cluster Listener Host Store Background workers Roles and Responsibilities Stream Operations Stream State Create Stream Update Stream Scale Stream Truncate Stream Seal Stream Delete Stream Stream Policy Manager Scaling infrastructure Retention infrastructure Transaction Manager Create Transaction Commit Transaction Abort Transaction Ping Transaction Transaction Timeout Management Segment Container to Host Mapping Resources","title":"Pravega Controller Service"},{"location":"controller-service/#introduction","text":"The controller service is a core component of Pravega that implements the control plane. It acts as the central coordinator and manager for various operations performed in the cluster, mainly divided into two categories: 1) stream management 2) cluster management. The controller service, referred to simply as controller henceforth, is responsible for providing the abstraction of a stream , which is the main abstraction that Pravega exposes to applications. A stream comprises one or more segments . Each segment is an append-only data structure that stores a sequence of bytes. A segment on its own is agnostic to presence of other segments and is not aware of its logical relationship with its peer segments. The segment store, which owns and manages these segments, does not have any notion of a stream. A stream is a logical view conceptualized by Controller by composing a dynamically changing set of segments that satisfy a predefined set of logical invariants. The controller provides the stream abstraction and orchestrates all lifecycle operations on a stream while ensuring that the abstraction stays consistent. The controller plays a central role in the lifecycle of a stream: creation, modification, scaling , and deletion. It does these by maintaining metadata per stream and performs requisite operations on segments as and when necessary. For example, as part of stream\u2019s lifecycle, new segments can be created and existing segments sealed. The controller decides when to perform these operation such that streams continue to be available and consistent to the clients accessing them.","title":"Introduction "},{"location":"controller-service/#architecture","text":"The Controller Service is made up of one or more instances of stateless worker nodes. Each new controller instance can be brought up independently and to become part of pravega cluster it merely needs to point to the same Apache Zookeeper . For high availability it is advised to have more than one instance of controller service per cluster. Each controller instance is capable of working independently and uses a shared persistent store as the source of truth for all state owned and managed by controller service. We currently use Apache ZooKeeper as the store for persisting all metadata consistently. Each instance comprises various subsystems which are responsible for performing specific operations on different categories of metadata. These subsystems include different API endpoints, metadata store handles, policy managers and background workers. The controller exposes two endpoints which can be used to interact with a controller service. The first port is for providing programmatic access for pravega clients and is implemented as an RPC using gRPC. The other endpoint is for administrative operations and is implemented as a REST endpoint.","title":"Architecture "},{"location":"controller-service/#stream-management","text":"The controller owns and manages the concept of stream and is responsible for maintaining metadata and lifecycle for each stream. Specifically, it is responsible for creating, updating, scaling, truncating, sealing and deleting streams. The Stream Management can be broadly divided into three categories: 1. Stream Abstraction A stream can be viewed as a series of dynamically changing segment sets where the stream transitions from one set of consistent segments to the next. Controller is the place for creating and managing this stream abstraction. Controller decides when and how a stream transitions from one state to another and is responsible for performing these transitions while keeping the state of the stream consistent and available. These transitions are governed user-defined policies that the controller enforces. Consequently, as part of stream management, the controller also performs roles of Policy Manager for policies like retention and scale. Automated policy Management Controller is responsible for storing and enforcing user-defined Stream policies by actively monitoring the state of the stream. Presently we have two policies that users can define, namely Scaling Policy and Retention Policy . Scaling policy describes if and under what circumstances a stream should automatically scale its number of segments. Retention policy describes a policy about how much data to retain within a stream. Transaction Management Implementing transactions requires the manipulation of segments. With each transaction, Pravega creates a set of transaction segments, which are later merged onto the stream segments upon commit or discarded upon aborts. The controller performs the role of transaction manager and is responsible for creating and committing transactions on a given stream. Upon creating transactions, controller also tracks transaction timeouts and aborts transactions whose timeouts have elapsed. Details of transaction management can be found later in the document.","title":"Stream Management "},{"location":"controller-service/#cluster-management","text":"The controller is responsible for managing segment store cluster. Controller manages life cycle of segment store nodes as they are added to/removed from the cluster and performs redistribution of segment containers across available segment store nodes.","title":"Cluster Management "},{"location":"controller-service/#system-diagram","text":"The following diagram shows the main components of a controller process. We discuss the elements of the diagram in detail next. Controller Process Diagram","title":"System diagram "},{"location":"controller-service/#components","text":"","title":"Components "},{"location":"controller-service/#service-endpoints","text":"There are two ports exposed by controller: client-controller APIs and administration APIs. The client controller communication is implemented as RPC which exposes APIs to perform all stream related control plane operations. Apart from this controller also exposes an administrative API set implemented as REST. Each endpoint performs appropriate call to the Controller Service backend subsystem which has the actual implementation for various create, read, update and delete (CRUD) operations on entities owned and managed by controller. ##### gRPC Client Controller communication endpoint is implemented as a gRPC interface. The complete list of APIs can be found here . This exposes APIs used by Pravega clients (readers, writers and stream manager) and enables stream management. Requests enabled by this API include creating, modifying, and deleting streams. The underlying gRPC framework provides both synchronous and asynchronous programming models. We use the asynchronous model in our client controller interactions so that the client thread does not block on the response from the server. To be able to append to and read data from streams, writers and readers query controller to get active segment sets, successor and predecessor segments while working with a stream. For transactions, the client uses specific API calls to request controller to create and commit transactions. ##### REST For administration, the controller implements and exposes a REST interface. This includes API calls for stream management as well as other administration API primarily dealing with creation and deletion of scopes. We use swagger to describe our REST APIs. The swagger yaml file can be found here .","title":"Service Endpoints "},{"location":"controller-service/#controller-service","text":"This is the backend layer behind the controller endpoints (gRPC and REST). All the business logic required to serve controller API calls are implemented here. This layer contains handles to all other subsystems like the various store implementations (stream store, host store and checkpoint store) and background processing frameworks (task framework, event processor framework). Stores are interfaces that provide access to various types of metadata managed by Controller. Background processing frameworks are used to perform asynchronous processing that typically implement workflows involving metadata updates and requests to segment store.","title":"Controller Service"},{"location":"controller-service/#stream-metadata-store","text":"A stream is dynamically-changing sequence of segments, where regions of the routing key space map to open segments. As the set of segments of a stream changes, so does the mapping of the routing key space to segment. A set of segments is consistent if 1) union of key space regions mapping to segments in the set covers the entire key space; and 2) There is no overlap between key space regions. For example, suppose a set S = { S 1 , S 2 , S 3 }, such that: - Region [0, 0.3) maps to segment S 1 - Region [0.3, 0.6) maps to segment S 2 - Region [0.6, 1.0) maps to segment S 3 S is a consistent segment set. A stream goes through transformations as it scales over time. A stream starts with an initial set of segments that is determined by the stream configuration when created and it transitions to new sets of segments as scale operations are performed on the stream. Each generation of segments that constitute stream at any given point in time are considered to belong to an epoch. So a stream starts with initial epoch which is epoch 0 and upon each transition, it moves ahead in its epochs to describe the change in generation of segments in the stream. The controller maintains the stream store the information about all epochs that constitute a given stream and how they transition. The store is designed to optimally store and query information pertaining to segments and their inter-relationships. Apart from the epoch information, it keeps some additional metadata, such as state and its policies and ongoing transactions on the stream. Various sub-components of controller access the stored metadata for each stream via a well-defined interface . We currently have two concrete implementations of the stream store interface: in-memory and zookeeper backed stores. Both share a common base implementation that relies on stream objects for providing store-type specific implementations for all stream-specific metadata. The base implementation of stream store creates and caches these stream objects. The stream objects implement a store/stream interface. The concrete stream implementation is specific to the store type and is responsible for implementation of store specific methods to provide consistency and correctness. We have a common base implementation of all store types that provide optimistic concurrency. This base class encapsulates the logic for queries against stream store for all concrete stores that support Compare and Swap (CAS). We currently use zookeeper as our underlying store which also supports CAS. We store all stream metadata in a hierarchical fashion under stream specific znodes (ZooKeeper data nodes). For the ZooKeeper based store, we structure our metadata into different groups to support a variety of queries against this metadata. All stream specific metadata is stored under a scoped/stream root node. Queries against this metadata include, but not limited to, querying segment sets that form the stream at different points in time, segment specific information, segment predecessors and successors. Refer to stream metadata interface for details about APIs exposed by stream metadata store.","title":"Stream Metadata Store"},{"location":"controller-service/#stream-metadata","text":"Clients need information about what segments constitute a stream to start their processing and they obtain it from the epoch information the controller stores in the stream store. A reader client typically starts from the head of the stream, but it might also choose to access the stream starting from any arbitrarily interesting position. Writers on the other hand always append to the tail of the stream. Clients need ability to query and find segments at any of the three cases efficiently. To enable such queries, the stream store provides API calls to get Initial set of Segments , get Segments At Specific Time and get Current set of Segments . As mentioned earlier, a stream can transition from one set of segments (epoch) to another set of segments that constitute the stream. A stream moves from one epoch to another if there is at least one segment that is sealed and that is replaced by one or more set of segments that cover precisely the key space of the sealed segments. As clients work on streams, they may encounter the end of sealed segments and consequently need to find new segments to be able to move forward. To enable the clients to query for the next segments, the stream store exposes via the controller service efficient queries for finding immediate successors and predecessors for any arbitrary segment. To enable serving queries like those mentioned above, we need to efficiently store a time series of these segment transitions and index them against time. We store this information about the current and historical state of a stream-segments in a set of tables which are designed to optimize on aforementioned queries. Apart from segment specific metadata record, the current state of stream comprises of other metadata types that are described henceforth.","title":"Stream Metadata"},{"location":"controller-service/#tables","text":"To efficiently store and query the segment information, we have split segment data into three append only tables, namely, segment-table, history-table and index-table. Segment Table segment-info: \u27e8segmentid, time, keySpace-start, keySpace-end\u27e9 The controller stores the segment table in an append-only table with i- th row corresponding to metadata for segment id i . It is important to note that each row in the segment table is of fixed size. As new segments are added, they are assigned new segment ids in a strictly increasing order. So this table is very efficient in creating new segments and querying segment information response with O(1) processing for both these operations. History Table epoch: \u27e8time, list-of-segments-in-epoch\u27e9 The History Table stores a series of active segments as they transition from one epoch to another. Each row in the history table stores an epoch which captures a logically consistent (as defined earlier) set of segments that form the stream and are valid through the lifespan of the epoch. This table is designed to optimize queries to find set of segments that form the stream at any arbitrary time. There are three most commonly used scenarios where we want to efficiently know the set of segments that form the stream - initial set of segments, current set of segments and segments at any arbitrary time. First two queries are very efficiently answered in O(1) time because they correspond to first and last rows in the table. Since rows in the table are sorted by increasing order of time and capture time series of streams segment set changes, so we could easily perform binary search to find row which corresponds to segment sets at any arbitrary time. Index Table index: \u27e8time, offset-in-history-table\u27e9 Since history rows are of variable length, we index history rows for timestamps in the index table. This enables us to navigate the history table and perform binary search to efficiently answer queries to get segment set at any arbitrary time. We also perform binary searches on history table to determine successors of any given segment.","title":"Tables"},{"location":"controller-service/#stream-configuration","text":"Znode under which stream configuration is serialized and persisted. A stream configuration contains stream policies that need to be enforced. Scaling policy and Retention policy are supplied by the application at the time of stream creation and enforced by controller by monitoring the rate and size of data in the stream. Scaling policy describes if and when to automatically scale based on incoming traffic conditions into the stream. The policy supports two flavours - traffic as rate of events per second and traffic as rate of bytes per second. The application specifies their desired traffic rates into each segment by means of scaling policy and the supplied value is chosen to compute thresholds that determine when to scale a given stream. Retention Policy describes the amount of data that needs to be retained into pravega cluster for this stream. We support a time based and a size based retention policy where applications can choose whether they want to retain data in the stream by size or by time by choosing the appropriate policy and supplying their desired values.","title":"Stream Configuration"},{"location":"controller-service/#stream-state","text":"Znode which captures the state of the stream. It is an enum with values from Creating, Active, Updating, Scaling, Truncating, Sealing, and Sealed*.* Once Active, a stream transitions between performing a specific operation and active until it is sealed. A transition map is defined in the State class which allows and prohibits various state transitions. Stream state describes the current state of the stream. It transitions from ACTIVE to respective action based on the action being performed on the stream. For example, during scaling the state of the stream transitions from ACTIVE to SCALING and once scaling completes, it transitions back to ACTIVE. Stream state is used as a barrier to ensure only one type of operation is being performed on a given stream at any point in time. Only certain state transitions are allowed and are described in the State Transition object. Only legitimate state transitions are allowed and any attempt for disallowed transition results in appropriate exception.","title":"Stream State"},{"location":"controller-service/#truncation-record","text":"This corresponds to the stream cut which was last used to truncate the given stream. All stream segment queries superimpose the truncation record and return segments that are strictly greater than or equal to the stream cut in truncation record.","title":"Truncation Record"},{"location":"controller-service/#sealed-segments-record","text":"Since the segment table is append only, any additional information that we need to persist when a segment is sealed is stored in sealed segments record. Presently, it simple contains a map of segment number to its sealed size. Transaction Related metadata records:","title":"Sealed Segments Record"},{"location":"controller-service/#active-transactions","text":"Each new transaction is created under this Znode. This stores metadata corresponding to each transaction as ActiveTransactionRecord . Once a transaction is completed, a new node is created under the global Completed Transaction Znode and removed from under the stream specific active transaction node.","title":"Active Transactions"},{"location":"controller-service/#completed-transactions","text":"All completed transactions for all streams are moved under this single znode upon completion (via either commit or abort paths). We can subsequently garbage collect these values periodically following any collection scheme we deem fit. We have not implemented any scheme at this point though.","title":"Completed Transactions"},{"location":"controller-service/#stream-store-caching","text":"Since there could be multiple concurrent requests for a given stream being processed by same controller instance, it is suboptimal to read the value by querying zookeeper every time. So we have introduced an in-memory cache that each stream store maintains. It caches retrieved metadata per stream so that there is maximum one copy of the data per stream in the cache. We have two in-memory caches \u2013 a) a cache of multiple stream objects in the store, b) cache properties of a stream in the stream object. We have introduced a concept of operation context and at the start of any new operation a new operation context is created. The creation of a new operation context invalidates the cached entities for a stream and each entity is lazily retrieved from the store whenever requested. If a value is updated during the course of the operation, it is again invalidated in the cache so that other concurrent read/update operations on the stream get the new value for their subsequent steps.","title":"Stream Store Caching"},{"location":"controller-service/#stream-buckets","text":"To enable some scenarios, we may need our background workers to periodically work on each of the streams in our cluster to perform some specific action on them. We bring in a notion of a bucket to distribute this periodic background work across all available controller instances. For this we hash each stream into one of the predefined buckets and then distribute buckets across available controller instances. Number of buckets for a cluster is a fixed (configurable) value for the lifetime of a cluster. Controller instances map all available streams in the system into buckets and distribute buckets amongst themselves so that all long running background work can be uniformly distributed across multiple controller instances. Each bucket corresponds to a unique Znode in zookeeper. Fully qualified scoped stream name is used to compute a hashed to assign the stream to a bucket. All controller instances, upon startup, attempt to take ownership of buckets. Upon failover, ownerships are transferred as surviving nodes compete to acquire ownership of orphaned buckets. The controller instance which owns a bucket is responsible for all long running scheduled background work corresponding to all nodes under the bucket. Presently this entails running periodic workflows to capture stream-cuts (called Retention-Set) for each stream at desired frequencies.","title":"Stream Buckets"},{"location":"controller-service/#retention-set","text":"One retention set per stream is stored under the corresponding bucket/stream Znode. As we compute stream-cuts periodically, we keep preserving them under this Znode. As some automatic truncation is performed, the stream-cuts that are no longer valid are purged from this set.","title":"Retention Set"},{"location":"controller-service/#controller-cluster-listener","text":"Each node in Pravega Cluster registers itself under a cluster Znode as an ephemeral node. This includes both controller and segment store nodes. Each controller instance registers a watch on the cluster Znode to listen for cluster change notifications. These notifications are of the kind node-added and node-removed. One controller instance assumes leadership amongst all controller instances. This leader controller instance is responsible for handling segment store node change notifications. Based on change in topology, controller instance periodically rebalances segment containers to segment store node mapping. All controller instances listen for controller node change notifications. Each controller instance has multiple sub components that implement the failover-sweeper interface. Presently there are three components that implement failover sweeper interface namely, TaskSweeper, EventProcessors and TransactionSweeper . Whenever a controller instance is identified to have been removed from the cluster, the cluster listener invokes all registered failover sweepers to optimistically try to sweep all the orphaned work previously owned by the failed controller host.","title":"Controller Cluster Listener"},{"location":"controller-service/#host-store","text":"Host store interface is used to store Segment Container to Segment Store node mapping. It exposes APIs like getHostForSegment where it computes consistent hash of segment id to compute the owner Segment Container. Then based on the container-host mapping, it returns the appropriate Uri to the caller.","title":"Host Store"},{"location":"controller-service/#background-workers","text":"Controller process has two different mechanisms/frameworks for processing background work. These background works typically entail multiple steps and updates to metadata under a specific metadata root entity and potential interactions with one or more segment stores. We initially started with a simple task framework that allows ability to run tasks that take exclusive rights over a given resource (typically a stream) and allow for tasks to failover from one controller instance to another. However, this model was limiting in its scope and locking semantics and had no inherent notion of ordering of tasks as multiple tasks could race to acquire working rights (lock) on a resource concurrently and any one of them could succeed. To overcome this limitation we came up with a new infrastructure called Event Processor. Event processor is the classic eat-your-own-dog-food. It is built using pravega streams. This provides us a neat mechanism to ensure mutually exclusive and ordered processing.","title":"Background workers"},{"location":"controller-service/#task-framework","text":"Task framework is designed to run exclusive background processing per resource such that in case of controller instance failure, the work can easily failover to another controller instance and brought to completion. The framework, on its own, does not guarantee idempotent processing and the author of a task has to handle it if required. The model of tasks is defined to work on a given resource exclusively, which means no other task can run concurrently on the same resource. This is implemented by way of a persisted distributed lock implemented on zookeeper. The failover of a task is achieved by following a scheme of indexing the work a given process is performing. So if a process fails, another process will sweep all outstanding work and attempt to transfer ownership to itself. Note: upon failure of a controller process, multiple surviving controller processes can concurrently attempt sweeping of orphaned tasks. Each of them will index the task in their host-index but exactly one of them will be able to successfully acquire the lock on the resource and hence permission to process the task. The parameters for executing a task are serialized and stored under the resource. Currently we use task framework only for create stream tasks. All the other background processing is done using event processor framework.","title":"Task Framework"},{"location":"controller-service/#event-processor-framework","text":"Event processors framework is a background worker sub system which reads events from an internal stream and processes it, hence the name event processor. All event processors in our system provide at least once processing guarantee. And in its basic flavor, the framework also provides strong ordering guarantees. But we also have different subtypes of event processors that allow concurrent processing. We create different event processors for different kinds of work. Presently we have three different event processors in our system for committing transaction, aborting transactions and processing stream specific requests like scale update seal etc. Each controller instance has one event processor of each type. The event processor framework allows for multiple readers to be created per event processor. All readers for a specific event processor across controller instances share the same reader group, which guarantees mutually exclusive distribution of work across controller instances. Each reader gets a dedicated thread where it reads the event, calls for its processing and upon completion of processing, updates its checkpoint. Events are posted in the event processor specific stream and are routed to specific segments based on using scoped stream name as the routing key. We have two flavors of event processors, one that performs serial processing, which essentially means it reads an event and initiates its processing and waits on it to complete before moving on to next event. This provides strong ordering guarantees in processing. And it checkpoints after processing each event. Commit transaction is implemented using this base flavor of event processor. The degree of parallelism for processing these events is upper bounded by number of segments in the internal stream and lower bounded by number of readers. Multiple events from across different streams could land up in the same segment and since we perform serial processing, serial processing has the drawback that processing stalls or flooding of events from one stream could adversely impact latencies for unrelated streams. To overcome these drawbacks we designed Concurrent Event Processor as an overlay on Serial Event processor. Concurrent event processor, as name implies, allows us to process multiple events concurrently. Here the reader thread reads an event, schedules it\u2019s asynchronous processing and returns to read the next event. There is a ceiling on number of events that are concurrently processed at any point in time and as processing of some event completes, newer events are allowed to be fetched. The checkpoint scheme here becomes slightly more involved because we want to guarantee at least once processing. However, with concurrent processing the ordering guarantees get broken. However, it is important to note that we only need ordering guarantees for processing events from a stream and not across streams. In order to satisfy ordering guarantee, we overlay Concurrent Event Processor with Serialized Request Handler, which queues up events from the same stream in an in-memory queue and processes them in order. Commit Transaction processing is implemented on a dedicated Serial Event Processor because we want strong Commit ordering while ensuring that commit does not interfere with processing of other kinds of requests on the stream. Abort Transaction processing is implemented on a dedicated Concurrent Event Processor which performs abort processing on transactions from across streams concurrently. All other requests for streams is implemented on a Serialized Request Handler which ensures exactly one request per stream is being processed at any given time and there is ordering guarantee within request processing. However, it allows for concurrent requests from across streams to go on concurrently. Workflows like scale, truncation, seal, update and delete stream are implemented for processing on the Request Event processor.","title":"Event Processor Framework"},{"location":"controller-service/#roles-and-responsibilities","text":"","title":"Roles and Responsibilities"},{"location":"controller-service/#stream-operations","text":"Controller is the store of truth for all stream related metadata. Pravega clients (EventStreamReaders and EventStreamWriters), in conjunction with the controller, ensure that stream invariants are satisfied and honored as they work on streams. The controller maintains the metadata of streams, including the entire history of segments. Client accessing a stream need to contact the controller to obtain information about segments. Clients query controller in order to know how to navigate streams. For this purpose controller exposes appropriate APIs to get active segments, successors, predecessors and segment information and Uris. These queries are served using metadata stored and accessed via stream store interface. Controller also provides workflows to modify state and behavior of the stream. These workflows include create, scale, truncation, update, seal, and delete. These workflows are invoked both via direct APIs and in some cases as applicable via background policy manager (auto-scale and retention). Request Processing Flow","title":"Stream Operations"},{"location":"controller-service/#create-stream","text":"Create stream is implemented as a task on Task Framework. Create stream workflow first creates initial stream metadata with stream state set to CREATING*.* Following this, it identifies segment containers that should own and create segments for this stream and calls create-segment concurrently. Once all create segments complete, the create stream task completes thus moving the stream to ACTIVE state. All failures are retried few times with exponential backoffs. However, if it is unable to complete any step, the stream is left dangling in CREATING state. * *","title":"Create Stream"},{"location":"controller-service/#update-stream","text":"Update stream is implemented as a task on Serialized Request Handler/Concurrent Event Processor framework. Update stream is invoked by an explicit API call into controller. It first posts an Update Request event into request-stream. Following that it tries to create a temporary update property. If it fails to create the temporary update property, the request is failed and the caller is notified of the failure to update a stream due to conflict with another ongoing update. The event is picked by Request Event processor. When the processing starts, the update stream task expects to find the temporary update stream property to be present. If it does not find the property, the update processing is delayed by pushing event the back in the in-memory queue until it deems the event expired. If it finds the property to be updated during this period, before the expiry, the event is processed and update stream operation is performed. Now that the processing starts, it first sets the state to UPDATING. Following this the stream configuration is updated in the metadata store followed by notifying segment stores for all active segments of the stream about change in policy. Now the state is reset to ACTIVE.","title":"Update Stream"},{"location":"controller-service/#scale-stream","text":"Scale can be invoked either by explicit API call (referred to as manual scale) or performed automatically based on scale policy (referred to as auto-scale). We first write the event followed by updating the segment table by creating new entries for desired segments to be created. This step is idempotent and ensures that if an existing ongoing scale operation is in progress, then this attempt to start a new scale fails. The start of processing is similar to mechanism followed in update stream. If metadata is updated, the event processes and proceeds with executing the task. If the metadata is not updated within the desired time frame, the event is discarded. Once scale processing starts, it first sets the stream state SCALING. This is followed by creating new segments in segment stores. After successfully creating new segments, it updates the history table with a partial record corresponding to new epoch which contains list of segments as they would appear post scale. Each new epoch creation also creates a new root epoch node under which metadata for all transactions from that epoch reside. So as the scale is performed, there would be a node corresponding to old epoch and now there will also be a root node for new epoch. Any transaction creation from this point on will be done against new epoch. Now the workflow attempts to complete scale by opportunistically attempting to delete the old epoch. Old epoch can be deleted if and only if there are no transactions under its tree. Once we are sure there are no transactions on old epoch, we can proceed with sealing old segments and completing the scale. After old segments are sealed successfully, the partial record in history table is now completed whereby completing the scale workflow. The state is now reset to ACTIVE.","title":"Scale Stream"},{"location":"controller-service/#truncate-stream","text":"Truncate follows similar mechanism to update and has a temporary stream-property for truncation that is used to supply input for truncate stream. Once truncate workflow identifies that it can proceed, it first sets the state to TRUNCATING. Truncate workflow then looks at the requested stream-cut, and checks if it is greater than or equal to the existing truncation point, only then is it a valid input for truncation and the workflow commences. The truncation workflow takes the requested stream-cut and computes all segments that are to be deleted as part of this truncation request. It then calls into respective segment stores to delete identified segments. Post deletion, we call truncate on segments that are described in the stream cut at the offsets as described in the stream cut. Following this the truncation record is updated with the new truncation point and deleted segments. The state is reset to ACTIVE.","title":"Truncate Stream"},{"location":"controller-service/#seal-stream","text":"Seal stream can be requested via an explicit API call into controller. It first posts a seal-stream event into request stream followed by attempts to set the state of stream to SEALING . If the event is picked and does not find the stream to be in desired state, it postpones the seal stream processing by reposting it at the back of in-memory queue. Once the stream is set to sealing state, all active segments for the stream are sealed by calling into segment store. After this the stream is marked as SEALED in the stream metadata.","title":"Seal Stream"},{"location":"controller-service/#delete-stream","text":"Delete stream can be requested via an explicit API call into controller. The request first verifies if the stream is in SEALED state. Only sealed streams can be deleted and an event to this effect is posted in request stream. When the event is picked for processing, it verifies the stream state again and then proceeds to delete all segments that belong to this stream from its inception by calling into segment store. Once all segments are deleted successfully, the stream metadata corresponding to this stream is cleaned up.","title":"Delete Stream"},{"location":"controller-service/#stream-policy-manager","text":"As described earlier, there are two types of user defined policies that controller is responsible for enforcing, namely Automatic Scaling and Automatic Retention. Controller is not just the store for stream policy but it actively enforces those user-defined policies for their streams.","title":"Stream Policy Manager"},{"location":"controller-service/#scaling-infrastructure","text":"Scaling infrastructure is built in conjunction with segment stores. As controller creates new segments in segment stores, it passes user defined scaling policies to segment stores. The Segment store then monitors traffic for the said segment and reports to controller if some thresholds, as determined from policy, are breached. Controller receives these notifications via events posted in dedicated internal streams. There are two types of traffic reports that can be received for segments. First type identifies if a segment should be scaled up (split) and second type identifies if a segment should be scaled down. For segments eligible for scale up, controller immediately posts request for segment scale up in the request stream for Request Event Processor to process. However, for scale down, controller needs to wait for at least two neighboring segments to become eligible for scale down. For this purpose it simply marks the segment as cold in the metadata store. And if and when there are neighboring segments that are marked as cold, controller consolidates them and posts a scale down request for them. The scale requests processing is then performed asynchronously on the Request Event processor.","title":"Scaling infrastructure"},{"location":"controller-service/#retention-infrastructure","text":"The retention policy defines how much data should be retained for a given stream. This can be defined as time based or size based. To apply this policy, controller periodically collects stream-cuts for the stream and opportunistically performs truncation on previously collected stream cuts if policy dictates it. Since this is a periodic background work that needs to be performed for all streams that have a retention policy defined, there is an imperative need to fairly distribute this workload across all available controller instances. To achieve this we rely on bucketing streams into predefined sets and distributing these sets across controller instances. This is done by using zookeeper to store this distribution. Each controller instance, during bootstrap, attempts to acquire ownership of buckets. All streams under a bucket are monitored for retention opportunities by the owning controller. At each period, controller collects a new stream cut and adds it to a retention-set for the said stream. Post this it looks for candidate stream-cuts stored in retention-set which are eligible for truncation based on the defined retention policy. For example, in time based retention, the latest stream-cut older than specified retention period is chosen as the truncation point.","title":"Retention infrastructure"},{"location":"controller-service/#transaction-manager","text":"Another important role played by controller is that of transaction manager. It is responsible for creation and committing and abortion of transactions. Since controller is the central brain and agency in our cluster, and is the holder of truth about stream, the writers request controller to perform all control plane actions with respect to transactions. Controller plays active roles in providing guarantees for transactions from the time since they are created till the time they are committed or aborted. Controller tracks each transaction for their specified timeouts and if the timeout exceeds, it automatically aborts the transaction. Controller is responsible for ensuring that the transaction and a potential concurrent scale operation play well with each other and ensure all promises made with respect to either are honored and enforced. Transaction Management Diagram Client calls into controller process to create, ping commit or abort transactions. Each of these requests is received on controller and handled by the Transaction Utility module which implements the business logic for processing each request.","title":"Transaction Manager"},{"location":"controller-service/#create-transaction","text":"Writers interact with Controller to create new transactions. Controller Service passes the create transaction request to Transaction Utility module. The create transaction function in the module performs follows steps in order to create a transaction: 1. Generates a unique UUID for the transaction. 2. It fetches current active set of segments for the stream from metadata store and its corresponding epoch identifier from the history. 3. It creates a new transaction record in the zookeeper using the metadata store interface. 4. It then requests segment store to create special transaction segments that are inherently linked to the parent active segments. While creating transactions, controller ensures that parent segments are not sealed as we attempt to create corresponding transaction segments. And during the lifespan of a transaction, should a scale commence, it should wait for transactions on older epoch to finish before the scale proceeds to seal segments from old epoch.","title":"Create transaction"},{"location":"controller-service/#commit-transaction","text":"Upon receiving request to commit a transaction, Controller Service passes the request to Transaction Utility module. This module first tries to mark the transaction for commit in the transaction specific metadata record via metadata store. Following this, it posts a commit event in the internal Commit Stream. Commit transaction workflow is implemented on commit event processor and thereby processed asynchronously. The commit transaction workflow checks for eligibility of transaction to be committed, and if true, it performs the commit workflow with indefinite retries until it succeeds. If the transaction is not eligible for commit, which typically happens if transaction is created on a new epoch while the old epoch is still active, then such events are reposted into the internal stream to be picked later. Once a transaction is committed successfully, the record for the transaction is removed from under its epoch root. Then if there is an ongoing scale, then it calls to attempt to complete the ongoing scale. Trying to complete scale hinges on ability to delete old epoch which can be deleted if and only if there are no outstanding active transactions against the said epoch (refer to scale workflow for more details).","title":"Commit Transaction"},{"location":"controller-service/#abort-transaction","text":"Abort, like commit, can be requested explicitly by the application. However, abort can also be initiated automatically if the transaction\u2019s timeout elapses. Controller tracks the timeout for each and every transaction in the system and whenever timeout elapses, or upon explicit user request, transaction utility module marks the transaction for abort in its respective metadata. Post this, the event is picked for processing by abort event processor and transactions abort is immediately attempted. There is no ordering requirement for abort transaction and hence it is performed concurrently and across streams. Like commit, once the transaction is aborted, its node is deleted from its epoch root and if there is an ongoing scale, it complete scale flow is attempted.","title":"Abort Transaction"},{"location":"controller-service/#ping-transaction","text":"Since controller has no visibility into data path with respect to data being written to segments in a transaction, Controller is unaware if a transaction is being actively worked upon or not and if the timeout elapses it may attempt to abort the transaction. To enable applications to control the destiny of a transaction, controller exposes an API to allow applications to renew transaction timeout period. This mechanism is called ping and whenever application pings a transaction, controller resets its timer for respective transaction.","title":"Ping Transaction"},{"location":"controller-service/#transaction-timeout-management","text":"Controllers track each transaction for their timeouts. This is implemented as timer wheel service. Each transaction, upon creation gets registered into the timer service on the controller where it is created. Subsequent pings for the transaction could be received on different controller instances and timer management is transferred to latest controller instance based on ownership mechanism implemented via zookeeper. Upon timeout expiry, an automatic abort is attempted and if it is able to successfully set transaction status to abort, the abort workflow is initiated. Each transaction that a controller is monitoring for timeouts is added to this processes index. If such a controller instance fails or crashes, other controller instances will receive node failed notification and attempt to sweep all outstanding transactions from the failed instance and monitor their timeouts from that point onward.","title":"Transaction Timeout Management"},{"location":"controller-service/#segment-container-to-host-mapping","text":"Controller is also responsible for assignment of segment containers to segment store nodes. The responsibility of maintaining this mapping befalls a single controller instance that is chosen via a leader election using zookeeper. This leader controller monitors lifecycle of segment store nodes as they are added to/removed from the cluster and performs redistribution of segment containers across available segment store nodes. This distribution mapping is stored in a dedicated ZNode. Each segment store periodically polls this znode to look for changes and if changes are found, it shuts down and relinquishes containers it no longer owns and attempts to acquire ownership of containers that are assigned to it. The details about implementation, esp with respect to how the metadata is stored and managed is already discussed here .","title":"Segment Container to Host Mapping"},{"location":"controller-service/#resources","text":"Pravega Code Other Documents","title":"Resources"},{"location":"faq/","text":"Frequently Asked Questions What is Pravega? Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. See here for more definitions of terms related to Pravega. What does \"Pravega\" mean? \"Pravega\" is a word from Sanskrit referring to \"good speed\". Is Pravega similiar to systems such as Kafka and Kinesis? Pravega is built from the ground up as an enterprise grade storage system to support features such as exactly once, durability etc. Pravega is an ideal store for streaming data, data from real-time applications and IoT data. How can I participate in open source? Disruptive innovation is accelerated by open source. When Pravega was created, there was no question it made sense to make it open source. We welcome contributions from experienced and new developers alike. Check out the code in Github . More detail about how to get involved can be found here . How do I get started? Read the Getting Started guide for more information, and also visit sample-apps repo for some sample applications. I am stuck. Where can I get help? Don\u2019t hesitate to ask! Contact the developers and community on the mailing lists if you need any help. See Join the Community for more details. Does Pravega support exactly once semantics? Absolutely. See Key Features for a discussion on how Pravega supports exactly once semantics. How does Pravega work with stream processors such as Apache Flink? So many features of Pravega make it ideal for stream processors. First, Pravega comes out of the box with a Flink connector. Critically, Pravega provides exactly once semantics, making it much easier to develop accurate stream processing applications. The combination of exactly once semantics, durable storage and transactions makes Pravega an ideal way to chain Flink jobs together, providing end-end consistency and exactly once semantics. See here for a list of key features of Pravega. How does auto scaling work between stream processors and Flink Auto scaling is a feature of Pravega where the number of segments in a stream changes based on the ingestion rate of data. If data arrives at a faster rate, Pravega increases the capacity of a stream by adding segments. When the data rate falls, Pravega can reduce capacity of a stream. As Pravega scales up and down the capacity of a stream, applications, such as a Flink job can observe this change and respond by adding or reducing the number of job instances consuming the stream. See the \"Auto Scaling\" section in Key Features for more discussion of auto scaling. What consistency guarantees does Pravega provide? Pravega makes several guarantees. Durability - once data is acknowledged to a client, Pravega guarantees it is protected. Ordering - events with the same routing key will always be read in the order they were written. Exactly once - data written to Pravega will not be duplicated. Why is supporting consistency and durability so important for storage systems such as Pravega? Primarily because it makes building applications easier. Consistency and durability are key for supporting exactly once semantics. Without exactly once semantics, it is difficult to build fault tolerant applications that consistency produce accurate results. See Key Features for a discussion on consistency and durability guarantees play a role in Pravega's support of exactly once semantics. Does Pravega support transactions? Yes. The Pravega API allows an application to create a transaction on a stream and write data to the transaction. The data is durably stored, just like any other data written to Pravega. When the application chooses, it can commit or abort the transaction. When a transaction is committed, the data in the transaction is atomically appended to the stream. See here for more details on Pravega's transaction support. Does Pravega support transactions across different routing keys? Yes. A transaction in Pravega is itself a stream; it can have 1 or more segments and data written to the transaction is placed into the segment associated with the data's routing key. When the transaction is committed, the transaction data is appended to the appropriate segment in the stream. Do I need HDFS installed in order to use Pravega? Yes. Normally, you would deploy an HDFS for Pravega to use as its Tier 2 storage. However, for simple test/dev environments, the so-called standAlone version of Pravega provides its own simulated HDFS. See the Running Pravega guide for more details. Which Tier 2 storage systems does Pravega support? Pravega is designed to support various types of Tier 2 storage systems. Currently we have implemented HDFS as the first embodiment of Tier 2 storage. What distributed computing primitives does Pravega provide? Pravega provides an API construct called StateSynchronizer. Using the StateSynchronizer, a developer can use Pravega to build synchronized shared state between multiple processes. This primitive can be used to build all sorts of distributed computing solutions such as shared configuration, leader election, etc. See the \"Distributed Computing Primitive\" section in Key Features for more details. What hardware do you recommend for Pravega? The Segment Store requires faster access to storage and more memory for its cache. It can run on 1 GB memory and 2 core CPU. 10 GB is a good start for storage. The Controller is less resource intensive, 1 CPU and 0.5 GB memory is a good start.","title":"Pravega FAQ"},{"location":"faq/#frequently-asked-questions","text":"What is Pravega? Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. See here for more definitions of terms related to Pravega. What does \"Pravega\" mean? \"Pravega\" is a word from Sanskrit referring to \"good speed\". Is Pravega similiar to systems such as Kafka and Kinesis? Pravega is built from the ground up as an enterprise grade storage system to support features such as exactly once, durability etc. Pravega is an ideal store for streaming data, data from real-time applications and IoT data. How can I participate in open source? Disruptive innovation is accelerated by open source. When Pravega was created, there was no question it made sense to make it open source. We welcome contributions from experienced and new developers alike. Check out the code in Github . More detail about how to get involved can be found here . How do I get started? Read the Getting Started guide for more information, and also visit sample-apps repo for some sample applications. I am stuck. Where can I get help? Don\u2019t hesitate to ask! Contact the developers and community on the mailing lists if you need any help. See Join the Community for more details. Does Pravega support exactly once semantics? Absolutely. See Key Features for a discussion on how Pravega supports exactly once semantics. How does Pravega work with stream processors such as Apache Flink? So many features of Pravega make it ideal for stream processors. First, Pravega comes out of the box with a Flink connector. Critically, Pravega provides exactly once semantics, making it much easier to develop accurate stream processing applications. The combination of exactly once semantics, durable storage and transactions makes Pravega an ideal way to chain Flink jobs together, providing end-end consistency and exactly once semantics. See here for a list of key features of Pravega. How does auto scaling work between stream processors and Flink Auto scaling is a feature of Pravega where the number of segments in a stream changes based on the ingestion rate of data. If data arrives at a faster rate, Pravega increases the capacity of a stream by adding segments. When the data rate falls, Pravega can reduce capacity of a stream. As Pravega scales up and down the capacity of a stream, applications, such as a Flink job can observe this change and respond by adding or reducing the number of job instances consuming the stream. See the \"Auto Scaling\" section in Key Features for more discussion of auto scaling. What consistency guarantees does Pravega provide? Pravega makes several guarantees. Durability - once data is acknowledged to a client, Pravega guarantees it is protected. Ordering - events with the same routing key will always be read in the order they were written. Exactly once - data written to Pravega will not be duplicated. Why is supporting consistency and durability so important for storage systems such as Pravega? Primarily because it makes building applications easier. Consistency and durability are key for supporting exactly once semantics. Without exactly once semantics, it is difficult to build fault tolerant applications that consistency produce accurate results. See Key Features for a discussion on consistency and durability guarantees play a role in Pravega's support of exactly once semantics. Does Pravega support transactions? Yes. The Pravega API allows an application to create a transaction on a stream and write data to the transaction. The data is durably stored, just like any other data written to Pravega. When the application chooses, it can commit or abort the transaction. When a transaction is committed, the data in the transaction is atomically appended to the stream. See here for more details on Pravega's transaction support. Does Pravega support transactions across different routing keys? Yes. A transaction in Pravega is itself a stream; it can have 1 or more segments and data written to the transaction is placed into the segment associated with the data's routing key. When the transaction is committed, the transaction data is appended to the appropriate segment in the stream. Do I need HDFS installed in order to use Pravega? Yes. Normally, you would deploy an HDFS for Pravega to use as its Tier 2 storage. However, for simple test/dev environments, the so-called standAlone version of Pravega provides its own simulated HDFS. See the Running Pravega guide for more details. Which Tier 2 storage systems does Pravega support? Pravega is designed to support various types of Tier 2 storage systems. Currently we have implemented HDFS as the first embodiment of Tier 2 storage. What distributed computing primitives does Pravega provide? Pravega provides an API construct called StateSynchronizer. Using the StateSynchronizer, a developer can use Pravega to build synchronized shared state between multiple processes. This primitive can be used to build all sorts of distributed computing solutions such as shared configuration, leader election, etc. See the \"Distributed Computing Primitive\" section in Key Features for more details. What hardware do you recommend for Pravega? The Segment Store requires faster access to storage and more memory for its cache. It can run on 1 GB memory and 2 core CPU. 10 GB is a good start for storage. The Controller is less resource intensive, 1 CPU and 0.5 GB memory is a good start.","title":"Frequently Asked Questions"},{"location":"getting-started/","text":"Getting started The best way to get to know Pravega is to start it up and run a sample Pravega application. Running Pravega is simple Verify the following prerequisite : Java 8 Download Pravega Download the Pravega release from the github releases page . If you prefer to build Pravega yourself, you can download the code and run ./gradlew distribution . More details are shown in the Pravega README.md . $ tar xfvz pravega- version .tgz Run Pravega in standalone mode This launches all the components of Pravega on your local machine. NOTE: this is for testing/demo purposes only, do not use this mode of deployment in Production! More options for Running Pravega are covered in the running Pravega guide. $ cd pravega- version $ bin/pravega-standalone That's it. Pravega should be up and running very soon. You can find additional ways to run Pravega in Running Pravega . Running a sample Pravega App is simple too Pravega maintains a separate github repository for sample applications. It is located at: https://github.com/pravega/pravega-samples Lets download and run the \"Hello World\" Pravega sample reader and writer apps. Pravega dependencies will be pulled from maven central. Note: The samples can also use a locally compiled version of Pravega. For more information about this see the note on maven publishing in the README.md . Download the Pravega-Samples git repo $ git clone https://github.com/pravega/pravega-samples $ cd pravega-samples Generate the scripts to run the apps $ ./gradlew installDist Run the sample \"HelloWorldWriter\" This runs a simple Java application that writes a \"hello world\" message as an event into a Pravega stream. $ cd pravega-samples/pravega-client-examples/build/install/pravega-client-examples $ bin/helloWorldWriter Example HelloWorldWriter output ... Writing message: hello world with routing-key: helloRoutingKey to stream examples / helloStream ... See the readme.md file in the standalone-examples for more details on running the HelloWorldWriter with different parameters Run the sample \"HelloWorldReader\" $ cd pravega-samples/pravega-client-examples/build/install/pravega-client-examples $ bin/helloWorldReader Example HelloWorldReader output ... Reading all the events from examples/helloStream ... Read event hello world No more events from examples/helloStream ... See the readme.md file in the pravega-client-examples for more details on running the HelloWorldReader application","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"The best way to get to know Pravega is to start it up and run a sample Pravega application.","title":"Getting started"},{"location":"getting-started/#running-pravega-is-simple","text":"Verify the following prerequisite : Java 8 Download Pravega Download the Pravega release from the github releases page . If you prefer to build Pravega yourself, you can download the code and run ./gradlew distribution . More details are shown in the Pravega README.md . $ tar xfvz pravega- version .tgz Run Pravega in standalone mode This launches all the components of Pravega on your local machine. NOTE: this is for testing/demo purposes only, do not use this mode of deployment in Production! More options for Running Pravega are covered in the running Pravega guide. $ cd pravega- version $ bin/pravega-standalone That's it. Pravega should be up and running very soon. You can find additional ways to run Pravega in Running Pravega .","title":"Running Pravega is simple"},{"location":"getting-started/#running-a-sample-pravega-app-is-simple-too","text":"Pravega maintains a separate github repository for sample applications. It is located at: https://github.com/pravega/pravega-samples Lets download and run the \"Hello World\" Pravega sample reader and writer apps. Pravega dependencies will be pulled from maven central. Note: The samples can also use a locally compiled version of Pravega. For more information about this see the note on maven publishing in the README.md . Download the Pravega-Samples git repo $ git clone https://github.com/pravega/pravega-samples $ cd pravega-samples Generate the scripts to run the apps $ ./gradlew installDist Run the sample \"HelloWorldWriter\" This runs a simple Java application that writes a \"hello world\" message as an event into a Pravega stream. $ cd pravega-samples/pravega-client-examples/build/install/pravega-client-examples $ bin/helloWorldWriter Example HelloWorldWriter output ... Writing message: hello world with routing-key: helloRoutingKey to stream examples / helloStream ... See the readme.md file in the standalone-examples for more details on running the HelloWorldWriter with different parameters Run the sample \"HelloWorldReader\" $ cd pravega-samples/pravega-client-examples/build/install/pravega-client-examples $ bin/helloWorldReader Example HelloWorldReader output ... Reading all the events from examples/helloStream ... Read event hello world No more events from examples/helloStream ... See the readme.md file in the pravega-client-examples for more details on running the HelloWorldReader application","title":"Running a sample Pravega App is simple too"},{"location":"javadoc/","text":"Java API Reference Clients A Writer is a client that creates Events and publishes them into Streams. A Reader is a client that Consumes events from Streams. We provide a Java library, which implements a convenient API for Writer and Reader applications to use. The client library encapsulates the wire protocol that is used to convey requests and responses between Pravega Clients and the Pravega service. Writer and Reader API","title":"Java API Reference"},{"location":"javadoc/#java-api-reference","text":"","title":"Java API Reference"},{"location":"javadoc/#clients","text":"A Writer is a client that creates Events and publishes them into Streams. A Reader is a client that Consumes events from Streams. We provide a Java library, which implements a convenient API for Writer and Reader applications to use. The client library encapsulates the wire protocol that is used to convey requests and responses between Pravega Clients and the Pravega service. Writer and Reader API","title":"Clients"},{"location":"join-community/","text":"Join the Pravega Community Slack Channel User Groups Developer Mailing List","title":"Join the Community"},{"location":"join-community/#join-the-pravega-community","text":"Slack Channel","title":"Join the Pravega Community"},{"location":"join-community/#user-groups","text":"","title":"User Groups"},{"location":"join-community/#developer-mailing-list","text":"","title":"Developer Mailing List"},{"location":"key-features/","text":"Pravega Key Features This document explains some of the key features of Pravega. It may be advantageous if you are already familiar with the core concepts of Pravega . Pravega Design Principles Pravega was designed to support the new generation of streaming applications: applications that deal with a large amount of data arriving continuously that also need to generate an accurate analysis of that data in the face of late arriving data, data arriving out of order and failure conditions. There are several open source tools to enable developers to build such applications, including Apache Flink, Apache Beam, Spark Streaming and others. To date, these applications used systems such as Apache Kafka, Apache ActiveMQ, RabbitMQ, Apache Cassandra, and Apache HDFS to ingest and store data. We envision instead a unification of the two concepts and our work focuses on both ingesting and storing stream data. Pravega approaches streaming applications from a storage perspective. It enables applications to ingest stream data continuously and storing it permanently. Such stream data can be accessed with low latency (order of milliseconds), but also months, years ahead as part of analyzing historical data. The design of Pravega incorporates lessons learned from using the Lambda architecture to build streaming applications and the challenges to deploy streaming applications at scale that consistently deliver accurate results in a fault tolerant manner. The Pravega architecture provides strong durability and consistency guarantees, delivering a rock solid foundation to build streaming applications upon. With the Lambda architecture, the developer uses a complex combination of middleware tools that include batch style middleware mainly influenced by Hadoop and continuous processing tools like Storm, Samza, Kafka and others. In this architecture, batch processing is used to deliver accurate, but potentially out of date analysis of data. The second path processes data as it is ingested, and in principle the results are innacurate, which justifies the first batch path. With this approach, there are two copies of the application logic because the programming models of the speed layer are different than those used in the batch layer. An implementation of the Lambda architecture can be difficult to maintain and manage in production. This style of big data application design consequently has been losing traction. A different kind of architecture has been gaining traction recently that does not rely on a batch processing data path. This architecture is called Kappa. The Kappa architecture style is a reaction to the complexity of the Lambda architecture and relies on components that are designed for streaming, supporting stronger semantics and delivering both fast and accurate data analysis. The Kappa architecture is a simpler approach: There is only one data path to execute, and one implementation of the application logic to maintain, not two. With the right tools, built for the demands of processing streaming data in a fast and accurate fashion, it becomes simpler to design and run applications in the space of IoT, connected cars, finance, risk management, online services, etc. With the right tooling, it is possible to build such pipelines and serve applications that present high volume and demand low latency. Applications often require more than one stage of processing. Any practical system for stream analytics must be able to accomodate the composition of stages in the form of data pipelines: With data pipelines, it is important to think of guarantees end-to-end rather than on a per componenent basis. For example, it is not sufficient that one stage guarantees exactly-once semantics while at least one other does not make such a guarantee. Our goal in Pravega is enable the design and implementation of data pipelines with strong guarantees end-to-end. Pravega - Storage Reimagined for a Streaming World Pravega introduces a new storage primitive, a stream, that matches continuous processing of unbounded data. In Pravega, a stream is a named, durable, append-only and unbounded sequence of bytes. With this primitive, and the key features discussed in this document, Pravega is an ideal component to combine with stream processing engines such as Flink to build streaming applications. Because of Pravega's key features, we imagine that it will be the fundamental storage primitive for a new generation of streaming-oriented middleware. Let's examine the key features of Pravega. Exactly Once Semantics By exactly once semantics we mean that Pravega ensures that data is not duplicated and no event is missed despite failures. Of course, this statement comes with a number of caveats, like any other system that promises exactly-once semantics, but let's not dive into the gory details here. An important consideration is that exactly-once semantics is a natural part of Pravega and has been a goal and part of the design from day zero. To achieve exactly once semantics, Pravega Streams are durable, ordered, consistent and transactional. We discuss durable and transactional in separate sections below. By ordering, we mean that data is observed by readers in the order it is written. In Pravega, data is written along with an application-defined routing key. Pravega makes ordering guarantees in terms of routing keys. Two pieces of data with the same routing key will always be read by a Reader in the order they were written. Pravega's ordering guarantees allow data reads to be replayed (e.g. when applications crash) and the results of replaying the reads will be the same. By consistency, we mean all Readers see the same ordered view of data for a given routing key, even in the face of failure. Systems that are \"mostly consistent\" are not sufficient for building accurate data processing. Systems that provide \"at least once\" semantics might present duplication. In such systems, a data producer might write the same data twice in some scenarios. In Pravega, writes are idempotent, rewrites done as a result of reconnection don't result in data duplication. Note that we make no guarantee when the data coming from the source already contains duplicates. Written data is opaque to Pravega and it makes no attempt to remove existing duplicates. We have not limited our focus to exactly-once semantics for writing, however. We also provide, and are actively working on extending the features, that enable exactly-once end-to-end for a data pipeline. The strong consistency guarantees that the Pravega store provides along with the semantics of a data analytics engine like Flink enables such end-to-end guarantees. Auto Scaling Unlike systems with static partitioning, Pravega can automatically scale individual data streams to accommodate changes in data ingestion rate. Imagine an IoT application with millions of devices feeding thousands of data streams with information about those devices. Imagine a pipeline of Flink jobs that process those streams to derive business value from all that raw IoT data: predicting device failures, optimizing service delivery through those devices, or tailoring a customer's experience when interacting with those devices. Building such an application at scale is difficult without having the components be able to scale automatically as the rate of data increases and decreases. With Pravega, it is easy to elastically and independently scale data ingestion, storage and processing \u2013 orchestrating the scaling of every component in a data pipeline. Pravega's support of auto scaling starts with the idea that Streams are partitioned into Stream Segments. A Stream may have 1 or more Stream Segments; recall that a Stream Segment is a partition of the Stream associated with a range of routing keys. Any data written into the Stream is written to the Stream Segment associated with the data's routing key. Writers use application-meaningful routing keys like customer-id, timestamp, machine-id, etc to make sure like data is grouped together. A Stream Segment is the fundamental unit of parallelism in Pravega Streams. A Stream with multiple Stream Segments can support more parallelism of data writes; multiple Writers writing data into the different Stream Segments potentially involving all the Pravega Servers in the cluster. On the Reader side, the number of Stream Segments represents the maximum degree of read parallelism possible. If a Stream has N Stream Segments, then a ReaderGroup with N Readers can consume from the Stream in parallel. Increase the number of Stream Segments, you can increase the number of Readers in the ReaderGroup to increase the scale of processing the data from that Stream. And of course if the number of Stream Segments decreases, it would be a good idea to reduce the number of Readers. A Stream can be configured to grow the number of Stream Segments as more data is written to the Stream, and to shrink when data volume drops off. We refer to this configuration as the Stream's Service Level Objective or SLO. Pravega monitors the rate of data input to the Stream and uses the SLO to add or remove Stream Segments from a Stream. Segments are added by splitting a Segment. Segments are removed by merging two Segments. See AutoScaling: The number of Stream Segments can vary over time , for more detail on how Pravega manages Stream Segments. It is possible to coordinate the auto scaling of Streams in Pravega with application scale out (in the works). Using metadata available from Pravega, applications can configure the scaling of their application components; for example, to drive the number of instances of a Flink job. Alternatively, you could use software such as Cloud Foundry, Mesos/Marathon, Kubernetes or the Docker stack to deploy new instances of an application to react to increased parallelism at the Pravega level, or to terminate instances as Pravega scales down in response to reduced rate of data ingestion. Distributed Computing Primitive Pravega is great for distributed applications, such as micro-services; it can be used as a data storage mechanism, for messaging between micro-services and for other distributed computing services such as leader election. State Synchronizer, a part of the Pravega API, is the basis of sharing state across a cluster with consistency and optimistic concurrency. State Synchronizer is based on a fundamental conditional write operation in Pravega, so that data is written only if it would appear at a given position in the Stream. If a conditional write operation cannot meet the condition, it fails. State Synchronizer is therefore a strong synchronization primitive that can be used for shared state in a cluster, membership management, leader election and other distributed computing scenarios. You can learn more about the State Synchronizer here . Write Efficiency Pravega write latency is of the order of milliseconds, and seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications. Streams are light weight, Pravega can support millions of Streams, this frees the application from worrying about statically configuring streams and preallocating a small fixed number of streams and husbanding or limiting stream resource. Write operations in Pravega are low latency, under 10 ms to return an acknowledgment is returned to a Writer. Furthermore, writes are optimized so that I/O throughput is limited by network bandwidth; the persistence mechanism is not the bottleneck. Pravega uses Apache BookKeeper to persist all write operations. BookKeeper persists and protects the data very efficiently. Because data is protected before the write operation is acknowledged to the Writer, data is always durable. As we discuss below, data durability is a fundamental characteristic of a storage primitive, To add further efficiency, writes to BookKeeper often involve data from multiple Stream Segments, so the cost of persisting data to disk can be amortized over several write operations. There is no durability-performance trade-off with Pravega. Reads are efficient too. A Reader can read from a Stream either at the tail of the Stream or at any part of the Stream's history. Unlike some log-based systems that use the same kind of storage for tail reads and writes as well as reads to historical data, Pravega uses two types of storage. The tail of the Stream is in so-called Tier-1 storage. Writes are implemented by Apache BookKeeper as mentioned above. Tail reads are served out of a Pravega-managed memory cache. In fact, BookKeeper serves reads only in failure recovery scenarios, where a Pravega Server has crashed and it is being recovered. This use of BookKeeper is exactly what it was designed for: fast writes, occasional reads. The historical part of the Stream is in so-called Tier 2 storage that is optimized for low-cost storage with high-throughput. Pravega uses efficient in-memory read ahead cache, taking advantage of the fact that Streams are usually read in large contiguous chunks and that HDFS is well suited for those sort of large, high-throughput reads. It is also worth noting that tail reads do not impact the performance of writes. Unlimited Retention Data in Streams can be retained for as long as the application needs it, constrained to the amount of data available, which is unbounded given the use of cloud storage in Tier 2. Pravega provides one convenient API to access both real-time and historical data. With Pravega, batch and real-time applications can both be handled efficiently; yet another reason why Pravega is a great storage primitive for Kappa architectures. If there is a value to keeping old data, why not keep it around? For example, in a machine learning example, you may want to periodically change the model and train the new version of the model against as much historical data as possible to yield more accurate predictive power of the model. With Pravega auto-tiering, keeping lots of historical data does not affect the performance of tail reads and writes. Size of a stream is not limited by the storage capacity of a single server, but rather, it is limited only by the storage capacity of your storage cluster or cloud provider. As cost of storage decreases, the economic incentive to delete data goes away Storage Efficiency Use Pravega to build pipelines of data processing, combining batch, real-time and other applications without duplicating data for every step of the pipeline. Consider the following data processing environment that combines real time processing using Spark, Flink, and or Storm; Haddoop for batch; some kind of Lucene-based Search mechanism like Elastic Search for full text search; and maybe one (or several) NoSQL databases to support micro-services apps. Using traditional approaches, one set of source data, for example, sensor data from an IoT app, would be ingested and replicated separately by each system. You would end up with 3 replicas of the data protected in the pub/sub system, 3 copies in HDFS, 3 copies in Lucene, 3 copies in the NoSQL database. When we consider the source data is measured in terabytes, the cost of data replication separated by middleware category becomes prohibitively expensive. Consider the same pipeline using Pravega and middleware adapted to use Pravega for its storage: With Pravega, the data is ingested and protected in one place; Pravega provides the single source of truth for the entire pipeline. Furthermore, with the bulk of the data being stored in Tier-2 enabled with erasure coding to efficiently protect the data, the storage cost of the data is substantially reduced. Durability With Pravega, you don't face a compromise between performance, durability and consistency. Pravega provides durable storage of streaming data with strong consistency, ordering guarantees and great performance. Durability is a fundamental storage primitive requirement. Storage that could lose data is not reliable storage. Systems based on such storage are not production quality. Once a write operation is acknowledged, the data will never be lost, even when failures occur. This is because Pravega always saves data in protected, persistent storage before the write operation returns to the Writer. With Pravega, data in the Stream is protected. A Stream can be treated as a system of record, just as you would treat data stored in databases or files. Transaction Support A developer uses a Pravega Transaction to ensure that a set of events are written to a stream atomically. A Pravega Transaction is part of Pravega's Writer API. Data can be written to a Stream directly through the API, or an application can write data through a Transaction. With Transactions, a Writer can persist data now, and later decide whether the data should be appended to a Stream or abandoned. Using a Transaction, data is written to the Stream only when the Transaction is committed. When the Transaction is committed, all data written to the Transaction is atomically appended to the Stream. Because Transactions are implemented in the same way as Stream Segments, data written to a Transaction is just as durable as data written directly to a Stream. If a Transaction is abandoned (e.g. if the Writer crashes) the Transaction is aborted and all data is discarded. Of course, an application can choose to abort the Transaction through the API if a condition occurs that suggests the Writer should discard the data. Transactions are key to chaining Flink jobs together. When a Flink job uses Pravega as a sink, it can begin a Transaction, and if it successfully finishes processing, commit the Transaction, writing the data into its Pravega-based sink. If the job fails for some reason, the Transaction times out and data is not written. When the job is restarted, there is no \"partial result\" in the sink that would need to be managed or cleaned up. Combining Transactions and other key features of Pravega, it is possible to chain Flink jobs together, having one job's Pravega-based sink be the source for a downstream Flink job. This provides the ability for an entire pipeline of Flink jobs to have end-end exactly once, guaranteed ordering of data processing. Of course, it is possible for Transactions across multiple Streams be coordinated with Transactions, so that a Flink job can use 2 or more Pravega-based sinks to provide source input to downstream Flink jobs. In addition, it is possible for application logic to coordinate Pravega Transactions with external databases such as Flink's checkpoint store. Learn more about Transactions here .","title":"Key Features"},{"location":"key-features/#pravega-key-features","text":"This document explains some of the key features of Pravega. It may be advantageous if you are already familiar with the core concepts of Pravega .","title":"Pravega Key Features"},{"location":"key-features/#pravega-design-principles","text":"Pravega was designed to support the new generation of streaming applications: applications that deal with a large amount of data arriving continuously that also need to generate an accurate analysis of that data in the face of late arriving data, data arriving out of order and failure conditions. There are several open source tools to enable developers to build such applications, including Apache Flink, Apache Beam, Spark Streaming and others. To date, these applications used systems such as Apache Kafka, Apache ActiveMQ, RabbitMQ, Apache Cassandra, and Apache HDFS to ingest and store data. We envision instead a unification of the two concepts and our work focuses on both ingesting and storing stream data. Pravega approaches streaming applications from a storage perspective. It enables applications to ingest stream data continuously and storing it permanently. Such stream data can be accessed with low latency (order of milliseconds), but also months, years ahead as part of analyzing historical data. The design of Pravega incorporates lessons learned from using the Lambda architecture to build streaming applications and the challenges to deploy streaming applications at scale that consistently deliver accurate results in a fault tolerant manner. The Pravega architecture provides strong durability and consistency guarantees, delivering a rock solid foundation to build streaming applications upon. With the Lambda architecture, the developer uses a complex combination of middleware tools that include batch style middleware mainly influenced by Hadoop and continuous processing tools like Storm, Samza, Kafka and others. In this architecture, batch processing is used to deliver accurate, but potentially out of date analysis of data. The second path processes data as it is ingested, and in principle the results are innacurate, which justifies the first batch path. With this approach, there are two copies of the application logic because the programming models of the speed layer are different than those used in the batch layer. An implementation of the Lambda architecture can be difficult to maintain and manage in production. This style of big data application design consequently has been losing traction. A different kind of architecture has been gaining traction recently that does not rely on a batch processing data path. This architecture is called Kappa. The Kappa architecture style is a reaction to the complexity of the Lambda architecture and relies on components that are designed for streaming, supporting stronger semantics and delivering both fast and accurate data analysis. The Kappa architecture is a simpler approach: There is only one data path to execute, and one implementation of the application logic to maintain, not two. With the right tools, built for the demands of processing streaming data in a fast and accurate fashion, it becomes simpler to design and run applications in the space of IoT, connected cars, finance, risk management, online services, etc. With the right tooling, it is possible to build such pipelines and serve applications that present high volume and demand low latency. Applications often require more than one stage of processing. Any practical system for stream analytics must be able to accomodate the composition of stages in the form of data pipelines: With data pipelines, it is important to think of guarantees end-to-end rather than on a per componenent basis. For example, it is not sufficient that one stage guarantees exactly-once semantics while at least one other does not make such a guarantee. Our goal in Pravega is enable the design and implementation of data pipelines with strong guarantees end-to-end.","title":"Pravega Design Principles"},{"location":"key-features/#pravega-storage-reimagined-for-a-streaming-world","text":"Pravega introduces a new storage primitive, a stream, that matches continuous processing of unbounded data. In Pravega, a stream is a named, durable, append-only and unbounded sequence of bytes. With this primitive, and the key features discussed in this document, Pravega is an ideal component to combine with stream processing engines such as Flink to build streaming applications. Because of Pravega's key features, we imagine that it will be the fundamental storage primitive for a new generation of streaming-oriented middleware. Let's examine the key features of Pravega.","title":"Pravega - Storage Reimagined for a Streaming World"},{"location":"key-features/#exactly-once-semantics","text":"By exactly once semantics we mean that Pravega ensures that data is not duplicated and no event is missed despite failures. Of course, this statement comes with a number of caveats, like any other system that promises exactly-once semantics, but let's not dive into the gory details here. An important consideration is that exactly-once semantics is a natural part of Pravega and has been a goal and part of the design from day zero. To achieve exactly once semantics, Pravega Streams are durable, ordered, consistent and transactional. We discuss durable and transactional in separate sections below. By ordering, we mean that data is observed by readers in the order it is written. In Pravega, data is written along with an application-defined routing key. Pravega makes ordering guarantees in terms of routing keys. Two pieces of data with the same routing key will always be read by a Reader in the order they were written. Pravega's ordering guarantees allow data reads to be replayed (e.g. when applications crash) and the results of replaying the reads will be the same. By consistency, we mean all Readers see the same ordered view of data for a given routing key, even in the face of failure. Systems that are \"mostly consistent\" are not sufficient for building accurate data processing. Systems that provide \"at least once\" semantics might present duplication. In such systems, a data producer might write the same data twice in some scenarios. In Pravega, writes are idempotent, rewrites done as a result of reconnection don't result in data duplication. Note that we make no guarantee when the data coming from the source already contains duplicates. Written data is opaque to Pravega and it makes no attempt to remove existing duplicates. We have not limited our focus to exactly-once semantics for writing, however. We also provide, and are actively working on extending the features, that enable exactly-once end-to-end for a data pipeline. The strong consistency guarantees that the Pravega store provides along with the semantics of a data analytics engine like Flink enables such end-to-end guarantees.","title":"Exactly Once Semantics"},{"location":"key-features/#auto-scaling","text":"Unlike systems with static partitioning, Pravega can automatically scale individual data streams to accommodate changes in data ingestion rate. Imagine an IoT application with millions of devices feeding thousands of data streams with information about those devices. Imagine a pipeline of Flink jobs that process those streams to derive business value from all that raw IoT data: predicting device failures, optimizing service delivery through those devices, or tailoring a customer's experience when interacting with those devices. Building such an application at scale is difficult without having the components be able to scale automatically as the rate of data increases and decreases. With Pravega, it is easy to elastically and independently scale data ingestion, storage and processing \u2013 orchestrating the scaling of every component in a data pipeline. Pravega's support of auto scaling starts with the idea that Streams are partitioned into Stream Segments. A Stream may have 1 or more Stream Segments; recall that a Stream Segment is a partition of the Stream associated with a range of routing keys. Any data written into the Stream is written to the Stream Segment associated with the data's routing key. Writers use application-meaningful routing keys like customer-id, timestamp, machine-id, etc to make sure like data is grouped together. A Stream Segment is the fundamental unit of parallelism in Pravega Streams. A Stream with multiple Stream Segments can support more parallelism of data writes; multiple Writers writing data into the different Stream Segments potentially involving all the Pravega Servers in the cluster. On the Reader side, the number of Stream Segments represents the maximum degree of read parallelism possible. If a Stream has N Stream Segments, then a ReaderGroup with N Readers can consume from the Stream in parallel. Increase the number of Stream Segments, you can increase the number of Readers in the ReaderGroup to increase the scale of processing the data from that Stream. And of course if the number of Stream Segments decreases, it would be a good idea to reduce the number of Readers. A Stream can be configured to grow the number of Stream Segments as more data is written to the Stream, and to shrink when data volume drops off. We refer to this configuration as the Stream's Service Level Objective or SLO. Pravega monitors the rate of data input to the Stream and uses the SLO to add or remove Stream Segments from a Stream. Segments are added by splitting a Segment. Segments are removed by merging two Segments. See AutoScaling: The number of Stream Segments can vary over time , for more detail on how Pravega manages Stream Segments. It is possible to coordinate the auto scaling of Streams in Pravega with application scale out (in the works). Using metadata available from Pravega, applications can configure the scaling of their application components; for example, to drive the number of instances of a Flink job. Alternatively, you could use software such as Cloud Foundry, Mesos/Marathon, Kubernetes or the Docker stack to deploy new instances of an application to react to increased parallelism at the Pravega level, or to terminate instances as Pravega scales down in response to reduced rate of data ingestion.","title":"Auto Scaling"},{"location":"key-features/#distributed-computing-primitive","text":"Pravega is great for distributed applications, such as micro-services; it can be used as a data storage mechanism, for messaging between micro-services and for other distributed computing services such as leader election. State Synchronizer, a part of the Pravega API, is the basis of sharing state across a cluster with consistency and optimistic concurrency. State Synchronizer is based on a fundamental conditional write operation in Pravega, so that data is written only if it would appear at a given position in the Stream. If a conditional write operation cannot meet the condition, it fails. State Synchronizer is therefore a strong synchronization primitive that can be used for shared state in a cluster, membership management, leader election and other distributed computing scenarios. You can learn more about the State Synchronizer here .","title":"Distributed Computing Primitive"},{"location":"key-features/#write-efficiency","text":"Pravega write latency is of the order of milliseconds, and seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications. Streams are light weight, Pravega can support millions of Streams, this frees the application from worrying about statically configuring streams and preallocating a small fixed number of streams and husbanding or limiting stream resource. Write operations in Pravega are low latency, under 10 ms to return an acknowledgment is returned to a Writer. Furthermore, writes are optimized so that I/O throughput is limited by network bandwidth; the persistence mechanism is not the bottleneck. Pravega uses Apache BookKeeper to persist all write operations. BookKeeper persists and protects the data very efficiently. Because data is protected before the write operation is acknowledged to the Writer, data is always durable. As we discuss below, data durability is a fundamental characteristic of a storage primitive, To add further efficiency, writes to BookKeeper often involve data from multiple Stream Segments, so the cost of persisting data to disk can be amortized over several write operations. There is no durability-performance trade-off with Pravega. Reads are efficient too. A Reader can read from a Stream either at the tail of the Stream or at any part of the Stream's history. Unlike some log-based systems that use the same kind of storage for tail reads and writes as well as reads to historical data, Pravega uses two types of storage. The tail of the Stream is in so-called Tier-1 storage. Writes are implemented by Apache BookKeeper as mentioned above. Tail reads are served out of a Pravega-managed memory cache. In fact, BookKeeper serves reads only in failure recovery scenarios, where a Pravega Server has crashed and it is being recovered. This use of BookKeeper is exactly what it was designed for: fast writes, occasional reads. The historical part of the Stream is in so-called Tier 2 storage that is optimized for low-cost storage with high-throughput. Pravega uses efficient in-memory read ahead cache, taking advantage of the fact that Streams are usually read in large contiguous chunks and that HDFS is well suited for those sort of large, high-throughput reads. It is also worth noting that tail reads do not impact the performance of writes.","title":"Write Efficiency"},{"location":"key-features/#unlimited-retention","text":"Data in Streams can be retained for as long as the application needs it, constrained to the amount of data available, which is unbounded given the use of cloud storage in Tier 2. Pravega provides one convenient API to access both real-time and historical data. With Pravega, batch and real-time applications can both be handled efficiently; yet another reason why Pravega is a great storage primitive for Kappa architectures. If there is a value to keeping old data, why not keep it around? For example, in a machine learning example, you may want to periodically change the model and train the new version of the model against as much historical data as possible to yield more accurate predictive power of the model. With Pravega auto-tiering, keeping lots of historical data does not affect the performance of tail reads and writes. Size of a stream is not limited by the storage capacity of a single server, but rather, it is limited only by the storage capacity of your storage cluster or cloud provider. As cost of storage decreases, the economic incentive to delete data goes away","title":"Unlimited Retention"},{"location":"key-features/#storage-efficiency","text":"Use Pravega to build pipelines of data processing, combining batch, real-time and other applications without duplicating data for every step of the pipeline. Consider the following data processing environment that combines real time processing using Spark, Flink, and or Storm; Haddoop for batch; some kind of Lucene-based Search mechanism like Elastic Search for full text search; and maybe one (or several) NoSQL databases to support micro-services apps. Using traditional approaches, one set of source data, for example, sensor data from an IoT app, would be ingested and replicated separately by each system. You would end up with 3 replicas of the data protected in the pub/sub system, 3 copies in HDFS, 3 copies in Lucene, 3 copies in the NoSQL database. When we consider the source data is measured in terabytes, the cost of data replication separated by middleware category becomes prohibitively expensive. Consider the same pipeline using Pravega and middleware adapted to use Pravega for its storage: With Pravega, the data is ingested and protected in one place; Pravega provides the single source of truth for the entire pipeline. Furthermore, with the bulk of the data being stored in Tier-2 enabled with erasure coding to efficiently protect the data, the storage cost of the data is substantially reduced.","title":"Storage Efficiency"},{"location":"key-features/#durability","text":"With Pravega, you don't face a compromise between performance, durability and consistency. Pravega provides durable storage of streaming data with strong consistency, ordering guarantees and great performance. Durability is a fundamental storage primitive requirement. Storage that could lose data is not reliable storage. Systems based on such storage are not production quality. Once a write operation is acknowledged, the data will never be lost, even when failures occur. This is because Pravega always saves data in protected, persistent storage before the write operation returns to the Writer. With Pravega, data in the Stream is protected. A Stream can be treated as a system of record, just as you would treat data stored in databases or files.","title":"Durability"},{"location":"key-features/#transaction-support","text":"A developer uses a Pravega Transaction to ensure that a set of events are written to a stream atomically. A Pravega Transaction is part of Pravega's Writer API. Data can be written to a Stream directly through the API, or an application can write data through a Transaction. With Transactions, a Writer can persist data now, and later decide whether the data should be appended to a Stream or abandoned. Using a Transaction, data is written to the Stream only when the Transaction is committed. When the Transaction is committed, all data written to the Transaction is atomically appended to the Stream. Because Transactions are implemented in the same way as Stream Segments, data written to a Transaction is just as durable as data written directly to a Stream. If a Transaction is abandoned (e.g. if the Writer crashes) the Transaction is aborted and all data is discarded. Of course, an application can choose to abort the Transaction through the API if a condition occurs that suggests the Writer should discard the data. Transactions are key to chaining Flink jobs together. When a Flink job uses Pravega as a sink, it can begin a Transaction, and if it successfully finishes processing, commit the Transaction, writing the data into its Pravega-based sink. If the job fails for some reason, the Transaction times out and data is not written. When the job is restarted, there is no \"partial result\" in the sink that would need to be managed or cleaned up. Combining Transactions and other key features of Pravega, it is possible to chain Flink jobs together, having one job's Pravega-based sink be the source for a downstream Flink job. This provides the ability for an entire pipeline of Flink jobs to have end-end exactly once, guaranteed ordering of data processing. Of course, it is possible for Transactions across multiple Streams be coordinated with Transactions, so that a Flink job can use 2 or more Pravega-based sinks to provide source input to downstream Flink jobs. In addition, it is possible for application logic to coordinate Pravega Transactions with external databases such as Flink's checkpoint store. Learn more about Transactions here .","title":"Transaction Support"},{"location":"metrics/","text":"In Pravega Metrics Framework, we use Dropwizard Metrics as the underlying library, and provide our own API to make it easier to use. 1. Metrics interfaces and examples usage There are four basic interfaces: StatsProvider, StatsLogger (short for Statistics Logger), OpStatsLogger (short for Operation Statistics Logger, and it is included in StatsLogger) and Dynamic Logger. StatsProvider provides us the whole Metric service; StatsLogger is the place at which we register and get required Metrics ( Counter / Gauge / Timer / Histograms ); while OpStatsLogger is a sub-metric for complex ones (Timer/Histograms). 1.1. Metrics Service Provider \u2014 Interface StatsProvider The starting point of Pravega Metric framework is the StatsProvider interface, it provides start and stop method for Metric service. Regarding the reporters, currently we have support for CSV reporter and StatsD reporter. public interface StatsProvider { void start ( MetricsConfig conf ); void close (); StatsLogger createStatsLogger ( String scope ); DynamicLogger createDynamicLogger (); } * start(): Initializes MetricRegistry and reporters for our Metrics service. * close(): Shutdown of Metrics service. * createStatsLogger(): Creates and returns a StatsLogger instance, which is used to retrieve a metric and do metric insertion and collection in Pravega code. * createDynamicLogger(): Create a dynamic logger. 1.2. Metric Logger \u2014 interface StatsLogger Using this interface we can register required metrics for simple types like Counter and Gauge and some complex statistics type of Metric OpStatsLogger, through which we provide Timer and Histogram . public interface StatsLogger { OpStatsLogger createStats ( String name ); Counter createCounter ( String name ); Meter createMeter ( String name ); T extends Number Gauge registerGauge ( String name , Supplier T value ); StatsLogger createScopeLogger ( String scope ); } * createStats(): Register and get a OpStatsLogger, which is used for complex type of metrics. * createCounter(): Register and get a Counter metric. * createMeter(): Create and register a Meter metric. * registerGauge(): Register a Gauge metric. * createScopeLogger(): Create the stats logger under given scope name. 1.3. Metric Sub Logger \u2014 OpStatsLogger OpStatsLogger provides complex statistics type of Metric, usually it is used in operations such as CreateSegment, ReadSegment, we could use it to record the number of operation, time/duration of each operation. public interface OpStatsLogger { void reportSuccessEvent ( Duration duration ); void reportFailEvent ( Duration duration ); void reportSuccessValue ( long value ); void reportFailValue ( long value ); OpStatsData toOpStatsData (); void clear (); } * reportSuccessEvent() : Used to track Timer of a successful operation and will record the latency in Nanoseconds in required metric. * reportFailEvent() : Used to track Timer of a failed operation and will record the latency in Nanoseconds in required metric. * reportSuccessValue() : Used to track Histogram of a success value. * reportFailValue() : Used to track Histogram of a failed value. * toOpStatsData() : Used to support JMX exports and inner test. * clear : Used to clear stats for this operation. 1.4 Metric Logger \u2014 interface DynamicLogger A simple interface that only exposes simple type metrics: Counter/Gauge/Meter. public interface DynamicLogger { void incCounterValue ( String name , long delta ); void updateCounterValue ( String name , long value ); void freezeCounter ( String name ); T extends Number void reportGaugeValue ( String name , T value ); void freezeGaugeValue ( String name ); void recordMeterEvents ( String name , long number ); } * incCounterValue() : Increase Counter with given value * updateCounterValue() : Updates the counter with given value * freezeCounter() : Notifies that the counter will not be updated * reportGaugeValue() : Reports Gauge value * freezeGaugeValue() : Notifies that the gauge value will not be updated. * recordMeterEvents() : Record the occurrence of a given number of events in Meter. 2. Example for starting a Metric service This example is from file io.pravega.segmentstore.server.host.ServiceStarter. It starts Pravega SegmentStore service and a Metrics service is started as a sub service. public final class ServiceStarter { ... private StatsProvider statsProvider ; ... private void start () { ... log . info ( Initializing metrics provider ... ); MetricsConfig config = MetricsConfig . builder () . with ( MetricsConfig . METRICS_PREFIX , metrics-prefix ) . build (); MetricsProvider . initialize ( metricsConfig ); statsProvider = MetricsProvider . getMetricsProvider (); statsProvider . start (); // Here metric service is started as a sub-service ... } private void shutdown () { ... if ( this . statsProvider != null ) { statsProvider . close (); statsProvider = null ; log . info ( Metrics statsProvider is now closed. ); } ... } ... } 2.1. Example for Dynamic Counter and OpStatsLogger(Timer) This is an example from io.pravega.segmentstore.server.host.handler.PravegaRequestProcessor. In this class, we registered two metrics: One timer (createStreamSegment), one dynamic counter (segmentReadBytes). public class PravegaRequestProcessor extends FailingRequestProcessor implements RequestProcessor { private static final StatsLogger STATS_LOGGER = MetricsProvider . createStatsLogger ( segmentstore ); private static final DynamicLogger DYNAMIC_LOGGER = MetricsProvider . getDynamicLogger (); private final OpStatsLogger createStreamSegment = STATS_LOGGER . createStats ( SEGMENT_CREATE_LATENCY ); private void handleReadResult ( ReadSegment request , ReadResult result ) { String segment = request . getSegment (); ArrayList ReadResultEntryContents cachedEntries = new ArrayList (); ReadResultEntry nonCachedEntry = collectCachedEntries ( request . getOffset (), result , cachedEntries ); boolean truncated = nonCachedEntry != null nonCachedEntry . getType () == Truncated ; boolean endOfSegment = nonCachedEntry != null nonCachedEntry . getType () == EndOfStreamSegment ; boolean atTail = nonCachedEntry != null nonCachedEntry . getType () == Future ; if (! cachedEntries . isEmpty () || endOfSegment ) { // We managed to collect some data. Send it. ByteBuffer data = copyData ( cachedEntries ); SegmentRead reply = new SegmentRead ( segment , request . getOffset (), atTail , endOfSegment , data ); connection . send ( reply ); DYNAMIC_LOGGER . incCounterValue ( nameFromSegment ( SEGMENT_READ_BYTES , segment ), reply . getData (). array (). length ); // Increasing the counter value for the counter metric SEGMENT_READ_BYTES } else if ( truncated ) { // We didn t collect any data, instead we determined that the current read offset was truncated. // Determine the current Start Offset and send that back. segmentStore . getStreamSegmentInfo ( segment , false , TIMEOUT ) . thenAccept ( info - connection . send ( new SegmentIsTruncated ( nonCachedEntry . getStreamSegmentOffset (), segment , info . getStartOffset ()))) . exceptionally ( e - handleException ( nonCachedEntry . getStreamSegmentOffset (), segment , Read segment , e )); } else { Preconditions . checkState ( nonCachedEntry != null , No ReadResultEntries returned from read!? ); nonCachedEntry . requestContent ( TIMEOUT ); nonCachedEntry . getContent () . thenAccept ( contents - { ByteBuffer data = copyData ( Collections . singletonList ( contents )); SegmentRead reply = new SegmentRead ( segment , nonCachedEntry . getStreamSegmentOffset (), false , endOfSegment , data ); connection . send ( reply ); DYNAMIC_LOGGER . incCounterValue ( nameFromSegment ( SEGMENT_READ_BYTES , segment ), reply . getData (). array (). length ); // Increasing the counter value for the counter metric SEGMENT_READ_BYTES }) . exceptionally ( e - { if ( Exceptions . unwrap ( e ) instanceof StreamSegmentTruncatedException ) { // The Segment may have been truncated in Storage after we got this entry but before we managed // to make a read. In that case, send the appropriate error back. connection . send ( new SegmentIsTruncated ( nonCachedEntry . getStreamSegmentOffset (), segment , nonCachedEntry . getStreamSegmentOffset ())); } else { handleException ( nonCachedEntry . getStreamSegmentOffset (), segment , Read segment , e ); } return null ; }) . exceptionally ( e - handleException ( nonCachedEntry . getStreamSegmentOffset (), segment , Read segment , e )); } } @Override public void createSegment ( CreateSegment createStreamsSegment ) { Timer timer = new Timer (); Collection AttributeUpdate attributes = Arrays . asList ( new AttributeUpdate ( SCALE_POLICY_TYPE , AttributeUpdateType . Replace , (( Byte ) createStreamsSegment . getScaleType ()). longValue ()), new AttributeUpdate ( SCALE_POLICY_RATE , AttributeUpdateType . Replace , (( Integer ) createStreamsSegment . getTargetRate ()). longValue ()), new AttributeUpdate ( CREATION_TIME , AttributeUpdateType . None , System . currentTimeMillis ()) ); if (! verifyToken ( createStreamsSegment . getSegment (), createStreamsSegment . getRequestId (), createStreamsSegment . getDelegationToken (), READ_UPDATE , Create Segment )) { return ; } log . debug ( Creating stream segment {} , createStreamsSegment ); segmentStore . createStreamSegment ( createStreamsSegment . getSegment (), attributes , TIMEOUT ) . thenAccept ( v - { createStreamSegment . reportSuccessEvent ( timer . getElapsed ()); // Reporting success event for Timer metric createStreamSegment connection . send ( new SegmentCreated ( createStreamsSegment . getRequestId (), createStreamsSegment . getSegment ())); }) . whenComplete (( res , e ) - { if ( e == null ) { if ( statsRecorder != null ) { statsRecorder . createSegment ( createStreamsSegment . getSegment (), createStreamsSegment . getScaleType (), createStreamsSegment . getTargetRate ()); } } else { createStreamSegment . reportFailEvent ( timer . getElapsed ()); // Reporting fail event for Timer metric createStreamSegment handleException ( createStreamsSegment . getRequestId (), createStreamsSegment . getSegment (), Create segment , e ); } }); } \u2026 } From the above example, we can see the reuired steps of how to register and use a metric in desired class and method: Get a StatsLogger from MetricsProvider: StatsLogger STATS_LOGGER = MetricsProvider.getStatsLogger(); Register all the desired metrics through StatsLogger: static final OpStatsLogger CREATE_STREAM_SEGMENT = STATS_LOGGER.createStats(SEGMENT_CREATE_LATENCY); Use these metrics within code at appropriate place where you would like to collect and record the values. Metrics.CREATE_STREAM_SEGMENT.reportSuccessEvent(timer.getElapsedNanos()); Here CREATE_STREAM_SEGMENT is the name of this metric, and CREATE_STREAM_SEGMENT is the name of our Metrics logger, it will track operations of createSegment, and we will get the time of each createSegment operation happened, how long each operation takes, and other numbers computed based on them. 2.1.1 Output example of OpStatsLogger An example output of OpStatsLogger CREATE_SEGMENT reported through CSV reporter: $ cat CREATE_STREAM_SEGMENT.csv t,count,max,mean,min,stddev,p50,p75,p95,p98,p99,p999,mean_rate,m1_rate,m5_rate,m15_rate,rate_unit,duration_unit 1480928806,1,8.973952,8.973952,8.973952,0.000000,8.973952,8.973952,8.973952,8.973952,8.973952,8.973952,0.036761,0.143306,0.187101,0.195605,calls/second,millisecond 2.2. Example for Dynamic Gauge and OpStatsLogger(Histogram) This is an example from io.pravega.controller.store.stream.AbstractStreamMetadataStore. In this class, we report a Dynamic Gauge which represents the open transactions and one histogram (CREATE_STREAM). public abstract class AbstractStreamMetadataStore implements StreamMetadataStore { ... private static final OpStatsLogger CREATE_STREAM = STATS_LOGGER . createStats ( MetricsNames . CREATE_STREAM ); // get stats logger from MetricsProvider private static final DynamicLogger DYNAMIC_LOGGER = MetricsProvider . getDynamicLogger (); // get dynamic logger from MetricsProvider ... @Override public CompletableFuture CreateStreamResponse createStream ( final String scope , final String name , final StreamConfiguration configuration , final long createTimestamp , final OperationContext context , final Executor executor ) { return withCompletion ( getStream ( scope , name , context ). create ( configuration , createTimestamp ), executor ) . thenApply ( result - { if ( result . getStatus (). equals ( CreateStreamResponse . CreateStatus . NEW )) { CREATE_STREAM . reportSuccessValue ( 1 ); // Report success event for histogram metric CREATE_STREAM DYNAMIC_LOGGER . reportGaugeValue ( nameFromStream ( OPEN_TRANSACTIONS , scope , name ), 0 ); // Report gauge value for Dynamic Gauge metric OPEN_TRANSACTIONS } return result ; }); } ... } 2.3 Example for Dynamic Meter This is an example from io.pravega.segmentstore.server.SegmentStoreMetrics. In this class, we report a Dynamic Meter which represents the segments created. public final class SegmentStoreMetrics { private static final DynamicLogger DYNAMIC_LOGGER = MetricsProvider . getDynamicLogger (); public void createSegment () { DYNAMIC_LOGGER . recordMeterEvents ( this . createSegmentCount , 1 ); // Record event for meter metric createSegmentCount } } 3. Metric reporter and Configurations Reporters are the way through which we export all the measurements being made by the metrics. We currently provide StatsD and CSV output. It is not difficult to add new output formats, such as JMX/SLF4J. CSV reporter will export each Metric out into one file. StatsD reporter will export Metrics through UDP/TCP to a StatsD server. The reporter could be configured through MetricsConfig. public class MetricsConfig extends ComponentConfig { //region Members public static final String COMPONENT_CODE = metrics ; public final static String ENABLE_STATISTICS = enableStatistics ; //enable metric, or will report nothing, default = true, public final static Property Long DYNAMIC_CACHE_SIZE = dynamicCacheSize ; //dynamic cache size , default = 10000000L public final static Property Integer DYNAMIC_CACHE_EVICTION_DURATION_MINUTES = dynamicCacheEvictionDurationMs ; //dynamic cache evcition duration, default = 30 public final static String OUTPUT_FREQUENCY = statsOutputFrequencySeconds ; //reporter output frequency, default = 60 public final static String METRICS_PREFIX = metricsPrefix ; //Metrics Prefix, default = pravega public final static String CSV_ENDPOINT = csvEndpoint ; // CSV reporter output dir, default = /tmp/csv public final static String STATSD_HOST = statsDHost ; // StatsD server host for the reporting, default = localhost public final static String STATSD_PORT = statsDPort ; // StatsD server port, default = 8125 public final static Property String GRAPHITE_HOST = graphiteHost ; // Graphite server host for the reporting, default = localhost public final static Property Integer GRAPHITE_PORT = graphitePort ; // Graphite server port, default = 2003 public final static Property String JMX_DOMAIN = jmxDomain ; // JMX domain for the reporting, default = domain public final static Property String GANGLIA_HOST = gangliaHost ; // Ganglia server host for the reporting, default = localhost public final static Property Integer GANGLIA_PORT = gangliaPort ; // Ganglia server port, default = 8649 public final static Property Boolean ENABLE_CSV_REPORTER = enableCSVReporter ; // Enables CSV reporter, default = true public final static Property Boolean ENABLE_STATSD_REPORTER = enableStatsdReporter ; // Enables StatsD reporter, default = true public final static Property Boolean ENABLE_GRAPHITE_REPORTER = enableGraphiteReporter ; // Enables Graphite reporter, default = false public final static Property Boolean ENABLE_JMX_REPORTER = enableJMXReporter ; // Enables JMX reporter, default = false public final static Property Boolean ENABLE_GANGLIA_REPORTER = enableGangliaReporter ; // Enables Ganglia reporter, default = false public final static Property Boolean ENABLE_CONSOLE_REPORTER = enableConsoleReporter ; // Enables Console reporter, default = false ... } 4. Steps to add your own Metrics Step 1. When start a segment store/controller service, start a Metrics service as a sub service. Reference above example in ServiceStarter.start() statsProvider = MetricsProvider . getProvider (); statsProvider . start ( metricsConfig ); Step 2. In the class that need Metrics: get StatsLogger through MetricsProvider; then get Metrics from StatsLogger; at last report it at the right place. ```java static final StatsLogger STATS_LOGGER = MetricsProvider.getStatsLogger(); --- 1 static final DynamicLogger DYNAMIC_LOGGER = MetricsProvider.getDynamicLogger(); public static class Metrics { --- 2 //Using Stats Logger static final String CREATE_STREAM = \"stream_created\"; static final OpStatsLogger CREATE_STREAM = STATS_LOGGER.createStats(CREATE_STREAM); static final String SEGMENT_CREATE_LATENCY = \"segment_create_latency_ms\"; static final OpStatsLogger createStreamSegment = STATS_LOGGER.createStats(SEGMENT_CREATE_LATENCY); //Using Dynamic Logger static final String SEGMENT_READ_BYTES = segmentstore.segment_read_bytes ; //Dynamic Counter static final String OPEN_TRANSACTIONS = controller.transactions_opened ; //Dynamic Gauge ... } //to report success or increment Metrics.CREATE_STREAM.reportSuccessValue(1); --- 3 Metrics.createStreamSegment.reportSuccessEvent(timer.getElapsed()); DYNAMIC_LOGGER.incCounterValue(Metrics.SEGMENT_READ_BYTES, 1); DYNAMIC_LOGGER.reportGaugeValue(OPEN_TRANSACTIONS, 0); //in case of failure Metrics.CREATE_STREAM.reportFailValue(1); Metrics.createStreamSegment.reportFailEvent(timer.getElapsed()); //to freeze DYNAMIC_LOGGER.freezeCounter(Metrics.SEGMENT_READ_BYTES); DYNAMIC_LOGGER.freezeGaugeValue(OPEN_TRANSACTIONS); ``` 5. Available Metrics and their names Metrics in Segment Store Service ```` segmentstore.segment_read_latency_ms segmentstore.segment_write_latency_ms segmentstore.segment_create_latency_ms //Dynamic segmentstore.segment_read_bytes.$scope.$stream.$segment.Counter segmentstore.segment_write_bytes.$scope.$stream.$segment.Counter ```` Tier-2 Storage Metrics: Read/Write Latency, Read/Write Rate ```` hdfs.tier2_read_latency_ms hdfs.tier2_write_latency_ms //Dynamic hdfs.tire2_read_bytes.Counter hdfs.tier2_write_bytes.Counter * Cache Metrics rocksdb.cache_insert_latency rocksdb.cache_get_latency ```` Tier-1 DurableDataLog Metrics: Read/Write Latency, Read/Write Rate ```` bookkeeper.bookkeeper_total_write_latency bookkeeper.bookkeeper_write_latency bookkeeper.bookkeeper_write_bytes bookkeeper.bookkeeper_write_queue_size bookkeeper.bookkeeper_write_queue_fill //Dynamic bookkeeper.bookkeeper_ledger_count.$containerId.Gauge ```` Container-specific metrics ```` process_operations_latency.$containerId process_operations_batch_size.$containerId operation_queue_size.$containerId operation_processor_in_flight.$containerId operation_queue_wait_time.$containerId operation_processor_delay_ms.$containerId operation_commit_latency_ms.$containerId operation_latency_ms.$containerId operation_commit_metadata_txn_count.$containerId operation_commit_memory_latency_ms.$containerId operation_log_size.$containerId //Dynamic container_append_count.$containerId.Meter container_append_offset_count.$containerId.Meter container_update_attributes_count.$containerId.Meter container_get_attributes_count.$containerId.Meter container_read_count.$containerId.Meter container_get_info_count.$containerId.Meter container_create_segment_count.$containerId.Meter container_delete_segment_count.$containerId.Meter container_merge_segment_count.$containerId.Meter container_seal_count.$containerId.Meter container_truncate_count.$containerId.Meter active_segments.$containerId.Gauge ```` Metrics in Controller ```` controller.stream_created controller.stream_sealed controller.stream_deleted //Dynamic controller.transactions_created.$scope.$stream.Counter controller.transactions_committed.$scope.$stream.Counter controller.transactions_aborted.$scope.$stream.Counter controller.transactions_opened.$scope.$stream.Gauge controller.transactions_timedout.$scope.$stream.Counter controller.segments_count.$scope.$stream.Gauge controller.$scope.$stream.segments_splits.$scope.$stream.Counter controller.$scope.$stream.segments_merges.$scope.$stream.Counter controller.retention_frequency.$scope.$stream.Meter controller.truncated_size..$scope.$stream.Gauge * General Metrics cache_size_bytes cache_gen thread_pool_queue_size thread_pool_active_threads ```` 6. Useful links Dropwizard Metrics Statsd_spec etsy_StatsD","title":"Pravega Metrics"},{"location":"metrics/#1-metrics-interfaces-and-examples-usage","text":"There are four basic interfaces: StatsProvider, StatsLogger (short for Statistics Logger), OpStatsLogger (short for Operation Statistics Logger, and it is included in StatsLogger) and Dynamic Logger. StatsProvider provides us the whole Metric service; StatsLogger is the place at which we register and get required Metrics ( Counter / Gauge / Timer / Histograms ); while OpStatsLogger is a sub-metric for complex ones (Timer/Histograms).","title":"1. Metrics interfaces and examples usage"},{"location":"metrics/#11-metrics-service-provider-interface-statsprovider","text":"The starting point of Pravega Metric framework is the StatsProvider interface, it provides start and stop method for Metric service. Regarding the reporters, currently we have support for CSV reporter and StatsD reporter. public interface StatsProvider { void start ( MetricsConfig conf ); void close (); StatsLogger createStatsLogger ( String scope ); DynamicLogger createDynamicLogger (); } * start(): Initializes MetricRegistry and reporters for our Metrics service. * close(): Shutdown of Metrics service. * createStatsLogger(): Creates and returns a StatsLogger instance, which is used to retrieve a metric and do metric insertion and collection in Pravega code. * createDynamicLogger(): Create a dynamic logger.","title":"1.1. Metrics Service Provider \u2014 Interface StatsProvider"},{"location":"metrics/#12-metric-logger-interface-statslogger","text":"Using this interface we can register required metrics for simple types like Counter and Gauge and some complex statistics type of Metric OpStatsLogger, through which we provide Timer and Histogram . public interface StatsLogger { OpStatsLogger createStats ( String name ); Counter createCounter ( String name ); Meter createMeter ( String name ); T extends Number Gauge registerGauge ( String name , Supplier T value ); StatsLogger createScopeLogger ( String scope ); } * createStats(): Register and get a OpStatsLogger, which is used for complex type of metrics. * createCounter(): Register and get a Counter metric. * createMeter(): Create and register a Meter metric. * registerGauge(): Register a Gauge metric. * createScopeLogger(): Create the stats logger under given scope name.","title":"1.2. Metric Logger \u2014 interface StatsLogger"},{"location":"metrics/#13-metric-sub-logger-opstatslogger","text":"OpStatsLogger provides complex statistics type of Metric, usually it is used in operations such as CreateSegment, ReadSegment, we could use it to record the number of operation, time/duration of each operation. public interface OpStatsLogger { void reportSuccessEvent ( Duration duration ); void reportFailEvent ( Duration duration ); void reportSuccessValue ( long value ); void reportFailValue ( long value ); OpStatsData toOpStatsData (); void clear (); } * reportSuccessEvent() : Used to track Timer of a successful operation and will record the latency in Nanoseconds in required metric. * reportFailEvent() : Used to track Timer of a failed operation and will record the latency in Nanoseconds in required metric. * reportSuccessValue() : Used to track Histogram of a success value. * reportFailValue() : Used to track Histogram of a failed value. * toOpStatsData() : Used to support JMX exports and inner test. * clear : Used to clear stats for this operation.","title":"1.3. Metric Sub Logger \u2014 OpStatsLogger"},{"location":"metrics/#14-metric-logger-interface-dynamiclogger","text":"A simple interface that only exposes simple type metrics: Counter/Gauge/Meter. public interface DynamicLogger { void incCounterValue ( String name , long delta ); void updateCounterValue ( String name , long value ); void freezeCounter ( String name ); T extends Number void reportGaugeValue ( String name , T value ); void freezeGaugeValue ( String name ); void recordMeterEvents ( String name , long number ); } * incCounterValue() : Increase Counter with given value * updateCounterValue() : Updates the counter with given value * freezeCounter() : Notifies that the counter will not be updated * reportGaugeValue() : Reports Gauge value * freezeGaugeValue() : Notifies that the gauge value will not be updated. * recordMeterEvents() : Record the occurrence of a given number of events in Meter.","title":"1.4 Metric Logger \u2014 interface DynamicLogger"},{"location":"metrics/#2-example-for-starting-a-metric-service","text":"This example is from file io.pravega.segmentstore.server.host.ServiceStarter. It starts Pravega SegmentStore service and a Metrics service is started as a sub service. public final class ServiceStarter { ... private StatsProvider statsProvider ; ... private void start () { ... log . info ( Initializing metrics provider ... ); MetricsConfig config = MetricsConfig . builder () . with ( MetricsConfig . METRICS_PREFIX , metrics-prefix ) . build (); MetricsProvider . initialize ( metricsConfig ); statsProvider = MetricsProvider . getMetricsProvider (); statsProvider . start (); // Here metric service is started as a sub-service ... } private void shutdown () { ... if ( this . statsProvider != null ) { statsProvider . close (); statsProvider = null ; log . info ( Metrics statsProvider is now closed. ); } ... } ... }","title":"2. Example for starting a Metric service"},{"location":"metrics/#21-example-for-dynamic-counter-and-opstatsloggertimer","text":"This is an example from io.pravega.segmentstore.server.host.handler.PravegaRequestProcessor. In this class, we registered two metrics: One timer (createStreamSegment), one dynamic counter (segmentReadBytes). public class PravegaRequestProcessor extends FailingRequestProcessor implements RequestProcessor { private static final StatsLogger STATS_LOGGER = MetricsProvider . createStatsLogger ( segmentstore ); private static final DynamicLogger DYNAMIC_LOGGER = MetricsProvider . getDynamicLogger (); private final OpStatsLogger createStreamSegment = STATS_LOGGER . createStats ( SEGMENT_CREATE_LATENCY ); private void handleReadResult ( ReadSegment request , ReadResult result ) { String segment = request . getSegment (); ArrayList ReadResultEntryContents cachedEntries = new ArrayList (); ReadResultEntry nonCachedEntry = collectCachedEntries ( request . getOffset (), result , cachedEntries ); boolean truncated = nonCachedEntry != null nonCachedEntry . getType () == Truncated ; boolean endOfSegment = nonCachedEntry != null nonCachedEntry . getType () == EndOfStreamSegment ; boolean atTail = nonCachedEntry != null nonCachedEntry . getType () == Future ; if (! cachedEntries . isEmpty () || endOfSegment ) { // We managed to collect some data. Send it. ByteBuffer data = copyData ( cachedEntries ); SegmentRead reply = new SegmentRead ( segment , request . getOffset (), atTail , endOfSegment , data ); connection . send ( reply ); DYNAMIC_LOGGER . incCounterValue ( nameFromSegment ( SEGMENT_READ_BYTES , segment ), reply . getData (). array (). length ); // Increasing the counter value for the counter metric SEGMENT_READ_BYTES } else if ( truncated ) { // We didn t collect any data, instead we determined that the current read offset was truncated. // Determine the current Start Offset and send that back. segmentStore . getStreamSegmentInfo ( segment , false , TIMEOUT ) . thenAccept ( info - connection . send ( new SegmentIsTruncated ( nonCachedEntry . getStreamSegmentOffset (), segment , info . getStartOffset ()))) . exceptionally ( e - handleException ( nonCachedEntry . getStreamSegmentOffset (), segment , Read segment , e )); } else { Preconditions . checkState ( nonCachedEntry != null , No ReadResultEntries returned from read!? ); nonCachedEntry . requestContent ( TIMEOUT ); nonCachedEntry . getContent () . thenAccept ( contents - { ByteBuffer data = copyData ( Collections . singletonList ( contents )); SegmentRead reply = new SegmentRead ( segment , nonCachedEntry . getStreamSegmentOffset (), false , endOfSegment , data ); connection . send ( reply ); DYNAMIC_LOGGER . incCounterValue ( nameFromSegment ( SEGMENT_READ_BYTES , segment ), reply . getData (). array (). length ); // Increasing the counter value for the counter metric SEGMENT_READ_BYTES }) . exceptionally ( e - { if ( Exceptions . unwrap ( e ) instanceof StreamSegmentTruncatedException ) { // The Segment may have been truncated in Storage after we got this entry but before we managed // to make a read. In that case, send the appropriate error back. connection . send ( new SegmentIsTruncated ( nonCachedEntry . getStreamSegmentOffset (), segment , nonCachedEntry . getStreamSegmentOffset ())); } else { handleException ( nonCachedEntry . getStreamSegmentOffset (), segment , Read segment , e ); } return null ; }) . exceptionally ( e - handleException ( nonCachedEntry . getStreamSegmentOffset (), segment , Read segment , e )); } } @Override public void createSegment ( CreateSegment createStreamsSegment ) { Timer timer = new Timer (); Collection AttributeUpdate attributes = Arrays . asList ( new AttributeUpdate ( SCALE_POLICY_TYPE , AttributeUpdateType . Replace , (( Byte ) createStreamsSegment . getScaleType ()). longValue ()), new AttributeUpdate ( SCALE_POLICY_RATE , AttributeUpdateType . Replace , (( Integer ) createStreamsSegment . getTargetRate ()). longValue ()), new AttributeUpdate ( CREATION_TIME , AttributeUpdateType . None , System . currentTimeMillis ()) ); if (! verifyToken ( createStreamsSegment . getSegment (), createStreamsSegment . getRequestId (), createStreamsSegment . getDelegationToken (), READ_UPDATE , Create Segment )) { return ; } log . debug ( Creating stream segment {} , createStreamsSegment ); segmentStore . createStreamSegment ( createStreamsSegment . getSegment (), attributes , TIMEOUT ) . thenAccept ( v - { createStreamSegment . reportSuccessEvent ( timer . getElapsed ()); // Reporting success event for Timer metric createStreamSegment connection . send ( new SegmentCreated ( createStreamsSegment . getRequestId (), createStreamsSegment . getSegment ())); }) . whenComplete (( res , e ) - { if ( e == null ) { if ( statsRecorder != null ) { statsRecorder . createSegment ( createStreamsSegment . getSegment (), createStreamsSegment . getScaleType (), createStreamsSegment . getTargetRate ()); } } else { createStreamSegment . reportFailEvent ( timer . getElapsed ()); // Reporting fail event for Timer metric createStreamSegment handleException ( createStreamsSegment . getRequestId (), createStreamsSegment . getSegment (), Create segment , e ); } }); } \u2026 } From the above example, we can see the reuired steps of how to register and use a metric in desired class and method: Get a StatsLogger from MetricsProvider: StatsLogger STATS_LOGGER = MetricsProvider.getStatsLogger(); Register all the desired metrics through StatsLogger: static final OpStatsLogger CREATE_STREAM_SEGMENT = STATS_LOGGER.createStats(SEGMENT_CREATE_LATENCY); Use these metrics within code at appropriate place where you would like to collect and record the values. Metrics.CREATE_STREAM_SEGMENT.reportSuccessEvent(timer.getElapsedNanos()); Here CREATE_STREAM_SEGMENT is the name of this metric, and CREATE_STREAM_SEGMENT is the name of our Metrics logger, it will track operations of createSegment, and we will get the time of each createSegment operation happened, how long each operation takes, and other numbers computed based on them.","title":"2.1. Example for Dynamic Counter and OpStatsLogger(Timer)"},{"location":"metrics/#211-output-example-of-opstatslogger","text":"An example output of OpStatsLogger CREATE_SEGMENT reported through CSV reporter: $ cat CREATE_STREAM_SEGMENT.csv t,count,max,mean,min,stddev,p50,p75,p95,p98,p99,p999,mean_rate,m1_rate,m5_rate,m15_rate,rate_unit,duration_unit 1480928806,1,8.973952,8.973952,8.973952,0.000000,8.973952,8.973952,8.973952,8.973952,8.973952,8.973952,0.036761,0.143306,0.187101,0.195605,calls/second,millisecond","title":"2.1.1 Output example of OpStatsLogger"},{"location":"metrics/#22-example-for-dynamic-gauge-and-opstatsloggerhistogram","text":"This is an example from io.pravega.controller.store.stream.AbstractStreamMetadataStore. In this class, we report a Dynamic Gauge which represents the open transactions and one histogram (CREATE_STREAM). public abstract class AbstractStreamMetadataStore implements StreamMetadataStore { ... private static final OpStatsLogger CREATE_STREAM = STATS_LOGGER . createStats ( MetricsNames . CREATE_STREAM ); // get stats logger from MetricsProvider private static final DynamicLogger DYNAMIC_LOGGER = MetricsProvider . getDynamicLogger (); // get dynamic logger from MetricsProvider ... @Override public CompletableFuture CreateStreamResponse createStream ( final String scope , final String name , final StreamConfiguration configuration , final long createTimestamp , final OperationContext context , final Executor executor ) { return withCompletion ( getStream ( scope , name , context ). create ( configuration , createTimestamp ), executor ) . thenApply ( result - { if ( result . getStatus (). equals ( CreateStreamResponse . CreateStatus . NEW )) { CREATE_STREAM . reportSuccessValue ( 1 ); // Report success event for histogram metric CREATE_STREAM DYNAMIC_LOGGER . reportGaugeValue ( nameFromStream ( OPEN_TRANSACTIONS , scope , name ), 0 ); // Report gauge value for Dynamic Gauge metric OPEN_TRANSACTIONS } return result ; }); } ... }","title":"2.2. Example for Dynamic Gauge and OpStatsLogger(Histogram)"},{"location":"metrics/#23-example-for-dynamic-meter","text":"This is an example from io.pravega.segmentstore.server.SegmentStoreMetrics. In this class, we report a Dynamic Meter which represents the segments created. public final class SegmentStoreMetrics { private static final DynamicLogger DYNAMIC_LOGGER = MetricsProvider . getDynamicLogger (); public void createSegment () { DYNAMIC_LOGGER . recordMeterEvents ( this . createSegmentCount , 1 ); // Record event for meter metric createSegmentCount } }","title":"2.3 Example for Dynamic Meter"},{"location":"metrics/#3-metric-reporter-and-configurations","text":"Reporters are the way through which we export all the measurements being made by the metrics. We currently provide StatsD and CSV output. It is not difficult to add new output formats, such as JMX/SLF4J. CSV reporter will export each Metric out into one file. StatsD reporter will export Metrics through UDP/TCP to a StatsD server. The reporter could be configured through MetricsConfig. public class MetricsConfig extends ComponentConfig { //region Members public static final String COMPONENT_CODE = metrics ; public final static String ENABLE_STATISTICS = enableStatistics ; //enable metric, or will report nothing, default = true, public final static Property Long DYNAMIC_CACHE_SIZE = dynamicCacheSize ; //dynamic cache size , default = 10000000L public final static Property Integer DYNAMIC_CACHE_EVICTION_DURATION_MINUTES = dynamicCacheEvictionDurationMs ; //dynamic cache evcition duration, default = 30 public final static String OUTPUT_FREQUENCY = statsOutputFrequencySeconds ; //reporter output frequency, default = 60 public final static String METRICS_PREFIX = metricsPrefix ; //Metrics Prefix, default = pravega public final static String CSV_ENDPOINT = csvEndpoint ; // CSV reporter output dir, default = /tmp/csv public final static String STATSD_HOST = statsDHost ; // StatsD server host for the reporting, default = localhost public final static String STATSD_PORT = statsDPort ; // StatsD server port, default = 8125 public final static Property String GRAPHITE_HOST = graphiteHost ; // Graphite server host for the reporting, default = localhost public final static Property Integer GRAPHITE_PORT = graphitePort ; // Graphite server port, default = 2003 public final static Property String JMX_DOMAIN = jmxDomain ; // JMX domain for the reporting, default = domain public final static Property String GANGLIA_HOST = gangliaHost ; // Ganglia server host for the reporting, default = localhost public final static Property Integer GANGLIA_PORT = gangliaPort ; // Ganglia server port, default = 8649 public final static Property Boolean ENABLE_CSV_REPORTER = enableCSVReporter ; // Enables CSV reporter, default = true public final static Property Boolean ENABLE_STATSD_REPORTER = enableStatsdReporter ; // Enables StatsD reporter, default = true public final static Property Boolean ENABLE_GRAPHITE_REPORTER = enableGraphiteReporter ; // Enables Graphite reporter, default = false public final static Property Boolean ENABLE_JMX_REPORTER = enableJMXReporter ; // Enables JMX reporter, default = false public final static Property Boolean ENABLE_GANGLIA_REPORTER = enableGangliaReporter ; // Enables Ganglia reporter, default = false public final static Property Boolean ENABLE_CONSOLE_REPORTER = enableConsoleReporter ; // Enables Console reporter, default = false ... }","title":"3. Metric reporter and Configurations"},{"location":"metrics/#4-steps-to-add-your-own-metrics","text":"Step 1. When start a segment store/controller service, start a Metrics service as a sub service. Reference above example in ServiceStarter.start() statsProvider = MetricsProvider . getProvider (); statsProvider . start ( metricsConfig ); Step 2. In the class that need Metrics: get StatsLogger through MetricsProvider; then get Metrics from StatsLogger; at last report it at the right place. ```java static final StatsLogger STATS_LOGGER = MetricsProvider.getStatsLogger(); --- 1 static final DynamicLogger DYNAMIC_LOGGER = MetricsProvider.getDynamicLogger(); public static class Metrics { --- 2 //Using Stats Logger static final String CREATE_STREAM = \"stream_created\"; static final OpStatsLogger CREATE_STREAM = STATS_LOGGER.createStats(CREATE_STREAM); static final String SEGMENT_CREATE_LATENCY = \"segment_create_latency_ms\"; static final OpStatsLogger createStreamSegment = STATS_LOGGER.createStats(SEGMENT_CREATE_LATENCY); //Using Dynamic Logger static final String SEGMENT_READ_BYTES = segmentstore.segment_read_bytes ; //Dynamic Counter static final String OPEN_TRANSACTIONS = controller.transactions_opened ; //Dynamic Gauge ... } //to report success or increment Metrics.CREATE_STREAM.reportSuccessValue(1); --- 3 Metrics.createStreamSegment.reportSuccessEvent(timer.getElapsed()); DYNAMIC_LOGGER.incCounterValue(Metrics.SEGMENT_READ_BYTES, 1); DYNAMIC_LOGGER.reportGaugeValue(OPEN_TRANSACTIONS, 0); //in case of failure Metrics.CREATE_STREAM.reportFailValue(1); Metrics.createStreamSegment.reportFailEvent(timer.getElapsed()); //to freeze DYNAMIC_LOGGER.freezeCounter(Metrics.SEGMENT_READ_BYTES); DYNAMIC_LOGGER.freezeGaugeValue(OPEN_TRANSACTIONS); ```","title":"4. Steps to add your own Metrics"},{"location":"metrics/#5-available-metrics-and-their-names","text":"Metrics in Segment Store Service ```` segmentstore.segment_read_latency_ms segmentstore.segment_write_latency_ms segmentstore.segment_create_latency_ms //Dynamic segmentstore.segment_read_bytes.$scope.$stream.$segment.Counter segmentstore.segment_write_bytes.$scope.$stream.$segment.Counter ```` Tier-2 Storage Metrics: Read/Write Latency, Read/Write Rate ```` hdfs.tier2_read_latency_ms hdfs.tier2_write_latency_ms //Dynamic hdfs.tire2_read_bytes.Counter hdfs.tier2_write_bytes.Counter * Cache Metrics rocksdb.cache_insert_latency rocksdb.cache_get_latency ```` Tier-1 DurableDataLog Metrics: Read/Write Latency, Read/Write Rate ```` bookkeeper.bookkeeper_total_write_latency bookkeeper.bookkeeper_write_latency bookkeeper.bookkeeper_write_bytes bookkeeper.bookkeeper_write_queue_size bookkeeper.bookkeeper_write_queue_fill //Dynamic bookkeeper.bookkeeper_ledger_count.$containerId.Gauge ```` Container-specific metrics ```` process_operations_latency.$containerId process_operations_batch_size.$containerId operation_queue_size.$containerId operation_processor_in_flight.$containerId operation_queue_wait_time.$containerId operation_processor_delay_ms.$containerId operation_commit_latency_ms.$containerId operation_latency_ms.$containerId operation_commit_metadata_txn_count.$containerId operation_commit_memory_latency_ms.$containerId operation_log_size.$containerId //Dynamic container_append_count.$containerId.Meter container_append_offset_count.$containerId.Meter container_update_attributes_count.$containerId.Meter container_get_attributes_count.$containerId.Meter container_read_count.$containerId.Meter container_get_info_count.$containerId.Meter container_create_segment_count.$containerId.Meter container_delete_segment_count.$containerId.Meter container_merge_segment_count.$containerId.Meter container_seal_count.$containerId.Meter container_truncate_count.$containerId.Meter active_segments.$containerId.Gauge ```` Metrics in Controller ```` controller.stream_created controller.stream_sealed controller.stream_deleted //Dynamic controller.transactions_created.$scope.$stream.Counter controller.transactions_committed.$scope.$stream.Counter controller.transactions_aborted.$scope.$stream.Counter controller.transactions_opened.$scope.$stream.Gauge controller.transactions_timedout.$scope.$stream.Counter controller.segments_count.$scope.$stream.Gauge controller.$scope.$stream.segments_splits.$scope.$stream.Counter controller.$scope.$stream.segments_merges.$scope.$stream.Counter controller.retention_frequency.$scope.$stream.Meter controller.truncated_size..$scope.$stream.Gauge * General Metrics cache_size_bytes cache_gen thread_pool_queue_size thread_pool_active_threads ````","title":"5. Available Metrics and their names"},{"location":"metrics/#6-useful-links","text":"Dropwizard Metrics Statsd_spec etsy_StatsD","title":"6. Useful links"},{"location":"pravega-concepts/","text":"Pravega Concepts Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. This page is an overview of the key concepts in Pravega. See Terminology for a concise definition for many Pravega concepts. Streams Pravega organizes data into Streams. A Stream is a durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. A Pravega Stream is similar to but more flexible than a \"topic\" in popular message-oriented middleware such as RabbitMQ or Apache Kafka . Pravega Streams are based on an append-only log data structure. By using append-only logs, Pravega can rapidly ingest data into durable storage, and support a large variety of application use cases such as stream processing using frameworks like Flink , publish/subscribe messaging, NoSQL databases such as a Time Series Database (TSDB), workflow engines, event-oriented applications and many other kinds of applications. When a developer creates a Stream in Pravega, s/he gives the Stream a meaningful name such as \"IoTSensorData\" or \"WebApplicationLog20170330\". The Stream's name helps other developers understand the kind of data that is stored in the Stream. It is also worth noting that Pravega Stream names are organized within a Scope. A Scope is a string and should convey some sort of meaning to developers such as \"FactoryMachines\" or \"HRWebsitelogs\". A Scope acts as a namespace for Stream names \u2013 all Stream names are unique within a Scope. Therefore a Stream is uniquely identified by the combination of its Stream name and Scope. Scope can be used to segregate names by tenant (in a multi tenant environment), by department in an organization, by geographic location or any other categorization the developer chooses. A Stream is unbounded in size \u2013 Pravega itself does not impose any limits on how many Events can be in a Stream or how many total bytes are stored in a Stream. Pravega\u2019s design horizontally scales from few machines to a whole datacenter\u2019s capacity. To deal with a potentially large amount of data within a Stream, Pravega Streams are divided into Stream Segments. A Stream Segment is a shard, or partition of the data within a Stream. We go into a lot more detail on Stream Segments a bit later in this document. Stream Segments are an important concept, but we need to introduce a few more things before we can dive into Stream Segments. Applications, such as a Java program reading from an IoT sensor, write data to the tail (front) of the Stream. Applications, such as a Flink analytics job, can read from any point in the Stream. Lots of applications can read and write the same Stream in parallel. Elastic, scalable support for a large volume of Streams, data and applications is at the heart of Pravega's design. We will get into more details about how applications read and write Streams a bit later in this document when we detail Readers and Writers. Events Pravega's client API allows applications to read and write data in Pravega in terms of an Event. An Event is a set of bytes within a Stream. An Event could be as simple as a small number of bytes containing a temperature reading from an IoT sensor composed of a timestamp, a metric identifier and a value. An Event could be web log data associated with a user click on a website. Events can be anything you can represent as a set of bytes. Applications make sense of Events using standard Java serializers and deserializers, allowing them to read and write objects in Pravega using similar techniques to reading and writing objects from files. Every Event has a Routing Key. A Routing Key allows Pravega and application developers to reason about which Events are related. A Routing Key is just a string that developers use to group similar Events together. A Routing Key is often derived from data naturally occurring in the Event, something like \"customer-id\" or \"machine-id\", but it could also be some artificial String. A Routing Key could be something like a date (to group Events together by time) or perhaps a Routing Key could be a IoT sensor id (to group Events by machine). A Routing Key is important to defining the precise read and write semantics that Pravega guarantees, we will get into that detail a bit later, after we have introduced a few more key concepts. Writers, Readers, ReaderGroups Pravega provides a client library, written in Java, that implements a convenient API for Writer and Reader applications to use. The Pravega Java Client Library encapsulates the wire protocol used to communicate between Pravega clients and Pravega. A Writer is an application that creates Events and writes them into a Stream. All data is written by appending to the tail (front) of a Stream. A Reader is an application that reads Events from a Stream. Readers can read from any point in the Stream. Many Readers will be reading Events from the tail of the Stream. These Events will be delivered to Readers as quickly as possible. Some Readers will read from earlier parts of the Stream (called catch-up reads). The application developer has control over where in the Stream the Reader starts reading. Pravega has the concept of a Position, that represents where in a Stream a Reader is currently located. The Position object can be used as a recovery mechanism \u2013 applications that persist the last Position a Reader has successfully processed can use that information to initialize a replacement Reader to pickup where a failed Reader left off. Using this pattern of persisting Position objects, applications can be built that guarantee exactly once Event processing in the face of Reader failure. Readers are organized into ReaderGroups. A ReaderGroup is a named collection of Readers that together, in parallel, read Events from a given Stream. When a Reader is created through the Pravega data plane API, the developer includes the name of the ReaderGroup it is part of. We guarantee that each Event published to a Stream is sent to exactly one Reader within the ReaderGroup. There could be 1 Reader in the ReaderGroup, there could be many. There could be many different ReaderGroups simultaneously reading from any given Stream. You can think of a ReaderGroup as a \"composite Reader\" or a \"distributed Reader\", that allows a distributed application to read and process Stream data in parallel, such that a massive amount of Stream data can be consumed by a coordinated fleet of Readers in a ReaderGroup. A collection of Flink tasks processing Stream data in parallel is a good example use of a ReaderGroup. For more details on the basics of working with Pravega Readers and Writers, see Working with Pravega: Basic Reader and Writer . We need to talk in more detail about the relationship between Readers, ReaderGroups and Streams and the ordering guarantees provided by Pravega. But first, we need to describe what a Stream Segment is. Stream Segments A Stream is decomposed into a set of Stream Segments; a Stream Segment is a shard or partition of a Stream. An Event is Stored within a Stream Segment The Stream Segment is the container for Events within the Stream. When an Event is written into a Stream, it is stored in one of the Stream Segments based on the Event's Routing Key. Pravega uses consistent hashing to assign Events to Stream Segments. Event Routing Keys are hashed to form a \"key space\" . The key space is then divided into a number of partitions, corresponding to the number of Stream Segments. Consistent hashing determines which Segment an Event is assigned to. AutoScaling: The number of Stream Segments can vary over time The number of Stream Segments in a Stream can grow and shrink over time as I/O load on the Stream increases and decreases. We refer to this feature as AutoScaling. Consider the following figure that shows the relationship between Routing Keys and time. A Stream starts out at time t0 with a configurable number of Segments. If the rate of data written to the Stream is constant, the number of Segments won\u2019t change. However at time t1, the system noted an increase in the ingestion rate and chose to split Segment 1 into two parts. We call this a Scale-up event. Before t1, Events with a Routing Key that hashes to the upper part of the key space (values 200-399) would be placed in Segment 1 and those that hash into the lower part of the key space (values 0-199) would be placed in Segment 0. After t1, Segment 1 is split into Segment 2 and Segment 3. Segment 1 is sealed and it no longer accepts writes. At this point in time, Events with Routing Key 300 and above are written to Segment 3 and those between 200 and 299 would be written into Segment 2. Segment 0 still keeps accepting the same range of Events as before t1. We also see another Scale-up event at time t2, as Segment 0\u2019s range of Routing Key is split into Segment 5 Segment 4. Also at this time, Segment 0 is sealed off so that it accepts no further writes. Segments covering a contiguous range of the key space can also be merged. At time t3, Segment 2\u2019s range and Segment 5\u2019s range are merged into Segment 6 to accommodate a decrease in load on the Stream. When a Stream is created, it is configured with an Scaling Policy that determines how a Stream reacts to changes in its load. Currently there are three kinds of Scaling Policy: Fixed. The number of Stream Segments does not vary with load Size-based. As the number of bytes of data per second written to the Stream increases past a certain target rate, the number of Stream Segments is increased. If it falls below a certain level, decrease the number of Stream Segments. Event-based. Similar to Size-based Scaling Policy, except it uses the number of Events instead of the number of bytes. Events, Stream Segments and AutoScaling We mentioned previously that an Event is written into one of the Stream's Segments. Taking into account AutoScaling, you should think of Stream Segments as a bucketing of Events based on Routing Key and time. At any given time, Events published to a Stream within a given value of Routing Key will all appear in the same Stream Segment. It is also worth emphasizing that Events are written into only the active Stream Segments. Segments that are sealed do not accept writes. In the figure above, at time \"now\", only Stream Segments 3, 6 and 4 are active and between those three Stream Segments the entire key space is covered. Stream Segments and ReaderGroups Stream Segments are important to understanding the way Reader Groups work. Pravega assigns each Reader in a ReaderGroup zero or more Stream Segments to read from. Pravega tries to balance out the number of Stream Segments each Reader is assigned. In the figure above, Reader B1 reads from 2 Stream Segments while each of the other Readers in the Reader Group have only 1 Stream Segment to read from. Pravega makes sure that each Stream Segment is read by exactly one Reader in any ReaderGroup configured to read from that Stream. As Readers are added to the ReaderGroup, or Readers crash and are removed from the ReaderGroup, Pravega reassigns Stream Segments so that Stream Segments are balanced amongst the Readers. The number of Stream Segments in a Stream determines the upper bound of parallelism of readers within a ReaderGroup \u2013 the more Stream Segments, the more separate, parallel sets of Readers we can have consuming the Stream. In the above figure, Stream1 has 4 Stream Segments. That means that the largest effective Reader Group would contain 4 Readers. Reader Group named \"B\" in the above figure is not quite optimal. If one more Reader was added to the ReaderGroup, each Reader would have 1 Stream Segment to process, maximizing read parallelism. However, the number of Readers in the ReaderGroup increases beyond 4, at least one of the Readers will not be assigned a Stream Segment. If Stream1 in the figure above experienced a Scale-Down event, reducing the number of Stream Segments to 3, then Reader Group B as depicted would have an ideal number of Readers. With the AutoScaling feature, Pravega developers don't have to configure their Streams with a fixed, pre-determined number of Stream Segments \u2013 Pravega can dynamically determine the right number. With this feature, Pravega Streams can grow and shrink to match the behavior of the data input. The size of any Stream is limited only by the total storage capacity made available to the Pravega cluster; if you need bigger streams, simply add more storage to your cluster. Applications can react to changes in the number of Segments in a Stream, adjusting the number of Readers within a ReaderGroup, to maintain optimal read parallelism if resources allow. This is useful, for example in a Flink application, to allow Flink to increase or decrease the number of task instances that are processing a Stream in parallel, as scale events occur over time. Ordering Guarantees A stream comprises a set of segments that can change over time. Segments that overlap in their area of keyspace have a defined order. An event written to a stream is written to a single segment and it is totally ordered with respect to the events of that segment. The existance and position of an event within a segment is strongly consistent. Readers can be assigned multiple parallel segments (from different parts of keyspace). A reader reading from multiple segments will interleave the events of the segments, but the order of events per segment respects the one of the segment. Specifically, if s is a segment, events e~1 and e~2 of s are such that e~1 precedes e~2, and a reader reads both e~1 and e~2, then the reader will read e~1 before e~2. This results in the following ordering guarantees: Events with the same Routing Key are consumed in the order they were written. Events with different Routing Keys sent to a specific segment will always be seen in the same order even if the Reader backs up and re-reads them. If an event has been acked to its writer or has been read by a reader it is guaranteed that it will continue to exist in the same place for all subsequent reads until it is deleted. If there are multiple Readers reading a Stream and they all back up to any given point, they will never see any reordering with respect to that point. (It will never be the case that an event that they read before the chosen point now comes after or vice versa.) ReaderGroup Checkpoints Pravega provides the ability for an application to initiate a Checkpoint on a ReaderGroup. The idea with a Checkpoint is to create a consistent \"point in time\" persistence of the state of each Reader in the ReaderGroup, by using a specialized Event (a Checkpoint Event) to signal each Reader to preserve its state. Once a Checkpoint has been completed, the application can use the Checkpoint to reset all the Readers in the ReaderGroup to the known consistent state represented by the Checkpoint. For more details on working with ReaderGroups, see ReaderGroup Basics . Transactions Pravega supports Transactions. The idea of a Transaction is that a Writer can \"batch\" up a bunch of Events and commit them as a unit into a Stream. This is useful, for example, with Flink jobs, using Pravega as a sink. The Flink job can continuously produce results of some data processing and use the Transaction to durably accumulate the results of the processing. At the end of some sort of time window (for example) the Flink job can commit the Transaction and therefore make the results of the processing available for downstream processing, or in the case of an error, abort the Transaction and the results disappear. A key difference between Pravega's Transactions and similar approaches (such as Kafka's producer-side batching) is related to durability. Events added to a Transaction are durable when the Event is ack'd back to the Writer. However, the Events in the Transaction are NOT visible to readers until the Transaction is committed by the Writer. A Transaction is a lot like a Stream; a Transaction is associated with multiple Stream Segments. When an Event is published into a Transaction, the Event itself is appended to a Stream Segment of the Transaction. Say a Stream had 5 Segments, when a Transaction is created on that Stream, conceptually that Transaction also has 5 Segments. When an Event is published into the Transaction, it is routed to the same numbered Segment as if it were published to the Stream itself (if the Event would have been placed in Segment 3 in the \"real\" Stream, then it will appear in Segment 3 of the Transaction). When the Transaction is committed, each of the Transaction's Segments is automatically appended to the corresponding Segment in the real Stream. If the Stream is aborted, the Transaction, all its Segments and all the Events published into the Transaction are removed from Pravega. Events published into a Transaction are never visible to the Reader until that Transaction is committed. For more details on working with Transactions, see Working with Pravega: Transactions . State Synchronizers Pravega is a streaming storage primitive; it can also be thought of as a mechanism to coordinate processes in a distributed computing environment. The State Synchronizer feature of Pravega falls into the latter category. A State Synchronizer uses a Pravega Stream to provide a synchronization mechanism for state shared between multiple processes running in a cluster, making it easier to build distributed applications. With State Synchronizer, an app developer can use Pravega to read and make changes to shared state with consistency and optimistic locking. State Synchronizer could be used to maintain a single, shared copy of an application's configuration property across all instances of that application in a cloud. State Synchronizer could also be used to store one piece of data or a map with thousands of different key value pairs. In fact, Pravega itself uses State Synchronizer internally, to manage the state of ReaderGroups and Readers distributed throughout the network. An app developer creates a State Synchronizer on a Stream in a fashion similar to how s/he creates a Writer. The State Synchronizer keeps a local copy of the shared state to make access to the data really fast for the application. Any changes to shared state are written through the StateSynchronizer to the Stream keeping track of all changes to the shared state. Each application instance uses the State Synchronizer to stay up to date with changes by pulling updates to shared state and modifying the local copy of the data. Consistency is maintained through a conditional append style of updates to the shared state through the State Synchronizer, making sure that updates are made only to the most recent version of the shared state. The State Synchronizer can occasionally be \"compacted\", compressing and removing old state updates so that only the most recent version of the state is kept in the backing stream. This feature helps app developers make sure that shared state does not grow unchecked. State Synchronizer works best when most updates to shared state are small in comparison to the total data size being stored, allowing them to be written as small deltas. As with any optimistic concurrency system, State Synchronizer is not at its best when many processes are all attempting to simultaneously update the same piece of data. For more details on working with State Synchronizers, see Working with Pravega: State Synchronizer . Architecture The following figure depicts the components deployed by Pravega: Pravega is deployed as a distributed system \u2013 a cluster of servers and storage coordinated to run Pravega called a \"Pravega cluster\". Pravega presents a software-defined storage (SDS) architecture formed by Controller instances (control plane) and Pravega Servers (data plane).The set of Pravega Servers is known collectively as the Segment Store. The set of Controller instances make up the control plane of Pravega, providing functionality to create, update and delete Streams, retrieve information about Streams, monitor the health of the Pravega cluster, gather metrics etc. There are usually multiple (recommended at least 3) Controller instances running in a cluster for high availability. The Segment Store implements the Pravega data plane. Pravega Servers provide the API to read and write data in Streams. Data storage is comprised of two tiers: Tier 1 Storage, which provides short term, low-latency, data storage, guaranteeing the durability of data written to Streams and Tier 2 Storage providing longer term storage of Stream data. Pravega uses Apache Bookkeeper to implement Tier 1 Storage and uses HDFS, Dell EMC's Isilon or Dell EMC's Elastic Cloud Storage (ECS) to implement Tier 2 Storage. Tier 1 Storage typically runs within the Pravega cluster. Tier 2 Storage is normally deployed outside the Pravega cluster. Tiering storage is important to deliver the combination of fast access to Stream data but also allow Streams to store a vast amount of data. Tier 1 storage persists the most recently written Stream data. As data in Tier 1 Storage ages, it is moved into Tier 2 Storage. Pravega uses Apache Zookeeper as the coordination mechanism for the components in the Pravega cluster. Pravega is built as a data storage primitive first and foremost. Pravega is carefully designed to take advantage of software defined storage so that the amount of data stored in Pravega is limited only by the total storage capacity of your data center. And like you would expect from a storage primitive, once data is written to Pravega it is durably stored. Short of a disaster that permanently destroys a large portion of a data center, data stored in Pravega is never lost. Pravega provides a client library, written in Java, for building client-side applications such as analytics applications using Flink. The Pravega Java Client Library manages the interaction between application code and Pravega via a custom TCP wire protocol. Putting the Concepts Together The concepts in Pravega are summarized in the following figure: Pravega clients are Writers and Readers. Writers write Events into a Stream. Readers read Events from a Stream. Readers are grouped into ReaderGroups to read from a Stream in parallel. The Controller is a server-side component that manages the control plane of Pravega. Streams are created, updated and listed using the Controller API. The Pravega Server is a server-side component that implements read, write and other data plane operations. Streams are the fundamental storage primitive in Pravega. Streams contain a set of data elements called Events. Events are appended to the \u201ctail\u201d of the Stream by Writers. Readers can read Events from anywhere in the Stream. A Stream is partitioned into a set of Stream Segments. The number of Stream Segments in a Stream can change over time. Events are written into exactly one of the Stream Segments based on Routing Key. For any ReaderGroup reading a Stream, each Stream Segment is assigned to one Reader in that ReaderGroup. Each Stream Segment is stored in a combination of Tier1 and Tier2 storage. The tail of the Segment is stored in Tier1 providing low latency reads and writes. The rest of the Segment is stored in Tier2, providing high throughput read access with horizontal scalibility and low cost. A Note on Tiered Storage To deliver an efficient implementation of Streams, Pravega is based on a tiered storage model. Events are persisted in low latency/high IOPS storage (Tier 1 Storage) and higher throughput storage (Tier 2 Storage). Writers and Readers are oblivious to the tiered storage model from an API perspective. Pravega is based on an append-only Log data structure. As Leigh Stewart observed , there are really three data access mechanisms in a Log: All of the write activity, and much of the read activity happens at the tail of the log. Writes are appended to the log and many clients want to read data as fast as it arrives in the log. These two data access mechanisms are dominated by the need for low-latency \u2013 low latency writes by Writers and near real time access to the published data by Readers. Not all Readers read from the tail of the log; some Readers want to read starting at some arbitrary position in the log. These reads are known as catch-up reads . Access to historical data traditionally was done by batch analytics jobs, often using HDFS and Map/Reduce. However with new streaming applications, you can access historical data as well as current data by just accessing the log. One approach would be to store all the historical data in SSDs like we do with the tail data, but that can get very expensive and force customers to economize by deleting historical data. Pravega offers a mechanism that allows customers to use cost-effective, highly-scalable, high-throughput storage for the historical part of the log, that way they won\u2019t have to decide when to delete historical data. Basically, if storage is cheap enough, why not keep all of the history? Tier 1 Storage is used to make writing to Streams fast and durable and to make sure reading from the tail of a Stream is as fast as possible. Tier 1 Storage is based on the open source Apache BookKeeper Project. Though not essential, we presume that the Tier 1 Storage will be typically implemented on faster SSDs or even non-volatile RAM. Tier 2 Storage provides a highly-scalable, high-throughput cost-effective storage. We expect this tier to be typically deployed on spinning disks. Pravega asynchronously migrates Events from Tier 1 to Tier 2 to reflect the different access patterns to Stream data. Tier 2 Storage is based on an HDFS model.","title":"Pravega Concepts"},{"location":"pravega-concepts/#pravega-concepts","text":"Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. This page is an overview of the key concepts in Pravega. See Terminology for a concise definition for many Pravega concepts.","title":"Pravega Concepts"},{"location":"pravega-concepts/#streams","text":"Pravega organizes data into Streams. A Stream is a durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. A Pravega Stream is similar to but more flexible than a \"topic\" in popular message-oriented middleware such as RabbitMQ or Apache Kafka . Pravega Streams are based on an append-only log data structure. By using append-only logs, Pravega can rapidly ingest data into durable storage, and support a large variety of application use cases such as stream processing using frameworks like Flink , publish/subscribe messaging, NoSQL databases such as a Time Series Database (TSDB), workflow engines, event-oriented applications and many other kinds of applications. When a developer creates a Stream in Pravega, s/he gives the Stream a meaningful name such as \"IoTSensorData\" or \"WebApplicationLog20170330\". The Stream's name helps other developers understand the kind of data that is stored in the Stream. It is also worth noting that Pravega Stream names are organized within a Scope. A Scope is a string and should convey some sort of meaning to developers such as \"FactoryMachines\" or \"HRWebsitelogs\". A Scope acts as a namespace for Stream names \u2013 all Stream names are unique within a Scope. Therefore a Stream is uniquely identified by the combination of its Stream name and Scope. Scope can be used to segregate names by tenant (in a multi tenant environment), by department in an organization, by geographic location or any other categorization the developer chooses. A Stream is unbounded in size \u2013 Pravega itself does not impose any limits on how many Events can be in a Stream or how many total bytes are stored in a Stream. Pravega\u2019s design horizontally scales from few machines to a whole datacenter\u2019s capacity. To deal with a potentially large amount of data within a Stream, Pravega Streams are divided into Stream Segments. A Stream Segment is a shard, or partition of the data within a Stream. We go into a lot more detail on Stream Segments a bit later in this document. Stream Segments are an important concept, but we need to introduce a few more things before we can dive into Stream Segments. Applications, such as a Java program reading from an IoT sensor, write data to the tail (front) of the Stream. Applications, such as a Flink analytics job, can read from any point in the Stream. Lots of applications can read and write the same Stream in parallel. Elastic, scalable support for a large volume of Streams, data and applications is at the heart of Pravega's design. We will get into more details about how applications read and write Streams a bit later in this document when we detail Readers and Writers.","title":"Streams"},{"location":"pravega-concepts/#events","text":"Pravega's client API allows applications to read and write data in Pravega in terms of an Event. An Event is a set of bytes within a Stream. An Event could be as simple as a small number of bytes containing a temperature reading from an IoT sensor composed of a timestamp, a metric identifier and a value. An Event could be web log data associated with a user click on a website. Events can be anything you can represent as a set of bytes. Applications make sense of Events using standard Java serializers and deserializers, allowing them to read and write objects in Pravega using similar techniques to reading and writing objects from files. Every Event has a Routing Key. A Routing Key allows Pravega and application developers to reason about which Events are related. A Routing Key is just a string that developers use to group similar Events together. A Routing Key is often derived from data naturally occurring in the Event, something like \"customer-id\" or \"machine-id\", but it could also be some artificial String. A Routing Key could be something like a date (to group Events together by time) or perhaps a Routing Key could be a IoT sensor id (to group Events by machine). A Routing Key is important to defining the precise read and write semantics that Pravega guarantees, we will get into that detail a bit later, after we have introduced a few more key concepts.","title":"Events"},{"location":"pravega-concepts/#writers-readers-readergroups","text":"Pravega provides a client library, written in Java, that implements a convenient API for Writer and Reader applications to use. The Pravega Java Client Library encapsulates the wire protocol used to communicate between Pravega clients and Pravega. A Writer is an application that creates Events and writes them into a Stream. All data is written by appending to the tail (front) of a Stream. A Reader is an application that reads Events from a Stream. Readers can read from any point in the Stream. Many Readers will be reading Events from the tail of the Stream. These Events will be delivered to Readers as quickly as possible. Some Readers will read from earlier parts of the Stream (called catch-up reads). The application developer has control over where in the Stream the Reader starts reading. Pravega has the concept of a Position, that represents where in a Stream a Reader is currently located. The Position object can be used as a recovery mechanism \u2013 applications that persist the last Position a Reader has successfully processed can use that information to initialize a replacement Reader to pickup where a failed Reader left off. Using this pattern of persisting Position objects, applications can be built that guarantee exactly once Event processing in the face of Reader failure. Readers are organized into ReaderGroups. A ReaderGroup is a named collection of Readers that together, in parallel, read Events from a given Stream. When a Reader is created through the Pravega data plane API, the developer includes the name of the ReaderGroup it is part of. We guarantee that each Event published to a Stream is sent to exactly one Reader within the ReaderGroup. There could be 1 Reader in the ReaderGroup, there could be many. There could be many different ReaderGroups simultaneously reading from any given Stream. You can think of a ReaderGroup as a \"composite Reader\" or a \"distributed Reader\", that allows a distributed application to read and process Stream data in parallel, such that a massive amount of Stream data can be consumed by a coordinated fleet of Readers in a ReaderGroup. A collection of Flink tasks processing Stream data in parallel is a good example use of a ReaderGroup. For more details on the basics of working with Pravega Readers and Writers, see Working with Pravega: Basic Reader and Writer . We need to talk in more detail about the relationship between Readers, ReaderGroups and Streams and the ordering guarantees provided by Pravega. But first, we need to describe what a Stream Segment is.","title":"Writers, Readers, ReaderGroups"},{"location":"pravega-concepts/#stream-segments","text":"A Stream is decomposed into a set of Stream Segments; a Stream Segment is a shard or partition of a Stream.","title":"Stream Segments"},{"location":"pravega-concepts/#an-event-is-stored-within-a-stream-segment","text":"The Stream Segment is the container for Events within the Stream. When an Event is written into a Stream, it is stored in one of the Stream Segments based on the Event's Routing Key. Pravega uses consistent hashing to assign Events to Stream Segments. Event Routing Keys are hashed to form a \"key space\" . The key space is then divided into a number of partitions, corresponding to the number of Stream Segments. Consistent hashing determines which Segment an Event is assigned to.","title":"An Event is Stored within a Stream Segment"},{"location":"pravega-concepts/#autoscaling-the-number-of-stream-segments-can-vary-over-time","text":"The number of Stream Segments in a Stream can grow and shrink over time as I/O load on the Stream increases and decreases. We refer to this feature as AutoScaling. Consider the following figure that shows the relationship between Routing Keys and time. A Stream starts out at time t0 with a configurable number of Segments. If the rate of data written to the Stream is constant, the number of Segments won\u2019t change. However at time t1, the system noted an increase in the ingestion rate and chose to split Segment 1 into two parts. We call this a Scale-up event. Before t1, Events with a Routing Key that hashes to the upper part of the key space (values 200-399) would be placed in Segment 1 and those that hash into the lower part of the key space (values 0-199) would be placed in Segment 0. After t1, Segment 1 is split into Segment 2 and Segment 3. Segment 1 is sealed and it no longer accepts writes. At this point in time, Events with Routing Key 300 and above are written to Segment 3 and those between 200 and 299 would be written into Segment 2. Segment 0 still keeps accepting the same range of Events as before t1. We also see another Scale-up event at time t2, as Segment 0\u2019s range of Routing Key is split into Segment 5 Segment 4. Also at this time, Segment 0 is sealed off so that it accepts no further writes. Segments covering a contiguous range of the key space can also be merged. At time t3, Segment 2\u2019s range and Segment 5\u2019s range are merged into Segment 6 to accommodate a decrease in load on the Stream. When a Stream is created, it is configured with an Scaling Policy that determines how a Stream reacts to changes in its load. Currently there are three kinds of Scaling Policy: Fixed. The number of Stream Segments does not vary with load Size-based. As the number of bytes of data per second written to the Stream increases past a certain target rate, the number of Stream Segments is increased. If it falls below a certain level, decrease the number of Stream Segments. Event-based. Similar to Size-based Scaling Policy, except it uses the number of Events instead of the number of bytes.","title":"AutoScaling:\u00a0The\u00a0number of Stream Segments can vary over time"},{"location":"pravega-concepts/#events-stream-segments-and-autoscaling","text":"We mentioned previously that an Event is written into one of the Stream's Segments. Taking into account AutoScaling, you should think of Stream Segments as a bucketing of Events based on Routing Key and time. At any given time, Events published to a Stream within a given value of Routing Key will all appear in the same Stream Segment. It is also worth emphasizing that Events are written into only the active Stream Segments. Segments that are sealed do not accept writes. In the figure above, at time \"now\", only Stream Segments 3, 6 and 4 are active and between those three Stream Segments the entire key space is covered.","title":"Events, Stream Segments and AutoScaling"},{"location":"pravega-concepts/#stream-segments-and-readergroups","text":"Stream Segments are important to understanding the way Reader Groups work. Pravega assigns each Reader in a ReaderGroup zero or more Stream Segments to read from. Pravega tries to balance out the number of Stream Segments each Reader is assigned. In the figure above, Reader B1 reads from 2 Stream Segments while each of the other Readers in the Reader Group have only 1 Stream Segment to read from. Pravega makes sure that each Stream Segment is read by exactly one Reader in any ReaderGroup configured to read from that Stream. As Readers are added to the ReaderGroup, or Readers crash and are removed from the ReaderGroup, Pravega reassigns Stream Segments so that Stream Segments are balanced amongst the Readers. The number of Stream Segments in a Stream determines the upper bound of parallelism of readers within a ReaderGroup \u2013 the more Stream Segments, the more separate, parallel sets of Readers we can have consuming the Stream. In the above figure, Stream1 has 4 Stream Segments. That means that the largest effective Reader Group would contain 4 Readers. Reader Group named \"B\" in the above figure is not quite optimal. If one more Reader was added to the ReaderGroup, each Reader would have 1 Stream Segment to process, maximizing read parallelism. However, the number of Readers in the ReaderGroup increases beyond 4, at least one of the Readers will not be assigned a Stream Segment. If Stream1 in the figure above experienced a Scale-Down event, reducing the number of Stream Segments to 3, then Reader Group B as depicted would have an ideal number of Readers. With the AutoScaling feature, Pravega developers don't have to configure their Streams with a fixed, pre-determined number of Stream Segments \u2013 Pravega can dynamically determine the right number. With this feature, Pravega Streams can grow and shrink to match the behavior of the data input. The size of any Stream is limited only by the total storage capacity made available to the Pravega cluster; if you need bigger streams, simply add more storage to your cluster. Applications can react to changes in the number of Segments in a Stream, adjusting the number of Readers within a ReaderGroup, to maintain optimal read parallelism if resources allow. This is useful, for example in a Flink application, to allow Flink to increase or decrease the number of task instances that are processing a Stream in parallel, as scale events occur over time.","title":"Stream Segments and ReaderGroups"},{"location":"pravega-concepts/#ordering-guarantees","text":"A stream comprises a set of segments that can change over time. Segments that overlap in their area of keyspace have a defined order. An event written to a stream is written to a single segment and it is totally ordered with respect to the events of that segment. The existance and position of an event within a segment is strongly consistent. Readers can be assigned multiple parallel segments (from different parts of keyspace). A reader reading from multiple segments will interleave the events of the segments, but the order of events per segment respects the one of the segment. Specifically, if s is a segment, events e~1 and e~2 of s are such that e~1 precedes e~2, and a reader reads both e~1 and e~2, then the reader will read e~1 before e~2. This results in the following ordering guarantees: Events with the same Routing Key are consumed in the order they were written. Events with different Routing Keys sent to a specific segment will always be seen in the same order even if the Reader backs up and re-reads them. If an event has been acked to its writer or has been read by a reader it is guaranteed that it will continue to exist in the same place for all subsequent reads until it is deleted. If there are multiple Readers reading a Stream and they all back up to any given point, they will never see any reordering with respect to that point. (It will never be the case that an event that they read before the chosen point now comes after or vice versa.)","title":"Ordering Guarantees"},{"location":"pravega-concepts/#readergroup-checkpoints","text":"Pravega provides the ability for an application to initiate a Checkpoint on a ReaderGroup. The idea with a Checkpoint is to create a consistent \"point in time\" persistence of the state of each Reader in the ReaderGroup, by using a specialized Event (a Checkpoint Event) to signal each Reader to preserve its state. Once a Checkpoint has been completed, the application can use the Checkpoint to reset all the Readers in the ReaderGroup to the known consistent state represented by the Checkpoint. For more details on working with ReaderGroups, see ReaderGroup Basics .","title":"ReaderGroup Checkpoints"},{"location":"pravega-concepts/#transactions","text":"Pravega supports Transactions. The idea of a Transaction is that a Writer can \"batch\" up a bunch of Events and commit them as a unit into a Stream. This is useful, for example, with Flink jobs, using Pravega as a sink. The Flink job can continuously produce results of some data processing and use the Transaction to durably accumulate the results of the processing. At the end of some sort of time window (for example) the Flink job can commit the Transaction and therefore make the results of the processing available for downstream processing, or in the case of an error, abort the Transaction and the results disappear. A key difference between Pravega's Transactions and similar approaches (such as Kafka's producer-side batching) is related to durability. Events added to a Transaction are durable when the Event is ack'd back to the Writer. However, the Events in the Transaction are NOT visible to readers until the Transaction is committed by the Writer. A Transaction is a lot like a Stream; a Transaction is associated with multiple Stream Segments. When an Event is published into a Transaction, the Event itself is appended to a Stream Segment of the Transaction. Say a Stream had 5 Segments, when a Transaction is created on that Stream, conceptually that Transaction also has 5 Segments. When an Event is published into the Transaction, it is routed to the same numbered Segment as if it were published to the Stream itself (if the Event would have been placed in Segment 3 in the \"real\" Stream, then it will appear in Segment 3 of the Transaction). When the Transaction is committed, each of the Transaction's Segments is automatically appended to the corresponding Segment in the real Stream. If the Stream is aborted, the Transaction, all its Segments and all the Events published into the Transaction are removed from Pravega. Events published into a Transaction are never visible to the Reader until that Transaction is committed. For more details on working with Transactions, see Working with Pravega: Transactions .","title":"Transactions"},{"location":"pravega-concepts/#state-synchronizers","text":"Pravega is a streaming storage primitive; it can also be thought of as a mechanism to coordinate processes in a distributed computing environment. The State Synchronizer feature of Pravega falls into the latter category. A State Synchronizer uses a Pravega Stream to provide a synchronization mechanism for state shared between multiple processes running in a cluster, making it easier to build distributed applications. With State Synchronizer, an app developer can use Pravega to read and make changes to shared state with consistency and optimistic locking. State Synchronizer could be used to maintain a single, shared copy of an application's configuration property across all instances of that application in a cloud. State Synchronizer could also be used to store one piece of data or a map with thousands of different key value pairs. In fact, Pravega itself uses State Synchronizer internally, to manage the state of ReaderGroups and Readers distributed throughout the network. An app developer creates a State Synchronizer on a Stream in a fashion similar to how s/he creates a Writer. The State Synchronizer keeps a local copy of the shared state to make access to the data really fast for the application. Any changes to shared state are written through the StateSynchronizer to the Stream keeping track of all changes to the shared state. Each application instance uses the State Synchronizer to stay up to date with changes by pulling updates to shared state and modifying the local copy of the data. Consistency is maintained through a conditional append style of updates to the shared state through the State Synchronizer, making sure that updates are made only to the most recent version of the shared state. The State Synchronizer can occasionally be \"compacted\", compressing and removing old state updates so that only the most recent version of the state is kept in the backing stream. This feature helps app developers make sure that shared state does not grow unchecked. State Synchronizer works best when most updates to shared state are small in comparison to the total data size being stored, allowing them to be written as small deltas. As with any optimistic concurrency system, State Synchronizer is not at its best when many processes are all attempting to simultaneously update the same piece of data. For more details on working with State Synchronizers, see Working with Pravega: State Synchronizer .","title":"State Synchronizers"},{"location":"pravega-concepts/#architecture","text":"The following figure depicts the components deployed by Pravega: Pravega is deployed as a distributed system \u2013 a cluster of servers and storage coordinated to run Pravega called a \"Pravega cluster\". Pravega presents a software-defined storage (SDS) architecture formed by Controller instances (control plane) and Pravega Servers (data plane).The set of Pravega Servers is known collectively as the Segment Store. The set of Controller instances make up the control plane of Pravega, providing functionality to create, update and delete Streams, retrieve information about Streams, monitor the health of the Pravega cluster, gather metrics etc. There are usually multiple (recommended at least 3) Controller instances running in a cluster for high availability. The Segment Store implements the Pravega data plane. Pravega Servers provide the API to read and write data in Streams. Data storage is comprised of two tiers: Tier 1 Storage, which provides short term, low-latency, data storage, guaranteeing the durability of data written to Streams and Tier 2 Storage providing longer term storage of Stream data. Pravega uses Apache Bookkeeper to implement Tier 1 Storage and uses HDFS, Dell EMC's Isilon or Dell EMC's Elastic Cloud Storage (ECS) to implement Tier 2 Storage. Tier 1 Storage typically runs within the Pravega cluster. Tier 2 Storage is normally deployed outside the Pravega cluster. Tiering storage is important to deliver the combination of fast access to Stream data but also allow Streams to store a vast amount of data. Tier 1 storage persists the most recently written Stream data. As data in Tier 1 Storage ages, it is moved into Tier 2 Storage. Pravega uses Apache Zookeeper as the coordination mechanism for the components in the Pravega cluster. Pravega is built as a data storage primitive first and foremost. Pravega is carefully designed to take advantage of software defined storage so that the amount of data stored in Pravega is limited only by the total storage capacity of your data center. And like you would expect from a storage primitive, once data is written to Pravega it is durably stored. Short of a disaster that permanently destroys a large portion of a data center, data stored in Pravega is never lost. Pravega provides a client library, written in Java, for building client-side applications such as analytics applications using Flink. The Pravega Java Client Library manages the interaction between application code and Pravega via a custom TCP wire protocol.","title":"Architecture"},{"location":"pravega-concepts/#putting-the-concepts-together","text":"The concepts in Pravega are summarized in the following figure: Pravega clients are Writers and Readers. Writers write Events into a Stream. Readers read Events from a Stream. Readers are grouped into ReaderGroups to read from a Stream in parallel. The Controller is a server-side component that manages the control plane of Pravega. Streams are created, updated and listed using the Controller API. The Pravega Server is a server-side component that implements read, write and other data plane operations. Streams are the fundamental storage primitive in Pravega. Streams contain a set of data elements called Events. Events are appended to the \u201ctail\u201d of the Stream by Writers. Readers can read Events from anywhere in the Stream. A Stream is partitioned into a set of Stream Segments. The number of Stream Segments in a Stream can change over time. Events are written into exactly one of the Stream Segments based on Routing Key. For any ReaderGroup reading a Stream, each Stream Segment is assigned to one Reader in that ReaderGroup. Each Stream Segment is stored in a combination of Tier1 and Tier2 storage. The tail of the Segment is stored in Tier1 providing low latency reads and writes. The rest of the Segment is stored in Tier2, providing high throughput read access with horizontal scalibility and low cost.","title":"Putting the Concepts Together"},{"location":"pravega-concepts/#a-note-on-tiered-storage","text":"To deliver an efficient implementation of Streams, Pravega is based on a tiered storage model. Events are persisted in low latency/high IOPS storage (Tier 1 Storage) and higher throughput storage (Tier 2 Storage). Writers and Readers are oblivious to the tiered storage model from an API perspective. Pravega is based on an append-only Log data structure. As Leigh Stewart observed , there are really three data access mechanisms in a Log: All of the write activity, and much of the read activity happens at the tail of the log. Writes are appended to the log and many clients want to read data as fast as it arrives in the log. These two data access mechanisms are dominated by the need for low-latency \u2013 low latency writes by Writers and near real time access to the published data by Readers. Not all Readers read from the tail of the log; some Readers want to read starting at some arbitrary position in the log. These reads are known as catch-up reads . Access to historical data traditionally was done by batch analytics jobs, often using HDFS and Map/Reduce. However with new streaming applications, you can access historical data as well as current data by just accessing the log. One approach would be to store all the historical data in SSDs like we do with the tail data, but that can get very expensive and force customers to economize by deleting historical data. Pravega offers a mechanism that allows customers to use cost-effective, highly-scalable, high-throughput storage for the historical part of the log, that way they won\u2019t have to decide when to delete historical data. Basically, if storage is cheap enough, why not keep all of the history? Tier 1 Storage is used to make writing to Streams fast and durable and to make sure reading from the tail of a Stream is as fast as possible. Tier 1 Storage is based on the open source Apache BookKeeper Project. Though not essential, we presume that the Tier 1 Storage will be typically implemented on faster SSDs or even non-volatile RAM. Tier 2 Storage provides a highly-scalable, high-throughput cost-effective storage. We expect this tier to be typically deployed on spinning disks. Pravega asynchronously migrates Events from Tier 1 to Tier 2 to reflect the different access patterns to Stream data. Tier 2 Storage is based on an HDFS model.","title":"A Note on Tiered Storage"},{"location":"reader-group-design/","text":"Reader Groups Design To be updated to reflect recent implementation details (05/07/2017) Motivation A set of Readers can be grouped together in order that the set of Events in a Stream can be read in parallel. The grouping of Readers is called a Readers Group. Pravega guarantees that each Event in the Stream is read by exactly one Reader in the Readers Group. Each Reader in a ReaderGroup is assigned zero or more Segments. The Reader assigned to a Segment is the only Reader within the ReaderGroup that reads Events from that Segment. This is the fundamental mechanism by which Pravega makes ordering guarantees of Event delivery to a Reader \u2013 a Reader will receive Events in the order they were published into a Segment. There are several challenges associated with this mechanism: - How to maintain the mapping of which Reader within a ReaderGroup is assigned which Segment - How to manage the above mapping when Segments split and merge - How to manage the above mapping when Readers are added to the ReaderGroup - How to manage the above mapping when Readers leave the ReaderGroup either by an explicit operation or the Reader becoming unavailable due to network outage or the Reader process failing To solve these problems we can use [[StateSynchronizer|StateSynchronizer-design]] to enable readers to coordinate among themselves. How Consistent replicated state can be used to solve the problem A consistent replicated state object representing the ReaderGroup metadata will be created in each reader. This ReaderGroup metadata consists of: * a map of online readers to the segments they own * a list of positions in segments that can be taken over. Every time the Readers in a ReaderGroup change, the state can be updated. Similarly each time one of the readers is going to start reading from a new segment, it can update the replicated state. This allows all readers to know about all the other Readers in their ReaderGroup and which segments they own. Given this information: * A new reader can infer which segments are available to read from. (By virtue of it being absent from the state) * Dealing with a segment being merged becomes easy, because the last reader to reach the end of its pre-merge segment knows it can freely take ownership of the new segment. * Readers can see their relative load and how they are progressing relative to the other readers in their group and can decide to transfer segments if things are out of balance. * This allows readers to take action directly to ensure all the events are read without the need for some external tracker. ReaderGroup APIs The external APIs to manage ReaderGroups could be added to the StreamManager object. They would consist of: ReaderGroup createReaderGroup(String name, Stream stream, ReaderGroupConfig config); ReaderGroup getReaderGroup(String name, Stream stream); void deleteReaderGroup(ReaderGroup group); When a ReaderGroup is created, it creates a [[StateSynchronizer|StateSynchronizer-design]] shared by the readers. To join a ReaderGroup readers would just specify it in their configuration: ReaderConfig cc = new ReaderConfig(props); Reader T reader = a_stream.createReader( my_reader_id , my_reader_group , cc); When readers join the group they use the state to determine which segments to read from. When they shut down they update the state so that other readers can take over their segments. Failure detector We still need some sort of heartbeating mechanism to tell if Readers are alive. The problem is greatly simplified because it need not produce a view of the cluster or manage any state. The component would just need to detect failures and invoke the void readerOffline(String readerId, Position lastPosition); api on the ReaderGroup For consistency, the Failure detector should not declare a host as dead that is still processing events. Doing so could violate exactly once processing guarantees. Examples New reader When a reader joins a group its online status is added to the shared state Other readers receive updates to the shared state. When a reader with more than average number of segments sees the new reader, it may give up a segment by writing its position for that segment to the shared state. The new reader can take over a segment by recording that it is doing so in the shared state. The new reader can start reading from the position it read from the shared state for the segment it picked up. There are no races between multiple readers coming online concurrently because only one of them can successfully claim ownership of any given segment. Segments merging When a reader comes to the end of its segment it records this information in the shared state. When all of the segments that are getting merged together are completed a reader may claim ownership of the following segment. There is no ambiguity as to who the owner is, because it is stored in the shared state. There is no risk of a segment being forgotten because any reader can see which segments are available by looking at the shared state and claim them. Reader going offline. When a reader dies the readerOffline() method will be invoked either by the reader itself in a graceful shutdown (internally to the close method) or via a liveness detector. In either case the reader's last position is passed in. The last position is written to the state. Other readers will see this when they update their local state. Any of them can decide to take over one or more of the segments owned by the old reader from where it left off by recording their intention to do so in the state object. Once the state has been updated the new reader is considered the owner of the segment and can read from it at will. What happens if a reader does not keep up to date A Reader with out-of-date state can read from their existing segments without interference. The only disadvantage to this is that they will not shed load to another reader should one become available. However because they have to write to the shared state to start reading from any segment they don't already own, they must fetch up-to-date information before moving on to a new segment. Impact of availability and latency Reading and updating the state object can occur in parallel to reading so there would likely be no visible latency impact. A stream would be unavailable for reading if Pravega failed in such a way that the segment containing the ReaderGroup information went down and remained offline for long enough for the readers to exhaust all the events in their existing segments. Of course if Pravega failed in this way, odds are at least some portion of the stream would also be directly impacted and not be able to read any events. This sort of failure mode would manifest as latency for the reader, similar to what would happen if they had reached the tail of the stream. This is preferable to using an external system to manage this coordination, as that would require adding new components that can fail in different ways, as opposed to further relying on a small set that we need to make highly available anyway. This is particularly notable in the case of a network partition. If the network is split any readers that are on the same side of the partition as the Pravega servers can continue working. If we were to utilize an external service, that service could be cut off and readers might not be able to make progress even if they could talk to Pravega.","title":"Reader Groups"},{"location":"reader-group-design/#reader-groups-design","text":"To be updated to reflect recent implementation details (05/07/2017)","title":"Reader Groups Design"},{"location":"reader-group-design/#motivation","text":"A set of Readers can be grouped together in order that the set of Events in a Stream can be read in parallel. The grouping of Readers is called a Readers Group. Pravega guarantees that each Event in the Stream is read by exactly one Reader in the Readers Group. Each Reader in a ReaderGroup is assigned zero or more Segments. The Reader assigned to a Segment is the only Reader within the ReaderGroup that reads Events from that Segment. This is the fundamental mechanism by which Pravega makes ordering guarantees of Event delivery to a Reader \u2013 a Reader will receive Events in the order they were published into a Segment. There are several challenges associated with this mechanism: - How to maintain the mapping of which Reader within a ReaderGroup is assigned which Segment - How to manage the above mapping when Segments split and merge - How to manage the above mapping when Readers are added to the ReaderGroup - How to manage the above mapping when Readers leave the ReaderGroup either by an explicit operation or the Reader becoming unavailable due to network outage or the Reader process failing To solve these problems we can use [[StateSynchronizer|StateSynchronizer-design]] to enable readers to coordinate among themselves.","title":"Motivation"},{"location":"reader-group-design/#how-consistent-replicated-state-can-be-used-to-solve-the-problem","text":"A consistent replicated state object representing the ReaderGroup metadata will be created in each reader. This ReaderGroup metadata consists of: * a map of online readers to the segments they own * a list of positions in segments that can be taken over. Every time the Readers in a ReaderGroup change, the state can be updated. Similarly each time one of the readers is going to start reading from a new segment, it can update the replicated state. This allows all readers to know about all the other Readers in their ReaderGroup and which segments they own. Given this information: * A new reader can infer which segments are available to read from. (By virtue of it being absent from the state) * Dealing with a segment being merged becomes easy, because the last reader to reach the end of its pre-merge segment knows it can freely take ownership of the new segment. * Readers can see their relative load and how they are progressing relative to the other readers in their group and can decide to transfer segments if things are out of balance. * This allows readers to take action directly to ensure all the events are read without the need for some external tracker.","title":"How Consistent replicated state can be used to solve the problem"},{"location":"reader-group-design/#readergroup-apis","text":"The external APIs to manage ReaderGroups could be added to the StreamManager object. They would consist of: ReaderGroup createReaderGroup(String name, Stream stream, ReaderGroupConfig config); ReaderGroup getReaderGroup(String name, Stream stream); void deleteReaderGroup(ReaderGroup group); When a ReaderGroup is created, it creates a [[StateSynchronizer|StateSynchronizer-design]] shared by the readers. To join a ReaderGroup readers would just specify it in their configuration: ReaderConfig cc = new ReaderConfig(props); Reader T reader = a_stream.createReader( my_reader_id , my_reader_group , cc); When readers join the group they use the state to determine which segments to read from. When they shut down they update the state so that other readers can take over their segments.","title":"ReaderGroup APIs"},{"location":"reader-group-design/#failure-detector","text":"We still need some sort of heartbeating mechanism to tell if Readers are alive. The problem is greatly simplified because it need not produce a view of the cluster or manage any state. The component would just need to detect failures and invoke the void readerOffline(String readerId, Position lastPosition); api on the ReaderGroup For consistency, the Failure detector should not declare a host as dead that is still processing events. Doing so could violate exactly once processing guarantees.","title":"Failure detector"},{"location":"reader-group-design/#examples","text":"","title":"Examples"},{"location":"reader-group-design/#new-reader","text":"When a reader joins a group its online status is added to the shared state Other readers receive updates to the shared state. When a reader with more than average number of segments sees the new reader, it may give up a segment by writing its position for that segment to the shared state. The new reader can take over a segment by recording that it is doing so in the shared state. The new reader can start reading from the position it read from the shared state for the segment it picked up. There are no races between multiple readers coming online concurrently because only one of them can successfully claim ownership of any given segment.","title":"New reader"},{"location":"reader-group-design/#segments-merging","text":"When a reader comes to the end of its segment it records this information in the shared state. When all of the segments that are getting merged together are completed a reader may claim ownership of the following segment. There is no ambiguity as to who the owner is, because it is stored in the shared state. There is no risk of a segment being forgotten because any reader can see which segments are available by looking at the shared state and claim them.","title":"Segments merging"},{"location":"reader-group-design/#reader-going-offline","text":"When a reader dies the readerOffline() method will be invoked either by the reader itself in a graceful shutdown (internally to the close method) or via a liveness detector. In either case the reader's last position is passed in. The last position is written to the state. Other readers will see this when they update their local state. Any of them can decide to take over one or more of the segments owned by the old reader from where it left off by recording their intention to do so in the state object. Once the state has been updated the new reader is considered the owner of the segment and can read from it at will.","title":"Reader going offline."},{"location":"reader-group-design/#what-happens-if-a-reader-does-not-keep-up-to-date","text":"A Reader with out-of-date state can read from their existing segments without interference. The only disadvantage to this is that they will not shed load to another reader should one become available. However because they have to write to the shared state to start reading from any segment they don't already own, they must fetch up-to-date information before moving on to a new segment.","title":"What happens if a reader does not keep up to date"},{"location":"reader-group-design/#impact-of-availability-and-latency","text":"Reading and updating the state object can occur in parallel to reading so there would likely be no visible latency impact. A stream would be unavailable for reading if Pravega failed in such a way that the segment containing the ReaderGroup information went down and remained offline for long enough for the readers to exhaust all the events in their existing segments. Of course if Pravega failed in this way, odds are at least some portion of the stream would also be directly impacted and not be able to read any events. This sort of failure mode would manifest as latency for the reader, similar to what would happen if they had reached the tail of the stream. This is preferable to using an external system to manage this coordination, as that would require adding new components that can fail in different ways, as opposed to further relying on a small set that we need to make highly available anyway. This is particularly notable in the case of a network partition. If the network is split any readers that are on the same side of the partition as the Pravega servers can continue working. If we were to utilize an external service, that service could be cut off and readers might not be able to make progress even if they could talk to Pravega.","title":"Impact of availability and latency"},{"location":"reader-group-notifications/","text":"Working with Pravega: ReaderGroup Notifications The ReaderGroup api supports different types of notifications. Currently, we have two types implemented, but we plan to add more over time. The types we currently support are the following: Segment Notification A segment notification is triggered when the total number of segments managed by the reader group changes. During a scale operation segments can be split into multiple or merged into some other segment causing the total number of segments to change. The total number of segments can also change when the configuration of the reader group changes, for example, when it adds or removes a stream. The method for subscribing to segment notifications is shown below @Cleanup ReaderGroupManager groupManager = new ReaderGroupManagerImpl ( SCOPE , controller , clientFactory , connectionFactory ); groupManager . createReaderGroup ( GROUP_NAME , ReaderGroupConfig . builder (). . stream ( Stream . of ( SCOPE , STREAM )) . build ()); groupManager . getReaderGroup ( GROUP_NAME ). getSegmentNotifier ( executor ). registerListener ( segmentNotification - { int numOfReaders = segmentNotification . getNumOfReaders (); int segments = segmentNotification . getNumOfSegments (); if ( numOfReaders segments ) { //Scale up number of readers based on application capacity } else { //More readers available time to shut down some } }); The application can register a listener to be notified of SegmentNotification using the registerListener api. This api takes io.pravega.client.stream.notifications.Listener as a parameter. Here the application can add custom logic to change the set of online readers according to the number of segments. For example, if the number of segments increases, then application might consider increasing the number of online readers. If the number of segments instead decreases according to a segment notification, then the application might want to change the set of online readers accordingly. EndOfData Notification An end of data notifier is triggered when the readers have read all the data of the stream(s) managed by the reader group. This is useful to process the stream data with a batch job where the application wants to read data of sealed stream(s). The method for subscribing to end of data notifications is shown below @Cleanup ReaderGroupManager groupManager = new ReaderGroupManagerImpl ( SCOPE , controller , clientFactory , connectionFactory ); groupManager . createReaderGroup ( GROUP_NAME , ReaderGroupConfig . builder () . stream ( Stream . of ( SCOPE , SEALED_STREAM )) . build ()); groupManager . getReaderGroup ( GROUP_NAME ). getEndOfDataNotifier ( executor ). registerListener ( notification - { //custom action e.g: close all readers }); The application can register a listener to be notified of EndOfDataNotification using the registerListener api. This api takes io.pravega.client.stream.notifications.Listener as a parameter. Here the application can add custom logic that can be invoked once all the data of the sealed streams are read.","title":"Working with Reader Group notifications"},{"location":"reader-group-notifications/#working-with-pravega-readergroup-notifications","text":"The ReaderGroup api supports different types of notifications. Currently, we have two types implemented, but we plan to add more over time. The types we currently support are the following: Segment Notification A segment notification is triggered when the total number of segments managed by the reader group changes. During a scale operation segments can be split into multiple or merged into some other segment causing the total number of segments to change. The total number of segments can also change when the configuration of the reader group changes, for example, when it adds or removes a stream. The method for subscribing to segment notifications is shown below @Cleanup ReaderGroupManager groupManager = new ReaderGroupManagerImpl ( SCOPE , controller , clientFactory , connectionFactory ); groupManager . createReaderGroup ( GROUP_NAME , ReaderGroupConfig . builder (). . stream ( Stream . of ( SCOPE , STREAM )) . build ()); groupManager . getReaderGroup ( GROUP_NAME ). getSegmentNotifier ( executor ). registerListener ( segmentNotification - { int numOfReaders = segmentNotification . getNumOfReaders (); int segments = segmentNotification . getNumOfSegments (); if ( numOfReaders segments ) { //Scale up number of readers based on application capacity } else { //More readers available time to shut down some } }); The application can register a listener to be notified of SegmentNotification using the registerListener api. This api takes io.pravega.client.stream.notifications.Listener as a parameter. Here the application can add custom logic to change the set of online readers according to the number of segments. For example, if the number of segments increases, then application might consider increasing the number of online readers. If the number of segments instead decreases according to a segment notification, then the application might want to change the set of online readers accordingly. EndOfData Notification An end of data notifier is triggered when the readers have read all the data of the stream(s) managed by the reader group. This is useful to process the stream data with a batch job where the application wants to read data of sealed stream(s). The method for subscribing to end of data notifications is shown below @Cleanup ReaderGroupManager groupManager = new ReaderGroupManagerImpl ( SCOPE , controller , clientFactory , connectionFactory ); groupManager . createReaderGroup ( GROUP_NAME , ReaderGroupConfig . builder () . stream ( Stream . of ( SCOPE , SEALED_STREAM )) . build ()); groupManager . getReaderGroup ( GROUP_NAME ). getEndOfDataNotifier ( executor ). registerListener ( notification - { //custom action e.g: close all readers }); The application can register a listener to be notified of EndOfDataNotification using the registerListener api. This api takes io.pravega.client.stream.notifications.Listener as a parameter. Here the application can add custom logic that can be invoked once all the data of the sealed streams are read.","title":"Working with Pravega: ReaderGroup Notifications"},{"location":"roadmap/","text":"Pravega Roadmap Version 0.3 The following will be the primary feature focus areas for our upcoming release. Retention Policy Implementation Retention policies allow an operator to define a specific Stream size or data age. Any data beyond this threshold will automatically be purged. Transactions API The current transactions API is functional, however it is cumbersome and requires detailed knowledge of Pravega to configure appropriate values such as timeouts. This work will simplify the API and automate as many timeouts as possible. Exactly Once Guarantees Focus is on testing and strengthening exactly once guarantees and correctness under failure conditions. Low-level Reader API This will expose a new low-level reader API that provides access the low level byte stream as opposed to the event semantics offered by the Java Client API. This can also be leveraged in the future to build different flavors of reader groups. Security Security for this release will focus on support for securing Pravega external interfaces along with basic access controls on stream access and administration. - Access Control on Stream operations - Auth between Clients and Controller/SegmentStore - Auth between SegmentStore and Tier 2 Storage Pravega Connectors Pravega ecosystem interconnectivity will be augmented with the following: - Expanded Flink connector support (batch table API support) - Logstash connector - Others also under consideration (Interested in writing a connector? Talk to us...) Future Items The following items are new features that we wish to build in upcoming Pravega releases, however no active work is currently underway. Please reach out on the Pravega channels if you're interested in picking one of these up. Operational features Non-disruptive and rolling upgrades for Pravega Provide default Failure Detector Exposing information for administration purposes Ability to define throughput quotas and other QoS guarantees Pravega connectors / integration Kafka API Compatibility (Producer and Consumer APIs) Spark connectors (source/sink) REST Proxy for Reader/Writer (REST proxy for Admin operations is already there) Stream Management Stream aliasing Ability to logically group multiple Streams Ability to assign arbitrary Key-Value pairs to streams - Tagging Tiering Support Policy driven tiering of Streams from Streaming Storage to Long-term storage Support for additional Tier 2 Storage backends","title":"Pravega Roadmap"},{"location":"roadmap/#pravega-roadmap","text":"","title":"Pravega Roadmap"},{"location":"roadmap/#version-03","text":"The following will be the primary feature focus areas for our upcoming release.","title":"Version 0.3"},{"location":"roadmap/#retention-policy-implementation","text":"Retention policies allow an operator to define a specific Stream size or data age. Any data beyond this threshold will automatically be purged.","title":"Retention Policy Implementation"},{"location":"roadmap/#transactions-api","text":"The current transactions API is functional, however it is cumbersome and requires detailed knowledge of Pravega to configure appropriate values such as timeouts. This work will simplify the API and automate as many timeouts as possible.","title":"Transactions API"},{"location":"roadmap/#exactly-once-guarantees","text":"Focus is on testing and strengthening exactly once guarantees and correctness under failure conditions.","title":"Exactly Once Guarantees"},{"location":"roadmap/#low-level-reader-api","text":"This will expose a new low-level reader API that provides access the low level byte stream as opposed to the event semantics offered by the Java Client API. This can also be leveraged in the future to build different flavors of reader groups.","title":"Low-level Reader API"},{"location":"roadmap/#security","text":"Security for this release will focus on support for securing Pravega external interfaces along with basic access controls on stream access and administration. - Access Control on Stream operations - Auth between Clients and Controller/SegmentStore - Auth between SegmentStore and Tier 2 Storage","title":"Security"},{"location":"roadmap/#pravega-connectors","text":"Pravega ecosystem interconnectivity will be augmented with the following: - Expanded Flink connector support (batch table API support) - Logstash connector - Others also under consideration (Interested in writing a connector? Talk to us...)","title":"Pravega Connectors"},{"location":"roadmap/#future-items","text":"The following items are new features that we wish to build in upcoming Pravega releases, however no active work is currently underway. Please reach out on the Pravega channels if you're interested in picking one of these up. Operational features Non-disruptive and rolling upgrades for Pravega Provide default Failure Detector Exposing information for administration purposes Ability to define throughput quotas and other QoS guarantees Pravega connectors / integration Kafka API Compatibility (Producer and Consumer APIs) Spark connectors (source/sink) REST Proxy for Reader/Writer (REST proxy for Admin operations is already there) Stream Management Stream aliasing Ability to logically group multiple Streams Ability to assign arbitrary Key-Value pairs to streams - Tagging Tiering Support Policy driven tiering of Streams from Streaming Storage to Long-term storage Support for additional Tier 2 Storage backends","title":"Future Items"},{"location":"segment-containers/","text":"Segment Containers in a Pravega Cluster This document describes the high level design of how we are managing the lifecyle of Segment Containers in a Pravega Cluster. Segment Containers In this document we refer to a segment container as container . The total number of containers is fixed for a given deployment. Each container can be owned by only one Pravega host and all containers in the cluster should be running at any given point in time. Pravega Host A pravega host is an instance of a pravega service which owns and executes a set of containers. Detecting Active Pravega Hosts Every pravega host on startup will register itself with Zookeeper using ephemeral nodes. The ephemeral nodes are present in zookeeper as long as zookeeper receives appropriate heartbeats from the Pravega host. We rely on these ephemeral nodes to detect which pravega hosts are currently active in the cluster. Monitoring the Pravega Cluster Each Pravega Controller runs a service which monitors the ephemeral nodes on the zookeeper and detects all active pravega hosts in the cluster. On detecting any changes to the cluster membership we verify and re-map all containers to the available set of pravega hosts. This information is persisted in the HostStore atomically. This is stored as a single blob and contains a Map of Host to Set of Containers that a host owns. We use zookeeper to ensure only one pravega controller is monitoring the cluster to avoid multiple simultaneous writers to the HostStore. This will avoid race conditions and allow the state to converge faster. Rebalance Frequency Rebalance of the container ownership happens when either a pravega host is added or removed from the cluster. Since this is a costly operation we need to guard against doing this very often. Currently we ensure that a configured minimum time interval is maintained between any 2 rebalance operations. In the worst case this will proportionally increase the time it takes to perform ownership change in the cluster. Ownership Change Notification Every pravega host has a long running Segment Manager Service. This constantly polls/watches the HostStore for any changes to the container ownership. On detecting any ownership changes for itself (identified by the host key in the Map) the Segment Manager triggers addition and removal of containers accordingly. Ensuring a container is stopped on one host before it is started on another host We currently plan to use conservative timeouts on the Pravega host to ensure a container is not started before it is stopped on another node. This needs further review/discussion. Detecting and handling container start/stop failures Any container start/stop failures is treated as local failures and we expect the Pravega host to try its best to handle these scenarios locally. The Pravega Controller does not make any effort to correct this by running them on different hosts. This needs further review/discussion.","title":"Segment Containers"},{"location":"segment-containers/#segment-containers-in-a-pravega-cluster","text":"This document describes the high level design of how we are managing the lifecyle of Segment Containers in a Pravega Cluster.","title":"Segment Containers in a Pravega Cluster"},{"location":"segment-containers/#segment-containers","text":"In this document we refer to a segment container as container . The total number of containers is fixed for a given deployment. Each container can be owned by only one Pravega host and all containers in the cluster should be running at any given point in time.","title":"Segment Containers"},{"location":"segment-containers/#pravega-host","text":"A pravega host is an instance of a pravega service which owns and executes a set of containers.","title":"Pravega Host"},{"location":"segment-containers/#detecting-active-pravega-hosts","text":"Every pravega host on startup will register itself with Zookeeper using ephemeral nodes. The ephemeral nodes are present in zookeeper as long as zookeeper receives appropriate heartbeats from the Pravega host. We rely on these ephemeral nodes to detect which pravega hosts are currently active in the cluster.","title":"Detecting Active Pravega Hosts"},{"location":"segment-containers/#monitoring-the-pravega-cluster","text":"Each Pravega Controller runs a service which monitors the ephemeral nodes on the zookeeper and detects all active pravega hosts in the cluster. On detecting any changes to the cluster membership we verify and re-map all containers to the available set of pravega hosts. This information is persisted in the HostStore atomically. This is stored as a single blob and contains a Map of Host to Set of Containers that a host owns. We use zookeeper to ensure only one pravega controller is monitoring the cluster to avoid multiple simultaneous writers to the HostStore. This will avoid race conditions and allow the state to converge faster.","title":"Monitoring the Pravega Cluster"},{"location":"segment-containers/#rebalance-frequency","text":"Rebalance of the container ownership happens when either a pravega host is added or removed from the cluster. Since this is a costly operation we need to guard against doing this very often. Currently we ensure that a configured minimum time interval is maintained between any 2 rebalance operations. In the worst case this will proportionally increase the time it takes to perform ownership change in the cluster.","title":"Rebalance Frequency"},{"location":"segment-containers/#ownership-change-notification","text":"Every pravega host has a long running Segment Manager Service. This constantly polls/watches the HostStore for any changes to the container ownership. On detecting any ownership changes for itself (identified by the host key in the Map) the Segment Manager triggers addition and removal of containers accordingly.","title":"Ownership Change Notification"},{"location":"segment-containers/#ensuring-a-container-is-stopped-on-one-host-before-it-is-started-on-another-host","text":"We currently plan to use conservative timeouts on the Pravega host to ensure a container is not started before it is stopped on another node. This needs further review/discussion.","title":"Ensuring a container is stopped on one host before it is started on another host"},{"location":"segment-containers/#detecting-and-handling-container-startstop-failures","text":"Any container start/stop failures is treated as local failures and we expect the Pravega host to try its best to handle these scenarios locally. The Pravega Controller does not make any effort to correct this by running them on different hosts. This needs further review/discussion.","title":"Detecting and handling container start/stop failures"},{"location":"segment-store-service/","text":"Pravega Segment Store Service The Pravega Segment Store Service is a subsystem that lies at the heart of the entire Pravega deployment. It is the main access point for managing Stream Segments, providing the ability to create, delete and modify/access their contents. The Pravega Client communicates with the Pravega Stream Controller to figure out which Segments need to be used (for a Stream), and both the Stream Controller and the Client deal with the Segment Store Service to actually operate on them. The basic idea behind the Segment Store Service is that it buffers incoming data in a very fast and durable append-only medium (Tier 1), and syncs it to a high-throughput (but not necessarily low latency) system (Tier 2) in the background, while aggregating multiple (smaller) operations to a Segment into a fewer (but larger) ones. The Pravega Segment Store Service can provide the following guarantees: Stream Segments that are unlimited in length, with append-only semantics, yet supporting arbitrary-offset reads. No throughput degradation when performing small appends, regardless of the performance of the underlying Tier-2 storage system. Multiple concurrent writers to the same Segment. Order is guaranteed within the context of a single writer, but appends from multiple concurrent writers will be added in the order in which they were received (appends are atomic without interleaving their contents). Writing to and reading from a Segment concurrently with relatively low latency between writing and reading. Terminology Throughout the rest of this document, we will use the following terminology: Stream Segment or Segment : A contiguous sequence of bytes. Similar to a file with no size limit. This is a part of a Stream, limited both temporally and laterally (by key). The scope of Streams and mapping Stream Segments to such Streams is beyond the purpose of this document. Tier-2 storage or Permanent Storage : The final resting place of the data. Tier-1 storage : Fast append storage, used for durably buffering incoming appends before distribution to Tier-2 Storage. Cache : A key-value local cache with no expectation of durability. Pravega Segment Store Service or Segment Store : The Service that this document describes. Transaction : A sequence of appends that are related to a Segment, which, if persisted, would make up a contiguous range of bytes within it. This is used for ingesting very large records or for accumulating data that may or may not be persisted into the Segment (but its fate cannot be determined until later in the future). Note that at the Pravega level, a Transaction applies to an entire Stream. In this document, a Transaction applies to a single Segment. Architecture The Segment Store is made up of the following components: Pravega Node : a host running a Pravega Process. Stream Segment Container (or Segment Container ): A logical grouping of Stream Segments. The mapping of Segments to Containers is deterministic and does not require any persistent store; Segments are mapped to Containers via a hash function (based on the Segment's name). Durable Data Log Adapter (or DurableDataLog ): an abstraction layer for Tier-1 Storage. Storage Adapter : an abstraction layer for Tier-2 Storage. Cache : an abstraction layer for append data caching. Streaming Client : an API that can be used to communicate with the Pravega Segment Store. Segment Container Manager : a component that can be used to determine the lifecycle of Segment Containers on a Pravega Node. This is used to start or stop Segment Containers based on an external coordination service (such as the Pravega Controller). The Segment Store handles writes by first writing them to a log (Durable Data Log) on a fast storage (SSDs preferably) and immediately acking back to the client after they have been persisted there. Subsequently, those writes are then aggregated into larger chunks and written in the background to Tier-2 storage. Data for appends that have been acknowledged (and are in Tier-1) but not yet in Tier-2 is stored in the Cache (in addition to Tier-1). Once such data has been written to Tier-2 Storage, it may or may not be kept in the Cache, depending on a number of factors, such as Cache utilization/pressure and access patterns. More details about each component described above can be found in the Components section (below). System Diagram In this image, we show the major components of the Segment Store (for simplicity, only one Segment Container is depicted). All Container components and major links between them (how they interact with each other) are shown. The Container Metadata component is not shown, because every other component communicates with it in one form or another, and adding it would only clutter the diagram. More detailed diagrams can be found under the Data Flow section (below). Components Segment Containers Segment Containers are a logical grouping of Segments, and are responsible for all operations on those Segments within their span. A Segment Container is made of multiple sub-components: Segment Container Metadata : A collection of Segment-specific metadata that describe the current state of each Segment (how much data in Tier-2, how much in Tier-1, whether it is sealed, etc.), as well as other misc info about each Container. Durable Log : The Container writes every operation it receives to this log and acks back only when the log says it has been accepted and durably persisted.. Read Index : An in-memory index of where data can be read from. The Container delegates all read requests to it, and it is responsible for fetching the data from wherever it is currently located (Cache, Tier-1 Storage or Tier-2 Storage). Cache : Used to store data for appends that exist in Tier-1 only (not yet in Tier-2), as well as blocks of data that support reads. Storage Writer : Processes the durable log operations and applies them to Tier-2 storage (in the order in which they were received). This component is also the one that coalesces multiple operations together, for better back-end throughput. Segment Container Metadata The Segment Container Metadata is critical to the good functioning and synchronization of its components. This metadata is shared across all components and it comes at two levels: Container-wide Metadata and per-Segment Metadata. Each serves a different purpose and is described below. Container Metadata Each Segment Container needs to keep some general-purpose metadata that affects all operations inside the container: Operation Sequence Number : The largest sequence number assigned by the Durable Log . Every time a new Operation is received and successfully processed by the Durable Log , this number is incremented (its value never decreases or otherwise roll back, even if an operation failed to be persisted). The Operation Sequence Number is guaranteed to be strict-monotonic increasing (no two Operations have the same value, and an Operation will always have a larger Sequence Number than all Operations before it). Epoch : A number that is incremented every time a successful recovery (Container Start) happens. This value is durably incremented and stored as part of recovery and can be used for a number of cases (a good use is Tier-2 fencing for HDFS, which doesn't provide a good, native mechanism for that). Active Segment Metadata : Information about each active Segment (see next section below). A Segment is active if it has had activity (read or write) recently and is currently loaded in memory. If a Segment is not used for a while, or if there are many Segments currently active, a Segment becomes inactive by having its outstanding metadata flushed to Tier-2 Storage and evicted from memory. Tier-1 Metadata : Various pieces of information that can be used to accurately truncate the Tier-1 Storage Log once all operations prior to that point have been durably stored to Tier-2. Checkpoints : Container Metadata is periodically checkpointed by having its entire snapshot (including Active Segments) serialized to Tier-1. A Checkpoint serves as a Truncation Point for Tier-1, meaning it contains all the updates that have been made to the Container via all the processed operations before it so we no longer need those operations in order to reconstruct the Metadata. If we truncate Tier-1 on a Checkpoint, then we can use information from Tier-2 and this Checkpoint to reconstruct what is in the Metadata previously, without relying on any operation prior to it in Tier-1. Segment Metadata Each Segment Container needs to keep per-segment metadata, which it uses to keep track of the state of each segment as it processes operations for it. The metadata can be volatile (it can be fully rebuilt upon recovery) and contains the following properties for each segment that is currently in use: Name the name of the Segment. Id : Internally assigned unique Segment Id. This is used to refer to Segments, which is preferred to the Name. This Id is sticky for the lifetime of the Segment, which means that even if the Segment becomes inactive, a future reactivation will have it mapped to the same Id. StartOffset (also known as TruncationOffset ): the lowest offset of the data that is available for reading. A non-truncated segment will have Start Offset equal to 0, while subsequent Truncate operations will increase (but never decrease) this number. StorageLength : the highest offset of the data that exists in Tier-2 Storage. Length : the highest offset of the committed data in Tier-1 Storage. LastModified : the timestamp of the last processed (and acknowledged) append. IsSealed : whether the Segment is closed for appends (this value may not have been applied to Tier-2 Storage yet). IsSealedInStorage : whether the Segment is closed for appends (and this has been persisted in Tier-2 Storage). IsMerged : whether this Segment has been merged into another one (but this has not yet been persisted in Tier-2 Storage). This only applies for Transactions. Once the merger is persisted into Tier-2, the Transaction Segment does not exist anymore (so IsDeleted will become true). IsDeleted : whether the Segment is deleted or has recently been merged into another Segment. This only applies for recently deleted Segments, and not for Segments that never existed. The following are always true for any Segment: StorageLength = Length StartOffset = Length Log Operations A Log Operation is the basic unit that is enqueued in the Durable Log. It does not represent an action, per se, but is the base for several serializable operations (we can serialize multiple types of operations, not just Appends). Each Operation is the result of an external action (which denote the alteration of a Segment), or an internal trigger, such as Metadata maintenance operations. Every Log Operation has the following elements: - SequenceNumber : the unique sequence number assigned to this entry (see more under Container Metadata ). These are the various types of Log Operations: Storage Operations represent operations that need to be applied to the underlying Tier-2 Storage: StreamSegmentAppendOperation : Represents an Append to a particular Segment. CachedStreamSegmentAppendOperation : Same as StreamSegmentAppendOperation , but this is for internal use (instead of having an actual data payload, it points to a location in the cache from where the data can be retrieved). StreamSegmentSealOperation : When processed, it sets a flag in the in-memory metadata that no more appends can be received. When the Storage Writer processes it, it marks the Segment as read-only in Tier-2 Storage. StreamSegmentTruncateOperation : Truncates a Segment at a particular offset. This causes the Segment's StartOffset to change. MergeTransactionOperation : Indicates that a Transaction is to be merged into its parent Segment. Metadata Operations are auxiliary operations that indicate a change to the Container Metadata. They can be the result of an external operation (we received an request for a Segment we never knew about before, so we must assign a unique Id to it) or to snapshot the entire Metadata (which helps with recovery and cleaning up Tier-1 Storage). The purpose of the Metadata Operations is to reduce the amount of time needed for failover recovery (when needed): StreamSegmentMapOperation : maps an Id to a Segment Name. TransactionMapOperation : maps an Id to a Transaction and to its Parent Segment. UpdateAttributesOperation : Updates any attributes on a Segment. MetadataCheckpoint includes an entire snapshot of the Metadata. This can be useful when during recovery - this contains all metadata up to this point, which is a sufficient base for all operations after it. Durable Log The Durable Log is the central component that handles all Log Operations. All Operations (which are created by the Container) are added to the Durable Log, which processes them in the order in which they were received. It is made up of a few other components, all of which are working towards a single goal of processing all incoming operations as quickly as possible, without compromising data integrity. Information Flow in the Durable Log All received operations are added to an Operation Queue (the caller receives a Future which will be completed when the operation is durably persisted) The Operation Processor picks all operations currently available in the queue (if the queue is empty, it will wait until at least one Operation is added). The Operation Processor runs as a continuous loop (in a background thread), and has four main stages. Dequeue all outstanding Operations from the Operation Queue (described above) Pre-process the Operations (validate that they are correct and would not cause undesired behavior, assign offsets (where needed), assign Sequence Numbers, etc.) Write the operations to a Data Frame Builder , which serializes and packs the operations in Data Frames . Once a Data Frame is complete (full or no more operations to add), the Data Frame Builder sends the Data Frame to the _Durable Data Log. Note that an Operation may span multiple DataFrames - the goal is to make best use of the Durable Data Log throughput capacity by making writes as large as possible (but also taking into account that there may be a maximum size limit per write). When a DataFrame has been durably persisted in the Durable Data Log, the Operation Processor post-processes all operations that were fully written so far (adds them to in-memory structures, updates indices, etc.) and completes the Futures associated with them. The Operation Processor works asynchronously, in that it does not wait for a particular Data Frame to be written before starting another one and sending it to the Durable Data Log. As such, multiple DataFrames may be in flight (but in a specific order), and the Operation Processor relies on certain ordering guarantees from the Durable Data Log (if a particular DataFrame was acked, then all DataFrames prior to that were also committed successfully, in the right order). The Operation Processor does not do any write throttling (it leaves that to the Durable Data Log implementation), but it control the size of the Data Frames that get sent to it. Truncation Based on supplied configuration, the Durable Log auto-adds a special kind of operation, named MetadataCheckpointOperation . This operation, when processed by the Operation Processor, collects a snapshot of the entire Container Metadata and serializes it to the Durable Data Log. This special Operation marks a Truncation Point - a place in the stream of Log Operations where we can issue Truncate operations. It is very important that after every truncation, the first operation to be found in the log is a MetadataCheckpointOperation , because without the prior operations to reconstruct metadata, this is the only way to be able to process subsequent operations. Note: Durable Data Log (Tier-1) truncation should not be confused with Segment Truncation. They serve different purposes and are applied to different targets. Operation Processor The Operation Processor is a sub-component of the Durable Log that deals with incoming Log Operations. Its purpose is to validate, persist, and update Metadata and other internal structures based on the contents of each operation. Operation Metadata Updater The Operation Metadata Updater is a sub-component of the Durable Log that is responsible with validating operations based on the current state of the metadata, as well as update the Metadata after a successful committal of an operation. Internally it has various mechanisms to cope with failures, and it can rollback certain changes in failure situations. Durable Data Log The _Durable Data Log is an abstraction layer to an external component that provides append-only semantics. It is supposed to be a system which provides very fast appends to a log, that guarantees the durability and consistency of the written data. The read performance is not so much a factor, because we do not read directly from this component - we only read from it when we need to recover the contents of the Durable Log. As explained above, Log Operations are serialized into Data Frames (with a single Operation able to span multiple such Frames if needed), and these Data Frames are then serialized as entries into this Durable Data Log. This is used only as a fail-safe, and we only need to read these Frames back if we need to perform recovery (in which case we need to deserialize all Log Operations contained in them, in the same order in which they were received). In-Memory Operation Log The In-Memory Operation Log contains committed (and replicated) Log Operations in the exact same order as they were added to the Durable Data Log. While the Durable Data Log contains a sequence of Data Frames (which contain serializations of Operations), the Memory Log contains the actual Operations, which can be used throughout the Durable Log (and the Storage Writer). The Memory Log is essentially a chain of Log Operations ordered by the time when the Operation was received. We always add at one end, and we remove from the other. When we truncate the Durable Data Log the Memory Log is also truncated at the same location. Read Index The Read Index helps the Segment Container perform reads from streams at arbitrary offsets. While the Durable Log records (and can only replay) data in the order in which it is received, the Read Index can access the data in a random fashion. The Read Index is made of multiple Segment Read Indices (one per live segment). The Segment Read Index is a data structure that is used to serve reads from memory, as well as pull data from Tier-2 Storage and provide Future Reads (tail reads) when data is not yet available. When a read request is received, the Segment Read Index returns a read iterator that will return data as long as the read request parameters have not yet been satisfied. The iterator will either fetch data that is immediately available in memory, or request data from Tier-2 storage (and bring it to memory) or, if it reached the current end of the Segment, return a Future that will be completed when new data is added (thus providing tailing or future reads). At the heart of the Segment Read Index lies a sorted index of entries (indexed by their start offsets) which is used to locate the requested data when needed. The index itself is implemented by a custom balanced binary search tree (AVL Tree to be more precise) with a goal of minimizing memory usage while not sacrificing insert or access performance. The entries themselves do not contain data, rather some small amount of metadata that is used to locate the data in the Cache and to determine usage patterns (good for cache evictions). Cache The Cache is a component where all data (whether from new appends or that was pulled from Tier-2 storage) is stored. It is a key-value store entirely managed by the Read Index. The Cache is defined as an abstraction layer, and there are two implementations of it: In-memory implementation (via a HashMap ). This is currently only used for unit tests. Memory-disk hybrid, powered by RocksDB . This is the preferred (and default) implementation, since it is not limited by available heap space or machine RAM. Performance is comparable to the in-memory implementation. Storage Writer Pravega is by no means the final resting place of the data, nor is it meant to be a storage service. The Tier-2 Storage is where we want data to be in the long term and Pravega is only used to store a very short tail-end of it (using Tier-1 Storage), enough to make appends fast and aggregate them into bigger chunks for committal to Tier-2 Storage. In order to perform this, it needs another component (Storage Writer) that reads data from the Durable Log in the order in which it was received, aggregates it, and sends it to Tier-2 Storage. Just like the Durable Log, there is one Storage Writer per Segment Container. Each Writer reads Log Operations from the In-Memory Operation Log (exposed via the read() method in the Durable Log) in the order they were processed. It keeps track of the last read item by means of its Sequence Number. This state is not persisted, and upon recovery, it can just start from the beginning of the available Durable Log. The Storage Writer can process any Storage Operation (Append, Seal, Merge), and since Pravega is the sole actor modifying such data in Tier-2, it applies them without restraint. It has several mechanisms to detect and recover from possible data loss or external actors modifying data concurrently. Integration with Controller Actual methods for how Segment Containers are mapped to hosts and what rules are used for moving from from one to another are beyond the scope of this document. Here, we just describe how the Segment Store Service interacts with the Controller and how it manages the lifecycle of Segment Containers based on external events. Segment Container Manager Each instance of a Segment Store Service needs a Segment Container Manager . The role of this component is to manage the lifecycle of the Segment Containers that are assigned to that node (service). It performs the following duties: Connects to the Controller Service-Side Client (i.e., a client that deals only with Segment Container events, and not with the management of Streams and listens to all changes that pertain to Containers that pertain to its own instance. When it receives a notification that it needs to start a Segment Container for a particular Container Id, it initiates the process of bootstrapping such an object. It does not notify the requesting client of success until the operation completes without error. When it receives a notification that it needs to stop a Segment Container for a particular Container Id, it initiates the process of shutting it down. It does not notify the requesting client of success until the operation completes without error. If the Segment Container shuts down unexpectedly (whether during Start, or during its normal operation), it will not attempt to restart it locally; instead it will notify the Controller of the fact. Storage Abstractions The Segment Store was not designed with particular implementations for Tier-1 or Tier-2 in mind. Instead, all these components have been abstracted out in simple, well-defined interfaces, which can be implemented against any standard file system (Tier-2) or append-only log system (Tier-1). Possible candidates for Tier-1 storage: Apache BookKeeper (preferred, adapter is fully implemented as part of Pravega) Non-durable, non-replicated solutions: In-Memory (Single node deployment only - Pravega becomes a volatile buffer for Tier-2 storage; data loss is unavoidable and unrecoverable from in the case of process crash or system restart). This is used for unit test only. Local File System (Single node deployment only - Pravega becomes a semi-durable buffer for Tier-2 storage; data loss is unavoidable and unrecoverable from in the case of complete node failure) Possible candidates for Tier-2 storage: HDFS (implementation available) Extended S3 (implementation available) NFS (general FileSystem ) (implementation available) In-Memory (Single node deployment only - limited usefulness; data loss is unavoidable and unrecoverable from in the case of process crash or system restart) This is used for unit test only. A note about Tier-2 Truncation : The Segment Store supports Segment truncation at a particular offset, which means that, once that request is complete, no offset below that one will be available for reading. This above is only a metadata update operation, however this also needs to be supported by Tier-2 so that the truncated data is physically deleted from it. If a Tier-2 implementation does not natively support truncation from the beginning of a file with offset preservation (i.e., a Segment of length 100 is truncated at offset 50, then offsets 0..49 are deleted, but offsets 50-99 are available and are not shifted down), then the Segment Store provides a wrapper on top of a generic Tier-2 implementation that can do that The RollingStorage Tier-2 wrapper splits a Segment into multiple Segment Chunks and exposes them as a single Segment to the upper layers. Segment Chunks that have been truncated out are deleted automatically. This is not a very precise application (since it relies heavily on a rollover policy dictating granularity), but it is a practical solution for those cases when the real Tier-2 implementation does not provide the features that we need. Data Flow Here are a few examples on how data flows inside the Pravega Segment Store Service. Appends The diagram above depicts these steps (note the step numbers may not match, but the order is the same): Segment Store receives append request with params: Segment Name, Payload and AttributeUpdates. Segment Store determines the ContainerId for the given Segment and verifies that the Segment Container is registered locally. If not, it returns an appropriate error code. Segment Store delegates request to the appropriate Segment Container instance. Segment Container verifies that the Segment belongs to the Segment Container and that the Segment actually exists. If not, it returns an appropriate error code. During this process, it also get an existing Segment Id or assigns a new one (by using the Segment Mapper component). Segment Container creates a StreamSegmentAppendOperation with the input data and sends it to the Durable Log . Durable Log takes the Append Operation and processes it according to the algorithm described in the Durable Log section. Puts it in its Operation Queue. Operation Processor pulls all operations off the Queue. Operation Processor uses the Data Frame Builder to construct Data Frames with the operations it has. Data Frame Builder asynchronously writes the Data Frame to the Durable Data Log . Upon completion, the following are done in parallel: Metadata is updated. The Operation is added to the Memory Operation Log and Read Index . An call that triggered the Operation is ack-ed. The above process is asynchronous, which means the Operation Processor will have multiple Data Frames in flight (not illustrated). It will keep track of each one's changes and apply or roll them back as needed. This process applies for every single operation that the Segment Store supports. All modify operations go through the Operation Processor and have a similar path. Reads The diagram above depicts these steps (note the step numbers may not match, but the order is the same): Segment Store receives read request with params: Segment Name, Read Offset, Max-Length. Segment Store determines the ContainerId for the given Segment and verifies if it is Leader for given Segment Container . If not, it returns an appropriate error code. Segment Store delegates request to the Segment Container instance. Segment Container verifies that the Segment belongs to that Container and that it actually exists. If not, it returns an appropriate error code to the client. During this process, it also get an existing Segment Id or assigns a new one (by using the Segment Mapper component). Segment Container delegates the request to its Read Index , which processes the read as described in the Read Index section, by issuing Reads from Storage (for data that is not in the Cache ), and querying/updating the Cache as needed. Synchronization with Tier-2 (Storage Writer) The diagram above depicts these steps (note the step numbers may not match, but the order is the same): The Storage Writer 's main loop is the sub-component that triggers all these operations Read next Operation from the Durable Log (in between each loop, the Writer remembers what the Sequence Number of the last processed Operation was) All operations are processed, and added to the internal Segment Aggregators (one Aggregator per Segment) Eligible Segment Aggregators are flushed to Storage (eligibility depends on the amount of data collected in each aggregator, and whether there are any Seal, Merge or Truncate operations queued up) Each time an Append operation is encountered, a trip to the Read Index may be required in order to get the contents of the append After every successful modification (write/seal/concat/truncate) to Storage , the Container Metadata is updated to reflect the changes. The Durable Log is truncated (if eligible). Container Startup (Normal/Recovery) The diagram above depicts these steps (note the step numbers may not match, but the order is the same): The Container Manager receives a request to start a Container in this instance of the Segment Store Service . It creates, registers, and starts the Container. The Container starts the Durable Log component. Durable Log initiates the recovery process (coordinated by the Recovery Executor ). Recovery Executor reads all Data Frames from Durable Data Log . Deserialized Operations from the read Data Frames are added to the Memory Operation Log . The Container Metadata is updated by means of the Operation Metadata Updater (same as the one used inside Operation Processor). The Read Index is populated with the contents of those Operations that apply to it. The Container Starts the Storage Writer . The Storage Writer 's Main Loop starts processing operations from the Durable Log , and upon first encountering a new Segment, it reconciles its content (and metadata) with the reality that exists in Storage . After both the Durable Log and the Storage Writer have started, the Container is ready to start accepting new external requests.","title":"Segment Store Service"},{"location":"segment-store-service/#pravega-segment-store-service","text":"The Pravega Segment Store Service is a subsystem that lies at the heart of the entire Pravega deployment. It is the main access point for managing Stream Segments, providing the ability to create, delete and modify/access their contents. The Pravega Client communicates with the Pravega Stream Controller to figure out which Segments need to be used (for a Stream), and both the Stream Controller and the Client deal with the Segment Store Service to actually operate on them. The basic idea behind the Segment Store Service is that it buffers incoming data in a very fast and durable append-only medium (Tier 1), and syncs it to a high-throughput (but not necessarily low latency) system (Tier 2) in the background, while aggregating multiple (smaller) operations to a Segment into a fewer (but larger) ones. The Pravega Segment Store Service can provide the following guarantees: Stream Segments that are unlimited in length, with append-only semantics, yet supporting arbitrary-offset reads. No throughput degradation when performing small appends, regardless of the performance of the underlying Tier-2 storage system. Multiple concurrent writers to the same Segment. Order is guaranteed within the context of a single writer, but appends from multiple concurrent writers will be added in the order in which they were received (appends are atomic without interleaving their contents). Writing to and reading from a Segment concurrently with relatively low latency between writing and reading.","title":"Pravega Segment Store Service"},{"location":"segment-store-service/#terminology","text":"Throughout the rest of this document, we will use the following terminology: Stream Segment or Segment : A contiguous sequence of bytes. Similar to a file with no size limit. This is a part of a Stream, limited both temporally and laterally (by key). The scope of Streams and mapping Stream Segments to such Streams is beyond the purpose of this document. Tier-2 storage or Permanent Storage : The final resting place of the data. Tier-1 storage : Fast append storage, used for durably buffering incoming appends before distribution to Tier-2 Storage. Cache : A key-value local cache with no expectation of durability. Pravega Segment Store Service or Segment Store : The Service that this document describes. Transaction : A sequence of appends that are related to a Segment, which, if persisted, would make up a contiguous range of bytes within it. This is used for ingesting very large records or for accumulating data that may or may not be persisted into the Segment (but its fate cannot be determined until later in the future). Note that at the Pravega level, a Transaction applies to an entire Stream. In this document, a Transaction applies to a single Segment.","title":"Terminology"},{"location":"segment-store-service/#architecture","text":"The Segment Store is made up of the following components: Pravega Node : a host running a Pravega Process. Stream Segment Container (or Segment Container ): A logical grouping of Stream Segments. The mapping of Segments to Containers is deterministic and does not require any persistent store; Segments are mapped to Containers via a hash function (based on the Segment's name). Durable Data Log Adapter (or DurableDataLog ): an abstraction layer for Tier-1 Storage. Storage Adapter : an abstraction layer for Tier-2 Storage. Cache : an abstraction layer for append data caching. Streaming Client : an API that can be used to communicate with the Pravega Segment Store. Segment Container Manager : a component that can be used to determine the lifecycle of Segment Containers on a Pravega Node. This is used to start or stop Segment Containers based on an external coordination service (such as the Pravega Controller). The Segment Store handles writes by first writing them to a log (Durable Data Log) on a fast storage (SSDs preferably) and immediately acking back to the client after they have been persisted there. Subsequently, those writes are then aggregated into larger chunks and written in the background to Tier-2 storage. Data for appends that have been acknowledged (and are in Tier-1) but not yet in Tier-2 is stored in the Cache (in addition to Tier-1). Once such data has been written to Tier-2 Storage, it may or may not be kept in the Cache, depending on a number of factors, such as Cache utilization/pressure and access patterns. More details about each component described above can be found in the Components section (below).","title":"Architecture"},{"location":"segment-store-service/#system-diagram","text":"In this image, we show the major components of the Segment Store (for simplicity, only one Segment Container is depicted). All Container components and major links between them (how they interact with each other) are shown. The Container Metadata component is not shown, because every other component communicates with it in one form or another, and adding it would only clutter the diagram. More detailed diagrams can be found under the Data Flow section (below).","title":"System Diagram"},{"location":"segment-store-service/#components","text":"","title":"Components"},{"location":"segment-store-service/#segment-containers","text":"Segment Containers are a logical grouping of Segments, and are responsible for all operations on those Segments within their span. A Segment Container is made of multiple sub-components: Segment Container Metadata : A collection of Segment-specific metadata that describe the current state of each Segment (how much data in Tier-2, how much in Tier-1, whether it is sealed, etc.), as well as other misc info about each Container. Durable Log : The Container writes every operation it receives to this log and acks back only when the log says it has been accepted and durably persisted.. Read Index : An in-memory index of where data can be read from. The Container delegates all read requests to it, and it is responsible for fetching the data from wherever it is currently located (Cache, Tier-1 Storage or Tier-2 Storage). Cache : Used to store data for appends that exist in Tier-1 only (not yet in Tier-2), as well as blocks of data that support reads. Storage Writer : Processes the durable log operations and applies them to Tier-2 storage (in the order in which they were received). This component is also the one that coalesces multiple operations together, for better back-end throughput.","title":"Segment Containers"},{"location":"segment-store-service/#segment-container-metadata","text":"The Segment Container Metadata is critical to the good functioning and synchronization of its components. This metadata is shared across all components and it comes at two levels: Container-wide Metadata and per-Segment Metadata. Each serves a different purpose and is described below.","title":"Segment Container Metadata"},{"location":"segment-store-service/#container-metadata","text":"Each Segment Container needs to keep some general-purpose metadata that affects all operations inside the container: Operation Sequence Number : The largest sequence number assigned by the Durable Log . Every time a new Operation is received and successfully processed by the Durable Log , this number is incremented (its value never decreases or otherwise roll back, even if an operation failed to be persisted). The Operation Sequence Number is guaranteed to be strict-monotonic increasing (no two Operations have the same value, and an Operation will always have a larger Sequence Number than all Operations before it). Epoch : A number that is incremented every time a successful recovery (Container Start) happens. This value is durably incremented and stored as part of recovery and can be used for a number of cases (a good use is Tier-2 fencing for HDFS, which doesn't provide a good, native mechanism for that). Active Segment Metadata : Information about each active Segment (see next section below). A Segment is active if it has had activity (read or write) recently and is currently loaded in memory. If a Segment is not used for a while, or if there are many Segments currently active, a Segment becomes inactive by having its outstanding metadata flushed to Tier-2 Storage and evicted from memory. Tier-1 Metadata : Various pieces of information that can be used to accurately truncate the Tier-1 Storage Log once all operations prior to that point have been durably stored to Tier-2. Checkpoints : Container Metadata is periodically checkpointed by having its entire snapshot (including Active Segments) serialized to Tier-1. A Checkpoint serves as a Truncation Point for Tier-1, meaning it contains all the updates that have been made to the Container via all the processed operations before it so we no longer need those operations in order to reconstruct the Metadata. If we truncate Tier-1 on a Checkpoint, then we can use information from Tier-2 and this Checkpoint to reconstruct what is in the Metadata previously, without relying on any operation prior to it in Tier-1.","title":"Container Metadata"},{"location":"segment-store-service/#segment-metadata","text":"Each Segment Container needs to keep per-segment metadata, which it uses to keep track of the state of each segment as it processes operations for it. The metadata can be volatile (it can be fully rebuilt upon recovery) and contains the following properties for each segment that is currently in use: Name the name of the Segment. Id : Internally assigned unique Segment Id. This is used to refer to Segments, which is preferred to the Name. This Id is sticky for the lifetime of the Segment, which means that even if the Segment becomes inactive, a future reactivation will have it mapped to the same Id. StartOffset (also known as TruncationOffset ): the lowest offset of the data that is available for reading. A non-truncated segment will have Start Offset equal to 0, while subsequent Truncate operations will increase (but never decrease) this number. StorageLength : the highest offset of the data that exists in Tier-2 Storage. Length : the highest offset of the committed data in Tier-1 Storage. LastModified : the timestamp of the last processed (and acknowledged) append. IsSealed : whether the Segment is closed for appends (this value may not have been applied to Tier-2 Storage yet). IsSealedInStorage : whether the Segment is closed for appends (and this has been persisted in Tier-2 Storage). IsMerged : whether this Segment has been merged into another one (but this has not yet been persisted in Tier-2 Storage). This only applies for Transactions. Once the merger is persisted into Tier-2, the Transaction Segment does not exist anymore (so IsDeleted will become true). IsDeleted : whether the Segment is deleted or has recently been merged into another Segment. This only applies for recently deleted Segments, and not for Segments that never existed. The following are always true for any Segment: StorageLength = Length StartOffset = Length","title":"Segment Metadata"},{"location":"segment-store-service/#log-operations","text":"A Log Operation is the basic unit that is enqueued in the Durable Log. It does not represent an action, per se, but is the base for several serializable operations (we can serialize multiple types of operations, not just Appends). Each Operation is the result of an external action (which denote the alteration of a Segment), or an internal trigger, such as Metadata maintenance operations. Every Log Operation has the following elements: - SequenceNumber : the unique sequence number assigned to this entry (see more under Container Metadata ). These are the various types of Log Operations: Storage Operations represent operations that need to be applied to the underlying Tier-2 Storage: StreamSegmentAppendOperation : Represents an Append to a particular Segment. CachedStreamSegmentAppendOperation : Same as StreamSegmentAppendOperation , but this is for internal use (instead of having an actual data payload, it points to a location in the cache from where the data can be retrieved). StreamSegmentSealOperation : When processed, it sets a flag in the in-memory metadata that no more appends can be received. When the Storage Writer processes it, it marks the Segment as read-only in Tier-2 Storage. StreamSegmentTruncateOperation : Truncates a Segment at a particular offset. This causes the Segment's StartOffset to change. MergeTransactionOperation : Indicates that a Transaction is to be merged into its parent Segment. Metadata Operations are auxiliary operations that indicate a change to the Container Metadata. They can be the result of an external operation (we received an request for a Segment we never knew about before, so we must assign a unique Id to it) or to snapshot the entire Metadata (which helps with recovery and cleaning up Tier-1 Storage). The purpose of the Metadata Operations is to reduce the amount of time needed for failover recovery (when needed): StreamSegmentMapOperation : maps an Id to a Segment Name. TransactionMapOperation : maps an Id to a Transaction and to its Parent Segment. UpdateAttributesOperation : Updates any attributes on a Segment. MetadataCheckpoint includes an entire snapshot of the Metadata. This can be useful when during recovery - this contains all metadata up to this point, which is a sufficient base for all operations after it.","title":"Log Operations"},{"location":"segment-store-service/#durable-log","text":"The Durable Log is the central component that handles all Log Operations. All Operations (which are created by the Container) are added to the Durable Log, which processes them in the order in which they were received. It is made up of a few other components, all of which are working towards a single goal of processing all incoming operations as quickly as possible, without compromising data integrity.","title":"Durable Log"},{"location":"segment-store-service/#information-flow-in-the-durable-log","text":"All received operations are added to an Operation Queue (the caller receives a Future which will be completed when the operation is durably persisted) The Operation Processor picks all operations currently available in the queue (if the queue is empty, it will wait until at least one Operation is added). The Operation Processor runs as a continuous loop (in a background thread), and has four main stages. Dequeue all outstanding Operations from the Operation Queue (described above) Pre-process the Operations (validate that they are correct and would not cause undesired behavior, assign offsets (where needed), assign Sequence Numbers, etc.) Write the operations to a Data Frame Builder , which serializes and packs the operations in Data Frames . Once a Data Frame is complete (full or no more operations to add), the Data Frame Builder sends the Data Frame to the _Durable Data Log. Note that an Operation may span multiple DataFrames - the goal is to make best use of the Durable Data Log throughput capacity by making writes as large as possible (but also taking into account that there may be a maximum size limit per write). When a DataFrame has been durably persisted in the Durable Data Log, the Operation Processor post-processes all operations that were fully written so far (adds them to in-memory structures, updates indices, etc.) and completes the Futures associated with them. The Operation Processor works asynchronously, in that it does not wait for a particular Data Frame to be written before starting another one and sending it to the Durable Data Log. As such, multiple DataFrames may be in flight (but in a specific order), and the Operation Processor relies on certain ordering guarantees from the Durable Data Log (if a particular DataFrame was acked, then all DataFrames prior to that were also committed successfully, in the right order). The Operation Processor does not do any write throttling (it leaves that to the Durable Data Log implementation), but it control the size of the Data Frames that get sent to it.","title":"Information Flow in the Durable Log"},{"location":"segment-store-service/#truncation","text":"Based on supplied configuration, the Durable Log auto-adds a special kind of operation, named MetadataCheckpointOperation . This operation, when processed by the Operation Processor, collects a snapshot of the entire Container Metadata and serializes it to the Durable Data Log. This special Operation marks a Truncation Point - a place in the stream of Log Operations where we can issue Truncate operations. It is very important that after every truncation, the first operation to be found in the log is a MetadataCheckpointOperation , because without the prior operations to reconstruct metadata, this is the only way to be able to process subsequent operations. Note: Durable Data Log (Tier-1) truncation should not be confused with Segment Truncation. They serve different purposes and are applied to different targets.","title":"Truncation"},{"location":"segment-store-service/#operation-processor","text":"The Operation Processor is a sub-component of the Durable Log that deals with incoming Log Operations. Its purpose is to validate, persist, and update Metadata and other internal structures based on the contents of each operation.","title":"Operation Processor"},{"location":"segment-store-service/#operation-metadata-updater","text":"The Operation Metadata Updater is a sub-component of the Durable Log that is responsible with validating operations based on the current state of the metadata, as well as update the Metadata after a successful committal of an operation. Internally it has various mechanisms to cope with failures, and it can rollback certain changes in failure situations.","title":"Operation Metadata Updater"},{"location":"segment-store-service/#durable-data-log","text":"The _Durable Data Log is an abstraction layer to an external component that provides append-only semantics. It is supposed to be a system which provides very fast appends to a log, that guarantees the durability and consistency of the written data. The read performance is not so much a factor, because we do not read directly from this component - we only read from it when we need to recover the contents of the Durable Log. As explained above, Log Operations are serialized into Data Frames (with a single Operation able to span multiple such Frames if needed), and these Data Frames are then serialized as entries into this Durable Data Log. This is used only as a fail-safe, and we only need to read these Frames back if we need to perform recovery (in which case we need to deserialize all Log Operations contained in them, in the same order in which they were received).","title":"Durable Data Log"},{"location":"segment-store-service/#in-memory-operation-log","text":"The In-Memory Operation Log contains committed (and replicated) Log Operations in the exact same order as they were added to the Durable Data Log. While the Durable Data Log contains a sequence of Data Frames (which contain serializations of Operations), the Memory Log contains the actual Operations, which can be used throughout the Durable Log (and the Storage Writer). The Memory Log is essentially a chain of Log Operations ordered by the time when the Operation was received. We always add at one end, and we remove from the other. When we truncate the Durable Data Log the Memory Log is also truncated at the same location.","title":"In-Memory Operation Log"},{"location":"segment-store-service/#read-index","text":"The Read Index helps the Segment Container perform reads from streams at arbitrary offsets. While the Durable Log records (and can only replay) data in the order in which it is received, the Read Index can access the data in a random fashion. The Read Index is made of multiple Segment Read Indices (one per live segment). The Segment Read Index is a data structure that is used to serve reads from memory, as well as pull data from Tier-2 Storage and provide Future Reads (tail reads) when data is not yet available. When a read request is received, the Segment Read Index returns a read iterator that will return data as long as the read request parameters have not yet been satisfied. The iterator will either fetch data that is immediately available in memory, or request data from Tier-2 storage (and bring it to memory) or, if it reached the current end of the Segment, return a Future that will be completed when new data is added (thus providing tailing or future reads). At the heart of the Segment Read Index lies a sorted index of entries (indexed by their start offsets) which is used to locate the requested data when needed. The index itself is implemented by a custom balanced binary search tree (AVL Tree to be more precise) with a goal of minimizing memory usage while not sacrificing insert or access performance. The entries themselves do not contain data, rather some small amount of metadata that is used to locate the data in the Cache and to determine usage patterns (good for cache evictions).","title":"Read Index"},{"location":"segment-store-service/#cache","text":"The Cache is a component where all data (whether from new appends or that was pulled from Tier-2 storage) is stored. It is a key-value store entirely managed by the Read Index. The Cache is defined as an abstraction layer, and there are two implementations of it: In-memory implementation (via a HashMap ). This is currently only used for unit tests. Memory-disk hybrid, powered by RocksDB . This is the preferred (and default) implementation, since it is not limited by available heap space or machine RAM. Performance is comparable to the in-memory implementation.","title":"Cache"},{"location":"segment-store-service/#storage-writer","text":"Pravega is by no means the final resting place of the data, nor is it meant to be a storage service. The Tier-2 Storage is where we want data to be in the long term and Pravega is only used to store a very short tail-end of it (using Tier-1 Storage), enough to make appends fast and aggregate them into bigger chunks for committal to Tier-2 Storage. In order to perform this, it needs another component (Storage Writer) that reads data from the Durable Log in the order in which it was received, aggregates it, and sends it to Tier-2 Storage. Just like the Durable Log, there is one Storage Writer per Segment Container. Each Writer reads Log Operations from the In-Memory Operation Log (exposed via the read() method in the Durable Log) in the order they were processed. It keeps track of the last read item by means of its Sequence Number. This state is not persisted, and upon recovery, it can just start from the beginning of the available Durable Log. The Storage Writer can process any Storage Operation (Append, Seal, Merge), and since Pravega is the sole actor modifying such data in Tier-2, it applies them without restraint. It has several mechanisms to detect and recover from possible data loss or external actors modifying data concurrently.","title":"Storage Writer"},{"location":"segment-store-service/#integration-with-controller","text":"Actual methods for how Segment Containers are mapped to hosts and what rules are used for moving from from one to another are beyond the scope of this document. Here, we just describe how the Segment Store Service interacts with the Controller and how it manages the lifecycle of Segment Containers based on external events.","title":"Integration with Controller"},{"location":"segment-store-service/#segment-container-manager","text":"Each instance of a Segment Store Service needs a Segment Container Manager . The role of this component is to manage the lifecycle of the Segment Containers that are assigned to that node (service). It performs the following duties: Connects to the Controller Service-Side Client (i.e., a client that deals only with Segment Container events, and not with the management of Streams and listens to all changes that pertain to Containers that pertain to its own instance. When it receives a notification that it needs to start a Segment Container for a particular Container Id, it initiates the process of bootstrapping such an object. It does not notify the requesting client of success until the operation completes without error. When it receives a notification that it needs to stop a Segment Container for a particular Container Id, it initiates the process of shutting it down. It does not notify the requesting client of success until the operation completes without error. If the Segment Container shuts down unexpectedly (whether during Start, or during its normal operation), it will not attempt to restart it locally; instead it will notify the Controller of the fact.","title":"Segment Container Manager"},{"location":"segment-store-service/#storage-abstractions","text":"The Segment Store was not designed with particular implementations for Tier-1 or Tier-2 in mind. Instead, all these components have been abstracted out in simple, well-defined interfaces, which can be implemented against any standard file system (Tier-2) or append-only log system (Tier-1). Possible candidates for Tier-1 storage: Apache BookKeeper (preferred, adapter is fully implemented as part of Pravega) Non-durable, non-replicated solutions: In-Memory (Single node deployment only - Pravega becomes a volatile buffer for Tier-2 storage; data loss is unavoidable and unrecoverable from in the case of process crash or system restart). This is used for unit test only. Local File System (Single node deployment only - Pravega becomes a semi-durable buffer for Tier-2 storage; data loss is unavoidable and unrecoverable from in the case of complete node failure) Possible candidates for Tier-2 storage: HDFS (implementation available) Extended S3 (implementation available) NFS (general FileSystem ) (implementation available) In-Memory (Single node deployment only - limited usefulness; data loss is unavoidable and unrecoverable from in the case of process crash or system restart) This is used for unit test only. A note about Tier-2 Truncation : The Segment Store supports Segment truncation at a particular offset, which means that, once that request is complete, no offset below that one will be available for reading. This above is only a metadata update operation, however this also needs to be supported by Tier-2 so that the truncated data is physically deleted from it. If a Tier-2 implementation does not natively support truncation from the beginning of a file with offset preservation (i.e., a Segment of length 100 is truncated at offset 50, then offsets 0..49 are deleted, but offsets 50-99 are available and are not shifted down), then the Segment Store provides a wrapper on top of a generic Tier-2 implementation that can do that The RollingStorage Tier-2 wrapper splits a Segment into multiple Segment Chunks and exposes them as a single Segment to the upper layers. Segment Chunks that have been truncated out are deleted automatically. This is not a very precise application (since it relies heavily on a rollover policy dictating granularity), but it is a practical solution for those cases when the real Tier-2 implementation does not provide the features that we need.","title":"Storage Abstractions"},{"location":"segment-store-service/#data-flow","text":"Here are a few examples on how data flows inside the Pravega Segment Store Service.","title":"Data Flow"},{"location":"segment-store-service/#appends","text":"The diagram above depicts these steps (note the step numbers may not match, but the order is the same): Segment Store receives append request with params: Segment Name, Payload and AttributeUpdates. Segment Store determines the ContainerId for the given Segment and verifies that the Segment Container is registered locally. If not, it returns an appropriate error code. Segment Store delegates request to the appropriate Segment Container instance. Segment Container verifies that the Segment belongs to the Segment Container and that the Segment actually exists. If not, it returns an appropriate error code. During this process, it also get an existing Segment Id or assigns a new one (by using the Segment Mapper component). Segment Container creates a StreamSegmentAppendOperation with the input data and sends it to the Durable Log . Durable Log takes the Append Operation and processes it according to the algorithm described in the Durable Log section. Puts it in its Operation Queue. Operation Processor pulls all operations off the Queue. Operation Processor uses the Data Frame Builder to construct Data Frames with the operations it has. Data Frame Builder asynchronously writes the Data Frame to the Durable Data Log . Upon completion, the following are done in parallel: Metadata is updated. The Operation is added to the Memory Operation Log and Read Index . An call that triggered the Operation is ack-ed. The above process is asynchronous, which means the Operation Processor will have multiple Data Frames in flight (not illustrated). It will keep track of each one's changes and apply or roll them back as needed. This process applies for every single operation that the Segment Store supports. All modify operations go through the Operation Processor and have a similar path.","title":"Appends"},{"location":"segment-store-service/#reads","text":"The diagram above depicts these steps (note the step numbers may not match, but the order is the same): Segment Store receives read request with params: Segment Name, Read Offset, Max-Length. Segment Store determines the ContainerId for the given Segment and verifies if it is Leader for given Segment Container . If not, it returns an appropriate error code. Segment Store delegates request to the Segment Container instance. Segment Container verifies that the Segment belongs to that Container and that it actually exists. If not, it returns an appropriate error code to the client. During this process, it also get an existing Segment Id or assigns a new one (by using the Segment Mapper component). Segment Container delegates the request to its Read Index , which processes the read as described in the Read Index section, by issuing Reads from Storage (for data that is not in the Cache ), and querying/updating the Cache as needed.","title":"Reads"},{"location":"segment-store-service/#synchronization-with-tier-2-storage-writer","text":"The diagram above depicts these steps (note the step numbers may not match, but the order is the same): The Storage Writer 's main loop is the sub-component that triggers all these operations Read next Operation from the Durable Log (in between each loop, the Writer remembers what the Sequence Number of the last processed Operation was) All operations are processed, and added to the internal Segment Aggregators (one Aggregator per Segment) Eligible Segment Aggregators are flushed to Storage (eligibility depends on the amount of data collected in each aggregator, and whether there are any Seal, Merge or Truncate operations queued up) Each time an Append operation is encountered, a trip to the Read Index may be required in order to get the contents of the append After every successful modification (write/seal/concat/truncate) to Storage , the Container Metadata is updated to reflect the changes. The Durable Log is truncated (if eligible).","title":"Synchronization with Tier-2 (Storage Writer)"},{"location":"segment-store-service/#container-startup-normalrecovery","text":"The diagram above depicts these steps (note the step numbers may not match, but the order is the same): The Container Manager receives a request to start a Container in this instance of the Segment Store Service . It creates, registers, and starts the Container. The Container starts the Durable Log component. Durable Log initiates the recovery process (coordinated by the Recovery Executor ). Recovery Executor reads all Data Frames from Durable Data Log . Deserialized Operations from the read Data Frames are added to the Memory Operation Log . The Container Metadata is updated by means of the Operation Metadata Updater (same as the one used inside Operation Processor). The Read Index is populated with the contents of those Operations that apply to it. The Container Starts the Storage Writer . The Storage Writer 's Main Loop starts processing operations from the Durable Log , and upon first encountering a new Segment, it reconciles its content (and metadata) with the reality that exists in Storage . After both the Durable Log and the Storage Writer have started, the Container is ready to start accepting new external requests.","title":"Container Startup (Normal/Recovery)"},{"location":"state-synchronizer-design/","text":"StateSynchronizer Design A StateSynchronizer provides a means for data to be written and read by multiple processes, while consistency is guaranteed using optimistic checks. A rough version of this API is checked in here. This works by having each process keep a copy of the data. All updates are written through the StateSynchronizer which appends them to a Pravega segment. They can keep up to date of changes to the data by consuming from the segment. To provide consistency a conditional append is used. This ensures that the updates can only proceed if the process performing them has the most recent data. Finally to prevent the segment from growing without bound, we use have a compact operation that re-writes the latest data, and truncates the old data so it can be dropped. This model works well when most updates are small in comparison to the total data size being stored, as they can be written a small deltas. As with any optimistic concurrency system it would work worst when many processes are all contending to try to update the same information at the same time. Example A concrete example synchroning the contents of a set is checked in here . We also have an example that is synchronizing membership of a set of hosts . Imagine you want many processes to share a Map. This can be done by creating by a StateSynchronizer, which will help coordinate changes to the map. Each client has their own copy of the map in memory and can apply updates by passing a generator to the StateSynchronizer. Every time an update is attempted the updated is recorded to a segment. Updates will fail unless the Map passed into the update method is consistent with all of the updates that have been recorded to the segment. If this occurs the generator is called with the latest state to try again. Thus the order of updates is defined by the order in which they are written to the segment. How this is implemented For this to work we use two features of the Pravega Segment Store Service. Conditional append The append method can specify what offset the append is expected to be at. If the append will not go in, nothing should be done and a failure should be returned to the client. Truncate segment Truncate segment deletes all data before a given offset. (This operation does not affect the existing offsets) Any reads for offsets lower than this value will fail. Any data stored below this offset can be removed.","title":"StateSynchronizer"},{"location":"state-synchronizer-design/#statesynchronizer-design","text":"A StateSynchronizer provides a means for data to be written and read by multiple processes, while consistency is guaranteed using optimistic checks. A rough version of this API is checked in here. This works by having each process keep a copy of the data. All updates are written through the StateSynchronizer which appends them to a Pravega segment. They can keep up to date of changes to the data by consuming from the segment. To provide consistency a conditional append is used. This ensures that the updates can only proceed if the process performing them has the most recent data. Finally to prevent the segment from growing without bound, we use have a compact operation that re-writes the latest data, and truncates the old data so it can be dropped. This model works well when most updates are small in comparison to the total data size being stored, as they can be written a small deltas. As with any optimistic concurrency system it would work worst when many processes are all contending to try to update the same information at the same time.","title":"StateSynchronizer Design"},{"location":"state-synchronizer-design/#example","text":"A concrete example synchroning the contents of a set is checked in here . We also have an example that is synchronizing membership of a set of hosts . Imagine you want many processes to share a Map. This can be done by creating by a StateSynchronizer, which will help coordinate changes to the map. Each client has their own copy of the map in memory and can apply updates by passing a generator to the StateSynchronizer. Every time an update is attempted the updated is recorded to a segment. Updates will fail unless the Map passed into the update method is consistent with all of the updates that have been recorded to the segment. If this occurs the generator is called with the latest state to try again. Thus the order of updates is defined by the order in which they are written to the segment.","title":"Example"},{"location":"state-synchronizer-design/#how-this-is-implemented","text":"For this to work we use two features of the Pravega Segment Store Service.","title":"How this is implemented"},{"location":"state-synchronizer-design/#conditional-append","text":"The append method can specify what offset the append is expected to be at. If the append will not go in, nothing should be done and a failure should be returned to the client.","title":"Conditional append"},{"location":"state-synchronizer-design/#truncate-segment","text":"Truncate segment deletes all data before a given offset. (This operation does not affect the existing offsets) Any reads for offsets lower than this value will fail. Any data stored below this offset can be removed.","title":"Truncate segment"},{"location":"state-synchronizer/","text":"Working with Pravega: State Synchronizer You can think about Pravega as a streaming storage primitive, because it is a great way to durably persist data. You can think about Pravega as a great pub-sub messaging system, because with Readers, Writers and ReaderGroups it is a great way to do messaging at scale. But you can also think about Pravega as a way to implement shared state in a consistent fashion across multiple cooperating processes distributed in a cluster. It is this latter category that we explore with this document. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts ) before continuing reading this page. In particular, you should be somewhat familiar with the State Synchronizer concept. Shared State and Pravega State Synchronizer is a facility provided by the Pravega programming model to make it easy for developers to use Pravega to coordinate shared state between processes. The idea is that a Stream is used to persist a sequence of changes to shared state and that various applications use their Pravega Java Client Library to concurrently read and write the shared state in a consistent fashion. SharedStateMap and Shared Configuration Example Before we dive into the details about how to use State Synchronizer, let's take a quick look at an example application that uses State Synchronizer. We have provided a simple yet illustrative example of using State Synchronizer here. The example uses State Synchronizer to build an implementation of Java's Map data structure called SharedMap. We use that primitive SharedMap data structure to build a Shared Config, that allows a set of processes to consistently read/write a shared, configuration object of key/value pair properties. Also as part of that example, we provide a simple command line-based application that allows you to play around with the SharedConfig app. Here is a menu of the available commands in the SharedConfigCLI application: Enter one of the following commands at the command line prompt: GET_ALL - prints out all of the properties in the Shared Config. GET {key} - print out the configuration property for the given key. PUT {key} , {value} - update the Shared Config with the given key/value pair. Print out previous value (if it existed). PUT_IF_ABSENT {key} , {value} - update the Shared Config with the given key/value pair, only if the property is not already defined. REMOVE {key} [ , {currentValue}] - remove the given property from the Shared Config. If {currentValue} is given, remove only if the property s current value matches {currentValue}.. REPLACE {key} , {newValue} [ , {currentValue}] - update the value of the property. If {currentValue} is given, update only if the property s current value matches {cuurentValue}. CLEAR - remove all the keys from the Shared Config. REFRESH - force an update from the Synchronized State. HELP - print out a list of commands. QUIT - terminate the program. Install the Pravega-Samples and launch two instances of the SharedConfigCLI using the same scope and stream name. This will simulate how two different processes can coordinate their local copy of the SharedConfig with one shared state object. You can follow these steps to get a feel for how the SharedConfig is coordinated: # Process 1 Process 2 Discussion 1 GET_ALL GET_ALL Shows that both processes see an empty SharedConfig 2 PUT p1,v1 Process 1 adds a property named p1 3 GET p1 GET p1 Process 1 sees value v1 for the property Process 2 does not have a property named p1. Why? Because it has not refreshed its state with the shared state 4 REFRESH Re-synchronize Process 2's state with the shared state 5 GET p1 Now Process 2 sees the change Process 1 made in step 2 6 REPLACE p1, newVal, v1 Process 2 attempts to change the value of p1, but uses a conditional replace, meaning the change should be made only if the old value of p1 is v1 (which it is at this point) 7 GET p1 Sure enough, the value of p1 was changed to newVal 8 REPLACE p1, anotherVal, v1 Process 1 tries to change the value of p1 in the same way Process 2 did in step 6. This will fail because the value of p1 in shared state is no longer v1 9 GET p1 The failed replace operation in step 8 caused Process 1's copy of the shared state to be updated, its value is now newVal because of step 6. You can repeat with a similar sequence to explore the semantics of PUT_IF_ABSENT and other operations that modify shared state. The idea is that modifications to the SharedConfig succeed only if they operate on the latest value. We use optimistic concurrency to implement efficient consistency across multiple consumers of the SharedConfig object. You can have multiple different SharedConfig state objects running simultaneously, each separate SharedConfig uses State Synchronizer objects based on a different Pravega Stream. Of course if you launch two applications using State Synchronizer objects backed by the same Stream, you get two processes concurrently accessing the shared state. This is exactly the situation we illustrated above. Using State Synchronizer to Build the SharedMap We used the State Synchronizer to build the SharedMap object in Pravega-Samples. State Synchronizer can be used to build a shared version of almost any data structure. Maybe your app needs to share just a simple integer count of something; we can use State Synchronizer to build a simple shared counter. Maybe the data you are sharing is a Set of currently running servers in a cluster; we can use State Synchronizer to build a shared Set. The possibilities are many. Let's explore how to build shared objects using State Synchronizer by examining how we built Shared Map. State Synchronizer State Synchronizer is a type of Pravega client, similar to an EventStreamReader or EventStreamWriter. A State Synchronizer is created via a ClientFactory object. Each State Synchronizer has a unique name within a Scope. A SynchronizerConfig object is used to tailor the behavior of a StateSynchronizer (although currently, there are no properties on a State Synchronizer that are configurable). State Synchronizer uses Java generic types to allow a developer to specify a type specific State Synchronizer. All of these things are done in a fashion similar to how EventStreamReaders and EventStreamWriters are used. StateT When designing an application that uses State Synchronizer, the developer needs to decide what type of state is going to be synchronized (shared). Are we sharing a Map? A Set? A Pojo? What is the data structure that is being shared. This defines the core \"type\" of the State Synchronizer (the StateT generic type in the State Synchronizer interface). The StateT object can be any Java object that implements the Revisioned interface defined by Pravega. Revisioned is a simple interface that allows Pravega to ensure it can properly compare two different StateT objects. In our example, the SharedMap is the State Synchronizer application. It defines a simple Map object presenting the typical get(key), set (key, value) etc. operations you would expect from a key-value pair map object. It implements the Revisioned interface, as required to use the State Synchronizer, and uses a simple ConcurrentHashMap as its internal implementation of the Map. So in our example, StateT corresponds to SharedStateMap\\ K,V>. UpdateT and InitialUpdateT In addition to StateT, there are two other generic types that need to be defined by a StateSynchronizer app: an Update type and an InitialUpdate type). The UpdateType represents the \"delta\" or change objects that are persisted on the Pravega Stream. The InitialUpdateType is a special update object used to to start the State Synchronizer off. Both UpdateType and InitialUpdateType are defined in terms of StateT. The StateSynchronizer uses a single Segment on a Stream to store updates (changes) to the shared state object. Changes, in the form of Initial or Update type objects, are written to the Stream based on whether the update is relative to the most current copy of the state in the Stream. If an update is presented that is based on an older version of the state, the update is not made. The StateSynchronizer object itself keeps a local in memory copy of the state, it also keeps version metadata about that copy of the state. Local state can be retrieved using the getState() operation. The local in memory copy could be stale, and it can be refreshed by an application using the fetchUpdates() operation, that retrieves all the changes made to the given version of the state. Most changes from the application are made through the updateState() operation. The updateState() operation takes a Function as parameter. The Function is invoked with the latest state object, and computes the updates to be applied. In our example, InitialUpdateT is implemented as: /** * Create a Map. This is used by StateSynchronizer to initialize shared state. */ private static class CreateState K , V implements InitialUpdate SharedStateMap K , V , Serializable { private static final long serialVersionUID = 1L ; private final ConcurrentHashMap K , V impl ; public CreateState ( ConcurrentHashMap K , V impl ) { this . impl = impl ; } @Override public SharedStateMap K , V create ( String scopedStreamName , Revision revision ) { return new SharedStateMap K , V ( scopedStreamName , impl , revision ); } } In this case, the CreateState class is used to initialize the shared state in the Stream by creating a new, empty SharedStateMap object. You could imagine other examples of InitialUpdate that would set a counter to 1, or perhaps initialize a Set to a fixed initial set of members. It may seem a bit odd that functions like \"initialize\" and \"update\" are expressed as classes, but when you think about it, that makes sense. The changes, like initialize and update, need to be stored in Pravega, therefore they need to be serializable objects. It must be possible for client applications to be able to start at any time, compute the current state and then keep up as changes are written to the Stream. If we just stored \"the latest state value\" in the Stream, there would be no way to consistently provide concurrent update and read using optimistic concurrency. UpdateT is a bit more tricky. There isn't just one kind of update to a Map, but rather there are all sorts of updates: put of a key/value pair, put of a collection of key/value pairs, removing a key/value pair and clearing all of the key/value pairs, Each of these \"kinds\" of updates are represented by their own Class. We define an abstract class, called StateUpdate, from which all of these \"operational\" update classes inherit. StateUpdate abstract class /** * A base class for all updates to the shared state. This allows for several different types of updates. */ private static abstract class StateUpdate K , V implements Update SharedStateMap K , V , Serializable { private static final long serialVersionUID = 1L ; @Override public SharedStateMap K , V applyTo ( SharedStateMap K , V oldState , Revision newRevision ) { ConcurrentHashMap K , V newState = new ConcurrentHashMap K , V ( oldState . impl ); process ( newState ); return new SharedStateMap K , V ( oldState . getScopedStreamName (), newState , newRevision ); } public abstract void process ( ConcurrentHashMap K , V updatableList ); } By defining an abstract class, we can define UpdateT in terms of the abstract StateUpdate class. The abstract class implements the \"applyTo\" method that is invoked by the StateSynchronizer to apply the update to the current state object and return an updated state object. The actual work is done on a copy of the old state's underlying Map (impl) object, a \"process\" operation is applied (specific to each subclass) to the impl object and a new version of the SharedState, using the post-processed impl as the internal state. The abstract class defines a process() method that actually does the work of whatever update needs to be applied. This method is implemented by the various concrete classes that represent Put, PutAll etc. operations on the shared map. Here, for example, is the way we implement the Put(key,value) operation on the SharedMap object: Put as an Update Object /** * Add a key/value pair to the State. */ private static class Put K , V extends StateUpdate K , V { private static final long serialVersionUID = 1L ; private final K key ; private final V value ; public Put ( K key , V value ) { this . key = key ; this . value = value ; } @Override public void process ( ConcurrentHashMap K , V impl ) { impl . put ( key , value ); } } Here, the process() operation is to add a key/value pair to the map, or if the key already exists, change the value. Each of the \"operations\" on the SharedMap is implemented in terms of creating instances of the various subclasses of StateUpdate. Executing Operations on SharedMap SharedMap demonstrates the typical operations on a StateSynchronizer. SharedMap presents an API, very similar to Java's Map\\ K,V> interface. It implements the Map operations in terms of manipulating the StateSynchronizer, using the various subclasses of StateUpdate to perform state change (write) operations. Create/Initialize Creating a SharedMap /** * Creates the shared state using a synchronizer based on the given stream name. * * @param clientFactory - the Pravega ClientFactory to use to create the StateSynchronizer. * @param streamManager - the Pravega StreamManager to use to create the Scope and the Stream used by the StateSynchronizer * @param scope - the Scope to use to create the Stream used by the StateSynchronizer. * @param name - the name of the Stream to be used by the StateSynchronizer. */ public SharedMap ( ClientFactory clientFactory , StreamManager streamManager , String scope , String name ){ streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder (). scope ( scope ). streamName ( name ) . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); streamManager . createStream ( scope , name , streamConfig ); this . stateSynchronizer = clientFactory . createStateSynchronizer ( name , new JavaSerializer StateUpdate K , V (), new JavaSerializer CreateState K , V (), SynchronizerConfig . builder (). build ()); stateSynchronizer . initialize ( new CreateState K , V ( new ConcurrentHashMap K , V ())); } A SharedMap object is created by defining the scope and stream (almost always the case, the scope and stream probably already exist, so the steps in lines 10-16 are usually no-ops. The StateSynchronizer object itself is constructed in lines 18-21 using the ClientFactory in a fashion similar to the way a Pravega Reader or Writer would be created. Note that the UpdateT object and InitialUpdateT object can have separate Java serializers specified. Currently, the SynchronizerConfig object is pretty dull; there are no configuration items currently available on the StateSynchronizer. The StateSynchronizer provides an initialize() API that takes an InitialUpdate object. This is called in the SharedMap constructor to make sure the SharedState is properly initialized. Note, in many cases, the SharedMap object will be created on a stream that already contains shared state for the SharedMap. Even in this case, it is ok to call initialize() because initialize() won't modify the shared state in the Stream. Read Operations The read operations, operations that do not alter shared state, like get(key) containsValue(value) etc., work against the local copy of the StateSynchronizer. All of these operations retrieve the current local state using getState() and then do the read operation from that state. The local state of the StateSynchronizer might be stale. In these cases, the SharedMap client would use refresh() to force the StateSynchronizer to refresh its state from shared state using the fetchUpdates() operation on the StateSynchronizer object. Note, this is a design decision to trade off staleness for responsiveness. We could easily have implemented the read operations to instead always do a refresh before doing the read against local state. That would be a very efficient strategy if the developer expected that there will be frequent updates to the shared state. In our case, we had imagined that the SharedMap would be read frequently but updated relatively infrequently, and therefore chose to read against local state. Write (update) Operations Each write operation is implemented in terms of the various concrete StateUpdate objects we discussed earlier. The clear() operation uses the Clear subclass of StateUpdate to remove all the key/value pairs, put() uses the Put class, etc. Lets dive into the implementation of the put() operation to discuss StateSynchronizer programming in a bit more detail: Implementing put(key,value) /** * Associates the specified value with the specified key in this map. * * @param key - the key at which the value should be found. * @param value - the value to be entered into the map. * @return - the previous value (if it existed) for the given key or null if the key did not exist before this operation. */ public V put ( K key , V value ){ final AtomicReference V oldValue = new AtomicReference V ( null ); stateSynchronizer . updateState (( state , updates ) - { oldValue . set ( state . get ( key )); updates . add ( new Put K , V ( key , value )); }); return oldValue . get (); } It is important to note that the function provided to the StateSynchronizer's updateState() will be called potentially multiple times. The result of applying the function to the old state is written only when it is applied against the most current revision of the state. If there was a race and the optimistic concurrency check fails, it will be called again. Most of the time there will only be a small number of invocations. In some cases, the developer may choose to use fetchUpdates() to synchronize the StateSynchronizer with the latest copy of shared state from the stream before running updateState(). This is a matter of optimizing the tradeoff between how frequent updates are expected and how efficient you want the update to be. If you expect a lot of updates, call fetchUpdates() before calling updateState(). In our case, we didn't expect a lot of updates and therefore we process potentially several invocations of the function each time put() is called. Delete Operations We chose to implement the delete (remove) operations to also leverage the compact() feature of StateSynchronizer. We have a policy that after every 5 remove operations, and after every clear() operation, we do a compact operation. Now, we could have chosen to do a compact() operation after every 5 update operations, but we wanted to isolate the illustration of using compact() to just delete operations. You can think of compact() as a form of \"garbage collection\" in StateSynchronizer. After a certain number of changes have been written to SharedState, it might be efficient to write out a new initial state, an accumulated representation of all the changes, to the Stream. That way data older than the compact operation can be ignored and eventually removed from the Stream. As a result of the compact() operation, a new initial sate (Initial2) is written to the stream. Now, all the data from Change3 and older is no longer relevant and can be garbage collected out of the Stream.","title":"Working with State Synchronizer"},{"location":"state-synchronizer/#working-with-pravega-state-synchronizer","text":"You can think about Pravega as a streaming storage primitive, because it is a great way to durably persist data. You can think about Pravega as a great pub-sub messaging system, because with Readers, Writers and ReaderGroups it is a great way to do messaging at scale. But you can also think about Pravega as a way to implement shared state in a consistent fashion across multiple cooperating processes distributed in a cluster. It is this latter category that we explore with this document. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts ) before continuing reading this page. In particular, you should be somewhat familiar with the State Synchronizer concept.","title":"Working with Pravega: State Synchronizer"},{"location":"state-synchronizer/#shared-state-and-pravega","text":"State Synchronizer is a facility provided by the Pravega programming model to make it easy for developers to use Pravega to coordinate shared state between processes. The idea is that a Stream is used to persist a sequence of changes to shared state and that various applications use their Pravega Java Client Library to concurrently read and write the shared state in a consistent fashion.","title":"Shared State and Pravega"},{"location":"state-synchronizer/#sharedstatemap-and-shared-configuration-example","text":"Before we dive into the details about how to use State Synchronizer, let's take a quick look at an example application that uses State Synchronizer. We have provided a simple yet illustrative example of using State Synchronizer here. The example uses State Synchronizer to build an implementation of Java's Map data structure called SharedMap. We use that primitive SharedMap data structure to build a Shared Config, that allows a set of processes to consistently read/write a shared, configuration object of key/value pair properties. Also as part of that example, we provide a simple command line-based application that allows you to play around with the SharedConfig app. Here is a menu of the available commands in the SharedConfigCLI application: Enter one of the following commands at the command line prompt: GET_ALL - prints out all of the properties in the Shared Config. GET {key} - print out the configuration property for the given key. PUT {key} , {value} - update the Shared Config with the given key/value pair. Print out previous value (if it existed). PUT_IF_ABSENT {key} , {value} - update the Shared Config with the given key/value pair, only if the property is not already defined. REMOVE {key} [ , {currentValue}] - remove the given property from the Shared Config. If {currentValue} is given, remove only if the property s current value matches {currentValue}.. REPLACE {key} , {newValue} [ , {currentValue}] - update the value of the property. If {currentValue} is given, update only if the property s current value matches {cuurentValue}. CLEAR - remove all the keys from the Shared Config. REFRESH - force an update from the Synchronized State. HELP - print out a list of commands. QUIT - terminate the program. Install the Pravega-Samples and launch two instances of the SharedConfigCLI using the same scope and stream name. This will simulate how two different processes can coordinate their local copy of the SharedConfig with one shared state object. You can follow these steps to get a feel for how the SharedConfig is coordinated: # Process 1 Process 2 Discussion 1 GET_ALL GET_ALL Shows that both processes see an empty SharedConfig 2 PUT p1,v1 Process 1 adds a property named p1 3 GET p1 GET p1 Process 1 sees value v1 for the property Process 2 does not have a property named p1. Why? Because it has not refreshed its state with the shared state 4 REFRESH Re-synchronize Process 2's state with the shared state 5 GET p1 Now Process 2 sees the change Process 1 made in step 2 6 REPLACE p1, newVal, v1 Process 2 attempts to change the value of p1, but uses a conditional replace, meaning the change should be made only if the old value of p1 is v1 (which it is at this point) 7 GET p1 Sure enough, the value of p1 was changed to newVal 8 REPLACE p1, anotherVal, v1 Process 1 tries to change the value of p1 in the same way Process 2 did in step 6. This will fail because the value of p1 in shared state is no longer v1 9 GET p1 The failed replace operation in step 8 caused Process 1's copy of the shared state to be updated, its value is now newVal because of step 6. You can repeat with a similar sequence to explore the semantics of PUT_IF_ABSENT and other operations that modify shared state. The idea is that modifications to the SharedConfig succeed only if they operate on the latest value. We use optimistic concurrency to implement efficient consistency across multiple consumers of the SharedConfig object. You can have multiple different SharedConfig state objects running simultaneously, each separate SharedConfig uses State Synchronizer objects based on a different Pravega Stream. Of course if you launch two applications using State Synchronizer objects backed by the same Stream, you get two processes concurrently accessing the shared state. This is exactly the situation we illustrated above.","title":"SharedStateMap and Shared Configuration Example"},{"location":"state-synchronizer/#using-state-synchronizer-to-build-the-sharedmap","text":"We used the State Synchronizer to build the SharedMap object in Pravega-Samples. State Synchronizer can be used to build a shared version of almost any data structure. Maybe your app needs to share just a simple integer count of something; we can use State Synchronizer to build a simple shared counter. Maybe the data you are sharing is a Set of currently running servers in a cluster; we can use State Synchronizer to build a shared Set. The possibilities are many. Let's explore how to build shared objects using State Synchronizer by examining how we built Shared Map.","title":"Using State Synchronizer to Build the SharedMap"},{"location":"state-synchronizer/#state-synchronizer","text":"State Synchronizer is a type of Pravega client, similar to an EventStreamReader or EventStreamWriter. A State Synchronizer is created via a ClientFactory object. Each State Synchronizer has a unique name within a Scope. A SynchronizerConfig object is used to tailor the behavior of a StateSynchronizer (although currently, there are no properties on a State Synchronizer that are configurable). State Synchronizer uses Java generic types to allow a developer to specify a type specific State Synchronizer. All of these things are done in a fashion similar to how EventStreamReaders and EventStreamWriters are used.","title":"State Synchronizer"},{"location":"state-synchronizer/#statet","text":"When designing an application that uses State Synchronizer, the developer needs to decide what type of state is going to be synchronized (shared). Are we sharing a Map? A Set? A Pojo? What is the data structure that is being shared. This defines the core \"type\" of the State Synchronizer (the StateT generic type in the State Synchronizer interface). The StateT object can be any Java object that implements the Revisioned interface defined by Pravega. Revisioned is a simple interface that allows Pravega to ensure it can properly compare two different StateT objects. In our example, the SharedMap is the State Synchronizer application. It defines a simple Map object presenting the typical get(key), set (key, value) etc. operations you would expect from a key-value pair map object. It implements the Revisioned interface, as required to use the State Synchronizer, and uses a simple ConcurrentHashMap as its internal implementation of the Map. So in our example, StateT corresponds to SharedStateMap\\ K,V>.","title":"StateT"},{"location":"state-synchronizer/#updatet-and-initialupdatet","text":"In addition to StateT, there are two other generic types that need to be defined by a StateSynchronizer app: an Update type and an InitialUpdate type). The UpdateType represents the \"delta\" or change objects that are persisted on the Pravega Stream. The InitialUpdateType is a special update object used to to start the State Synchronizer off. Both UpdateType and InitialUpdateType are defined in terms of StateT. The StateSynchronizer uses a single Segment on a Stream to store updates (changes) to the shared state object. Changes, in the form of Initial or Update type objects, are written to the Stream based on whether the update is relative to the most current copy of the state in the Stream. If an update is presented that is based on an older version of the state, the update is not made. The StateSynchronizer object itself keeps a local in memory copy of the state, it also keeps version metadata about that copy of the state. Local state can be retrieved using the getState() operation. The local in memory copy could be stale, and it can be refreshed by an application using the fetchUpdates() operation, that retrieves all the changes made to the given version of the state. Most changes from the application are made through the updateState() operation. The updateState() operation takes a Function as parameter. The Function is invoked with the latest state object, and computes the updates to be applied. In our example, InitialUpdateT is implemented as: /** * Create a Map. This is used by StateSynchronizer to initialize shared state. */ private static class CreateState K , V implements InitialUpdate SharedStateMap K , V , Serializable { private static final long serialVersionUID = 1L ; private final ConcurrentHashMap K , V impl ; public CreateState ( ConcurrentHashMap K , V impl ) { this . impl = impl ; } @Override public SharedStateMap K , V create ( String scopedStreamName , Revision revision ) { return new SharedStateMap K , V ( scopedStreamName , impl , revision ); } } In this case, the CreateState class is used to initialize the shared state in the Stream by creating a new, empty SharedStateMap object. You could imagine other examples of InitialUpdate that would set a counter to 1, or perhaps initialize a Set to a fixed initial set of members. It may seem a bit odd that functions like \"initialize\" and \"update\" are expressed as classes, but when you think about it, that makes sense. The changes, like initialize and update, need to be stored in Pravega, therefore they need to be serializable objects. It must be possible for client applications to be able to start at any time, compute the current state and then keep up as changes are written to the Stream. If we just stored \"the latest state value\" in the Stream, there would be no way to consistently provide concurrent update and read using optimistic concurrency. UpdateT is a bit more tricky. There isn't just one kind of update to a Map, but rather there are all sorts of updates: put of a key/value pair, put of a collection of key/value pairs, removing a key/value pair and clearing all of the key/value pairs, Each of these \"kinds\" of updates are represented by their own Class. We define an abstract class, called StateUpdate, from which all of these \"operational\" update classes inherit. StateUpdate abstract class /** * A base class for all updates to the shared state. This allows for several different types of updates. */ private static abstract class StateUpdate K , V implements Update SharedStateMap K , V , Serializable { private static final long serialVersionUID = 1L ; @Override public SharedStateMap K , V applyTo ( SharedStateMap K , V oldState , Revision newRevision ) { ConcurrentHashMap K , V newState = new ConcurrentHashMap K , V ( oldState . impl ); process ( newState ); return new SharedStateMap K , V ( oldState . getScopedStreamName (), newState , newRevision ); } public abstract void process ( ConcurrentHashMap K , V updatableList ); } By defining an abstract class, we can define UpdateT in terms of the abstract StateUpdate class. The abstract class implements the \"applyTo\" method that is invoked by the StateSynchronizer to apply the update to the current state object and return an updated state object. The actual work is done on a copy of the old state's underlying Map (impl) object, a \"process\" operation is applied (specific to each subclass) to the impl object and a new version of the SharedState, using the post-processed impl as the internal state. The abstract class defines a process() method that actually does the work of whatever update needs to be applied. This method is implemented by the various concrete classes that represent Put, PutAll etc. operations on the shared map. Here, for example, is the way we implement the Put(key,value) operation on the SharedMap object: Put as an Update Object /** * Add a key/value pair to the State. */ private static class Put K , V extends StateUpdate K , V { private static final long serialVersionUID = 1L ; private final K key ; private final V value ; public Put ( K key , V value ) { this . key = key ; this . value = value ; } @Override public void process ( ConcurrentHashMap K , V impl ) { impl . put ( key , value ); } } Here, the process() operation is to add a key/value pair to the map, or if the key already exists, change the value. Each of the \"operations\" on the SharedMap is implemented in terms of creating instances of the various subclasses of StateUpdate.","title":"UpdateT and InitialUpdateT"},{"location":"state-synchronizer/#executing-operations-on-sharedmap","text":"SharedMap demonstrates the typical operations on a StateSynchronizer. SharedMap presents an API, very similar to Java's Map\\ K,V> interface. It implements the Map operations in terms of manipulating the StateSynchronizer, using the various subclasses of StateUpdate to perform state change (write) operations.","title":"Executing Operations on SharedMap"},{"location":"state-synchronizer/#createinitialize","text":"Creating a SharedMap /** * Creates the shared state using a synchronizer based on the given stream name. * * @param clientFactory - the Pravega ClientFactory to use to create the StateSynchronizer. * @param streamManager - the Pravega StreamManager to use to create the Scope and the Stream used by the StateSynchronizer * @param scope - the Scope to use to create the Stream used by the StateSynchronizer. * @param name - the name of the Stream to be used by the StateSynchronizer. */ public SharedMap ( ClientFactory clientFactory , StreamManager streamManager , String scope , String name ){ streamManager . createScope ( scope ); StreamConfiguration streamConfig = StreamConfiguration . builder (). scope ( scope ). streamName ( name ) . scalingPolicy ( ScalingPolicy . fixed ( 1 )) . build (); streamManager . createStream ( scope , name , streamConfig ); this . stateSynchronizer = clientFactory . createStateSynchronizer ( name , new JavaSerializer StateUpdate K , V (), new JavaSerializer CreateState K , V (), SynchronizerConfig . builder (). build ()); stateSynchronizer . initialize ( new CreateState K , V ( new ConcurrentHashMap K , V ())); } A SharedMap object is created by defining the scope and stream (almost always the case, the scope and stream probably already exist, so the steps in lines 10-16 are usually no-ops. The StateSynchronizer object itself is constructed in lines 18-21 using the ClientFactory in a fashion similar to the way a Pravega Reader or Writer would be created. Note that the UpdateT object and InitialUpdateT object can have separate Java serializers specified. Currently, the SynchronizerConfig object is pretty dull; there are no configuration items currently available on the StateSynchronizer. The StateSynchronizer provides an initialize() API that takes an InitialUpdate object. This is called in the SharedMap constructor to make sure the SharedState is properly initialized. Note, in many cases, the SharedMap object will be created on a stream that already contains shared state for the SharedMap. Even in this case, it is ok to call initialize() because initialize() won't modify the shared state in the Stream.","title":"Create/Initialize"},{"location":"state-synchronizer/#read-operations","text":"The read operations, operations that do not alter shared state, like get(key) containsValue(value) etc., work against the local copy of the StateSynchronizer. All of these operations retrieve the current local state using getState() and then do the read operation from that state. The local state of the StateSynchronizer might be stale. In these cases, the SharedMap client would use refresh() to force the StateSynchronizer to refresh its state from shared state using the fetchUpdates() operation on the StateSynchronizer object. Note, this is a design decision to trade off staleness for responsiveness. We could easily have implemented the read operations to instead always do a refresh before doing the read against local state. That would be a very efficient strategy if the developer expected that there will be frequent updates to the shared state. In our case, we had imagined that the SharedMap would be read frequently but updated relatively infrequently, and therefore chose to read against local state.","title":"Read Operations"},{"location":"state-synchronizer/#write-update-operations","text":"Each write operation is implemented in terms of the various concrete StateUpdate objects we discussed earlier. The clear() operation uses the Clear subclass of StateUpdate to remove all the key/value pairs, put() uses the Put class, etc. Lets dive into the implementation of the put() operation to discuss StateSynchronizer programming in a bit more detail: Implementing put(key,value) /** * Associates the specified value with the specified key in this map. * * @param key - the key at which the value should be found. * @param value - the value to be entered into the map. * @return - the previous value (if it existed) for the given key or null if the key did not exist before this operation. */ public V put ( K key , V value ){ final AtomicReference V oldValue = new AtomicReference V ( null ); stateSynchronizer . updateState (( state , updates ) - { oldValue . set ( state . get ( key )); updates . add ( new Put K , V ( key , value )); }); return oldValue . get (); } It is important to note that the function provided to the StateSynchronizer's updateState() will be called potentially multiple times. The result of applying the function to the old state is written only when it is applied against the most current revision of the state. If there was a race and the optimistic concurrency check fails, it will be called again. Most of the time there will only be a small number of invocations. In some cases, the developer may choose to use fetchUpdates() to synchronize the StateSynchronizer with the latest copy of shared state from the stream before running updateState(). This is a matter of optimizing the tradeoff between how frequent updates are expected and how efficient you want the update to be. If you expect a lot of updates, call fetchUpdates() before calling updateState(). In our case, we didn't expect a lot of updates and therefore we process potentially several invocations of the function each time put() is called.","title":"Write (update) Operations"},{"location":"state-synchronizer/#delete-operations","text":"We chose to implement the delete (remove) operations to also leverage the compact() feature of StateSynchronizer. We have a policy that after every 5 remove operations, and after every clear() operation, we do a compact operation. Now, we could have chosen to do a compact() operation after every 5 update operations, but we wanted to isolate the illustration of using compact() to just delete operations. You can think of compact() as a form of \"garbage collection\" in StateSynchronizer. After a certain number of changes have been written to SharedState, it might be efficient to write out a new initial state, an accumulated representation of all the changes, to the Stream. That way data older than the compact operation can be ignored and eventually removed from the Stream. As a result of the compact() operation, a new initial sate (Initial2) is written to the stream. Now, all the data from Change3 and older is no longer relevant and can be garbage collected out of the Stream.","title":"Delete Operations"},{"location":"streamcuts/","text":"Working with Pravega: StreamCuts This section describes StreamCut s and how they can be used with streaming clients and batch clients. Pre-requisites: You should be familiar with Pravega Concepts . Definition A Pravega stream is formed by one or multiple parallel segments for storing/reading events. A Pravega stream is elastic, which means that the number of parallel segments may change along time to accommodate fluctuating workloads. That said, a StreamCut represents a consistent position in the stream. It contains a set of segment and offset pairs for a single stream which represents the complete keyspace at a given point in time. The offset always points to the event boundary and hence there will be no offset pointing to an incomplete event. The StreamCut representing the tail of the stream (with the newest event) is an ever changing one since events can be continuously added to the stream and the StreamCut pointing to the tail of the stream with newer events would have a different value. Similarly the StreamCut representing the head of the stream (with the oldest event) is an ever changing one as the stream retention policy could truncate the stream and the StreamCut pointing to the head of the stream post truncation would have a different value. StreamCut.UNBOUNDED is used to represent such a position in the stream and the user can use it to specify this ever changing stream position (both head and tail of the stream). It should be noted that StreamCut s obtained using the streaming client and batch client can be used interchangeably. StreamCut with Reader A ReaderGroup is a named collection of Readers that together, in parallel, read Events from a given Stream. Every Reader is always associated with a ReaderGroup. StreamCut (s) can be obtained from a ReaderGroup using the following api io.pravega.client.stream.ReaderGroup.getStreamCuts . This api returns a Map Stream, StreamCut which represents the last known position of the Readers for all the streams managed by the ReaderGroup. A StreamCut can be used to configure a ReaderGroup to enable bounded processing of a Stream. The start and/or end StreamCut of a Stream can be passed as part of the ReaderGroup configuration. The below example shows the different ways to use StreamCut s as part of the ReaderGroup configuration. /* * The below ReaderGroup configuration ensures that the readers belonging to * the ReaderGroup read events from * - Stream s1 from startStreamCut1 (representing the oldest event) upto endStreamCut1 (representing the newest event) * - Stream s2 from startStreamCut2 upto the tail of the stream, this is similar to using StreamCut.UNBOUNDED * for endStreamCut. * - Stream s3 from the current head of the stream upto endStreamCut2 * - Stream s4 from the current head of the stream upto the tail of the stream. */ ReaderGroupConfig . builder () . stream ( scope/s1 , startStreamCut1 , endStreamCut1 ) . stream ( scope/s2 , startStreamCut2 ) . stream ( scope/s3 , StreamCut . UNBOUNDED , endStreamCut2 ) . stream ( scope/s4 ) . build (); The below API can be used to reset an existing ReaderGroup with a new ReaderGroup configuration instead creating a ReaderGroup. /* * ReaderGroup api used to reset a ReaderGroup to a newer ReaderGroup configuration. */ io.pravega.client.stream.ReaderGroup.resetReaderGroup(ReaderGroupConfig config) StreamCut with BatchClient StreamCut representing the current head and current tail of a stream can be obtained using below BatchClient API. /* * The API io.pravega.client.batch.BatchClient.getStreamInfo(Stream stream) fetches the StreamCut representing the * current head and tail of the stream. StreamInfo.getHeadStreamCut() and StreamInfo.getTailStreamCut() can be * used to fetch the StreamCuts. */ CompletableFuture StreamInfo getStreamInfo(Stream stream); BatchClient can be used to perform bounded processing of the stream given the start and end StreamCut s. BatchClient api io.pravega.client.batch.BatchClient.getSegments(stream, startStreamCut, endStreamCut) is used to fetch segments which reside between the given startStreamCut and endStreamCut. With the retrieved segment information the user can consume all the events in parallel without adhering to time ordering of events. It must be noted that passing StreamCut.UNBOUNDED to startStreamCut and endStreamCut will result in using the current head of stream and the current tail of the stream, respectively. We have provided a simple yet illustrative example of using StreamCut here.","title":"Working with StreamCuts"},{"location":"streamcuts/#working-with-pravega-streamcuts","text":"This section describes StreamCut s and how they can be used with streaming clients and batch clients. Pre-requisites: You should be familiar with Pravega Concepts .","title":"Working with Pravega: StreamCuts"},{"location":"streamcuts/#definition","text":"A Pravega stream is formed by one or multiple parallel segments for storing/reading events. A Pravega stream is elastic, which means that the number of parallel segments may change along time to accommodate fluctuating workloads. That said, a StreamCut represents a consistent position in the stream. It contains a set of segment and offset pairs for a single stream which represents the complete keyspace at a given point in time. The offset always points to the event boundary and hence there will be no offset pointing to an incomplete event. The StreamCut representing the tail of the stream (with the newest event) is an ever changing one since events can be continuously added to the stream and the StreamCut pointing to the tail of the stream with newer events would have a different value. Similarly the StreamCut representing the head of the stream (with the oldest event) is an ever changing one as the stream retention policy could truncate the stream and the StreamCut pointing to the head of the stream post truncation would have a different value. StreamCut.UNBOUNDED is used to represent such a position in the stream and the user can use it to specify this ever changing stream position (both head and tail of the stream). It should be noted that StreamCut s obtained using the streaming client and batch client can be used interchangeably.","title":"Definition"},{"location":"streamcuts/#streamcut-with-reader","text":"A ReaderGroup is a named collection of Readers that together, in parallel, read Events from a given Stream. Every Reader is always associated with a ReaderGroup. StreamCut (s) can be obtained from a ReaderGroup using the following api io.pravega.client.stream.ReaderGroup.getStreamCuts . This api returns a Map Stream, StreamCut which represents the last known position of the Readers for all the streams managed by the ReaderGroup. A StreamCut can be used to configure a ReaderGroup to enable bounded processing of a Stream. The start and/or end StreamCut of a Stream can be passed as part of the ReaderGroup configuration. The below example shows the different ways to use StreamCut s as part of the ReaderGroup configuration. /* * The below ReaderGroup configuration ensures that the readers belonging to * the ReaderGroup read events from * - Stream s1 from startStreamCut1 (representing the oldest event) upto endStreamCut1 (representing the newest event) * - Stream s2 from startStreamCut2 upto the tail of the stream, this is similar to using StreamCut.UNBOUNDED * for endStreamCut. * - Stream s3 from the current head of the stream upto endStreamCut2 * - Stream s4 from the current head of the stream upto the tail of the stream. */ ReaderGroupConfig . builder () . stream ( scope/s1 , startStreamCut1 , endStreamCut1 ) . stream ( scope/s2 , startStreamCut2 ) . stream ( scope/s3 , StreamCut . UNBOUNDED , endStreamCut2 ) . stream ( scope/s4 ) . build (); The below API can be used to reset an existing ReaderGroup with a new ReaderGroup configuration instead creating a ReaderGroup. /* * ReaderGroup api used to reset a ReaderGroup to a newer ReaderGroup configuration. */ io.pravega.client.stream.ReaderGroup.resetReaderGroup(ReaderGroupConfig config)","title":"StreamCut with Reader"},{"location":"streamcuts/#streamcut-with-batchclient","text":"StreamCut representing the current head and current tail of a stream can be obtained using below BatchClient API. /* * The API io.pravega.client.batch.BatchClient.getStreamInfo(Stream stream) fetches the StreamCut representing the * current head and tail of the stream. StreamInfo.getHeadStreamCut() and StreamInfo.getTailStreamCut() can be * used to fetch the StreamCuts. */ CompletableFuture StreamInfo getStreamInfo(Stream stream); BatchClient can be used to perform bounded processing of the stream given the start and end StreamCut s. BatchClient api io.pravega.client.batch.BatchClient.getSegments(stream, startStreamCut, endStreamCut) is used to fetch segments which reside between the given startStreamCut and endStreamCut. With the retrieved segment information the user can consume all the events in parallel without adhering to time ordering of events. It must be noted that passing StreamCut.UNBOUNDED to startStreamCut and endStreamCut will result in using the current head of stream and the current tail of the stream, respectively. We have provided a simple yet illustrative example of using StreamCut here.","title":"StreamCut with BatchClient"},{"location":"terminology/","text":"Terminology Here is a glossary of terms related to Pravega: Term Definition Pravega Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. Stream A durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. A Stream is identified by a name and a Scope . A Stream is comprised of one or more Stream Segments. Stream Segment A shard of a Stream . The number of Stream Segments in a Stream might vary over time according to load and Scaling Policy . In the absence of a Scale Event , Events written to a Stream with the same Routing Key are stored in the same Stream Segment and are totally ordered. When a Scale Event occurs, the set of Stream Segments of a Stream changes and Events written with a given Routing Key K before the Scaling Event are stored in a different Stream Segment compared to Events written with the same Routing Key K after the event. In conjunction with Reader Groups , the number of Stream Segments is the maximum amount of read parallelism of a Stream . Scope A namespace for Stream names. A Stream name must be unique within a Scope . Event A collection of bytes within a Stream. An Event is associated with a Routing Key. Routing Key A property of an Event used to route messages to Readers. Two Events with the same Routing Key will be read by Readers in exactly the same order they were written. Reader A software application that reads data from one or more Streams . Writer A software application that writes data to one or more Streams. Pravega Java Client Library A Java library that applications use to interface with Pravega Reader Group A named collection of one or more Readers that read from a Stream in parallel. Pravega assigns Stream Segments to the Readers making sure that ll Stream Segments are assigned to at least one Reader and that they are balanced across the Readers . Position An offset within a Stream , representing a type of recovery point for a Reader . If a Reader crashes, a Position can be used to initialize the failed Reader 's replacement so that the replacement resumes processing the Stream from where the failed Reader left off. Tier 1 Storage Short term, low-latency, data storage that guarantees the durability of data written to Streams . The current implementation of Tier 1 uses Apache Bookkeeper . Tier 1 storage keeps the most recent appends to streams in Pravega. As data in Tier 1 ages, it is moved out of Tier 1 into Tier 2. Tier 2 Storage A portion of Pravega storage based on cheap and deep persistent storage technology such as HDFS , DellEMC's Isilon or DellEMC's Elastic Cloud Storage . Pravega Server A component of Pravega that implements the Pravega data plane API for operations such as reading from and writing to Streams . The data plane of Pravega, also called the Segment Store, is composed of 1 or more Pravega Server instances. Segment Store A collection of Pravega Servers that in aggregate form the data plane of a Pravega cluster. Controller A component of Pravega that implements the Pravega control plane API for operations such as creating and retrieving information about Streams . The control plane of Pravega is composed of 1 or more Controller instances coordinated by Zookeeper . Auto Scaling A Pravega concept that allows the number of Stream Segments in a Stream to change over time, based on Scaling Policy. Scaling Policy A configuration item of a Stream that determines how the number of Stream Segments in the Stream should change over time. There are three kinds of Scaling Policy , a Stream has exactly one of these at any given time. - Fixed number of Stream Segments - Change the number of Stream Segments based on the number of bytes per second written to the Stream - Change the number of Stream Segments based on the number of Events er second written to the Stream Scale Event There are two types of Scale Event : Scale-Up Event and Scale-Down Event. A Scale Event triggers Auto Scaling . A Scale-Up Event is a situation where an increase in load causes one or more Stream Segments to be split, increasing the number of Stream Segments in the Stream . A Scale-Down Event is a situation where a decrease in load causes one or more Stream Segments to be merged, reducing the number of Stream Segments in the Stream . Transaction A collection of Stream write operations that are applied atomically to the Stream . Either all of the bytes in a Transaction are written to the Stream or none of them are. State Synchronizer An abstraction built on top of Pravega to enable the implementation of replicated state using a Pravega segment to back up the state transformations. A State Synchronizer allows a piece of data to be shared between multiple processes with strong consistency and optimistic concurrency. Checkpoint A kind of Event that signals all Readers within a Reader Group to persist their state. StreamCut A StreamCut represents a consistent position in the Stream . It contains a set of Segment and offset pairs for a single Stream which represents the complete keyspace at a given point in time. The offset always points to the event boundary and hence there will be no offset pointing to an incomplete Event .","title":"Terminology"},{"location":"terminology/#terminology","text":"Here is a glossary of terms related to Pravega: Term Definition Pravega Pravega is an open source storage primitive implementing Streams for continuous and unbounded data. Stream A durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. A Stream is identified by a name and a Scope . A Stream is comprised of one or more Stream Segments. Stream Segment A shard of a Stream . The number of Stream Segments in a Stream might vary over time according to load and Scaling Policy . In the absence of a Scale Event , Events written to a Stream with the same Routing Key are stored in the same Stream Segment and are totally ordered. When a Scale Event occurs, the set of Stream Segments of a Stream changes and Events written with a given Routing Key K before the Scaling Event are stored in a different Stream Segment compared to Events written with the same Routing Key K after the event. In conjunction with Reader Groups , the number of Stream Segments is the maximum amount of read parallelism of a Stream . Scope A namespace for Stream names. A Stream name must be unique within a Scope . Event A collection of bytes within a Stream. An Event is associated with a Routing Key. Routing Key A property of an Event used to route messages to Readers. Two Events with the same Routing Key will be read by Readers in exactly the same order they were written. Reader A software application that reads data from one or more Streams . Writer A software application that writes data to one or more Streams. Pravega Java Client Library A Java library that applications use to interface with Pravega Reader Group A named collection of one or more Readers that read from a Stream in parallel. Pravega assigns Stream Segments to the Readers making sure that ll Stream Segments are assigned to at least one Reader and that they are balanced across the Readers . Position An offset within a Stream , representing a type of recovery point for a Reader . If a Reader crashes, a Position can be used to initialize the failed Reader 's replacement so that the replacement resumes processing the Stream from where the failed Reader left off. Tier 1 Storage Short term, low-latency, data storage that guarantees the durability of data written to Streams . The current implementation of Tier 1 uses Apache Bookkeeper . Tier 1 storage keeps the most recent appends to streams in Pravega. As data in Tier 1 ages, it is moved out of Tier 1 into Tier 2. Tier 2 Storage A portion of Pravega storage based on cheap and deep persistent storage technology such as HDFS , DellEMC's Isilon or DellEMC's Elastic Cloud Storage . Pravega Server A component of Pravega that implements the Pravega data plane API for operations such as reading from and writing to Streams . The data plane of Pravega, also called the Segment Store, is composed of 1 or more Pravega Server instances. Segment Store A collection of Pravega Servers that in aggregate form the data plane of a Pravega cluster. Controller A component of Pravega that implements the Pravega control plane API for operations such as creating and retrieving information about Streams . The control plane of Pravega is composed of 1 or more Controller instances coordinated by Zookeeper . Auto Scaling A Pravega concept that allows the number of Stream Segments in a Stream to change over time, based on Scaling Policy. Scaling Policy A configuration item of a Stream that determines how the number of Stream Segments in the Stream should change over time. There are three kinds of Scaling Policy , a Stream has exactly one of these at any given time. - Fixed number of Stream Segments - Change the number of Stream Segments based on the number of bytes per second written to the Stream - Change the number of Stream Segments based on the number of Events er second written to the Stream Scale Event There are two types of Scale Event : Scale-Up Event and Scale-Down Event. A Scale Event triggers Auto Scaling . A Scale-Up Event is a situation where an increase in load causes one or more Stream Segments to be split, increasing the number of Stream Segments in the Stream . A Scale-Down Event is a situation where a decrease in load causes one or more Stream Segments to be merged, reducing the number of Stream Segments in the Stream . Transaction A collection of Stream write operations that are applied atomically to the Stream . Either all of the bytes in a Transaction are written to the Stream or none of them are. State Synchronizer An abstraction built on top of Pravega to enable the implementation of replicated state using a Pravega segment to back up the state transformations. A State Synchronizer allows a piece of data to be shared between multiple processes with strong consistency and optimistic concurrency. Checkpoint A kind of Event that signals all Readers within a Reader Group to persist their state. StreamCut A StreamCut represents a consistent position in the Stream . It contains a set of Segment and offset pairs for a single Stream which represents the complete keyspace at a given point in time. The offset always points to the event boundary and hence there will be no offset pointing to an incomplete Event .","title":"Terminology"},{"location":"transactions/","text":"Working with Pravega: Transactions This article explores how to write a set of Events to a Stream atomically using Pravega Transactions. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts ) before continuing reading this page. Pravega Transactions and the Console Writer and Console Reader Apps We have written a couple of applications, ConsoleReader and ConsoleWriter that help illustrate reading and writing data with Pravega and in particular to illustrate the Transaction facility in the Pravega programming model. You can find those applications here . ConsoleReader The ConsoleReader app is very simple. It uses the Pravega Java Client Library to read from a Stream and output each Event onto the console. It runs indefinitely, so you have to kill the process to terminate the program. ConsoleWriter The ConsoleWriter app is a bit more sophisticated. It uses the Pravega Java Client Library to write Events to a Stream, including Events written in the context of a Pravega Transaction. To make manipulating Transactions a bit easier, we provide a console-based CLI. The help text for the CLI is shown below: ConsoleWriter Help text Enter one of the following commands at the command line prompt: If no command is entered, the line is treated as a parameter to the WRITE_EVENT command. WRITE_EVENT {event} - write the {event} out to the Stream or the current Transaction. WRITE_EVENT_RK {routingKey} , {event} - write the {event} out to the Stream or the current Transaction using {routingKey}. Note and around {routingKey}. BEGIN - begin a Transaction. Only one Transaction at a time is supported by the CLI. GET_TXN_ID - output the current Transaction s Id (if a Transaction is running) FLUSH - flush the current Transaction (if a Transaction is running) COMMIT - commit the Transaction (if a Transaction is running) ABORT - abort the Transaction (if a Transaction is running) STATUS - check the status of the Transaction(if a Transaction is running) HELP - print out a list of commands. QUIT - terminate the program. examples/someStream So writing a single Event is simple, just type some text (you don't even have to type the WRITE_EVENT command if you don't want to). But we really want to talk about Pravega Transactions, so lets dive into that. Pravega Transactions The idea with a Pravega Transaction is that it allows an application to prepare a set of Events that can be written \"all at once\" to a Stream. This allows an application to \"commit\" a bunch of Events Atomically. This is done by writing them into the Transaction and calling commit to append them to the Stream. An application might want to do this in cases where it wants the Events to be durably stored and later decided whether or not those Events should be appended to the Stream. This allows the application to control when the set of Events are made visible to Readers. A Transaction is created via an EventStreamWriter. Recall that an EventStreamWriter itself is created through a ClientFactory and is constructed to operate against a Stream. Transactions are therefore bound to a Stream. Once a Transaction is created, it acts a lot like a Writer. Applications Write Events to the Transaction and once acknowledged, the data is considered durably persisted in the Transaction. Note that the data written to a Transaction will not be visible to Readers until the Transaction is committed. In addition to writeEvent and writeEvent using a routing key, there are several Transaction specific operations provided: Operation Discussion getTxnId() Retrieve the unique identifier for the Transaction. Pravega generates a unique UUID for each Transaction. flush() Ensure that all Writes have been persisted. ping() Extend the duration of a Transaction. Note that after a certain amount of idle time, the Transaction will automatically abort. This is to handle the case where the client has crashed and it is no longer appropriate to keep resources associated with the Transaction. checkStatus() Return the state of the Transaction. The Transaction can be in one of the following states: Open, Committing, Committed, Aborting or Aborted. commit() Append all of the Events written to the Transaction into the Stream. Either all of the Event data will be appended to the Stream or none of it will be. abort() Terminate the Transaction, the data written to the Transaction will be deleted. Using the ConsoleWriter to Begin and Commit a Transaction All of the Transaction API is reflected in the ConsoleWriter's CLI command set. To begin a transaction, type BEGIN: Begin Transaction examples/someStream begin 346d8561-3fd8-40b6-8c15-9343eeea2992 When a Transaction is created, it returns a Transaction object parameterized to the type of Event supported by the Stream. In the case of the ConsoleWriter, the type of Event is a Java String. The command prompt changes to show the Transaction's id. Now any of the Transaction related commands can be issued (GET_TXN_ID, FLUSH, PING, COMMIT, ABORT and STATUS). Note that the BEGIN command won't work because the ConsoleWriter supports only one Transaction at a time (this is a limitation of the app, not a limitation of Pravega). When the ConsoleWriter is in a Transactional context, the WRITE_EVENT (remember if you don't type a command, ConsoleWriter assumes you want to write the text as an Event) or the WRITE_EVENT_RK will be written to the Transaction: Write Events to a Transaction 346d8561-3fd8-40b6-8c15-9343eeea2992 m1 **** Wrote m1 346d8561-3fd8-40b6-8c15-9343eeea2992 m2 **** Wrote m2 346d8561-3fd8-40b6-8c15-9343eeea2992 m3 **** Wrote m3 At this point, if you look at the Stream (by invoking the ConsoleReader app on the Stream, for example), you won't see those Events written to the Stream. Events not Written to the Stream (yet) $ bin/consoleReader ... ******** Reading events from examples/someStream But when a COMMIT command is given, causing the Transaction to commit: Do the Commit 346d8561-3fd8-40b6-8c15-9343eeea2992 commit **** Transaction commit completed. those Events are appended to the Stream and are now all available: After commit, the Events are Visible ******** Reading events from examples/someStream m1 m2 m3 More on Begin Transaction The Begin Transaction (beginTxn()) operation takes three parameters (ConsoleWriter chooses some reasonable defaults so in the CLI these are optional): Param Discussion transactionTimeout The amount of time a transaction should be allowed to run before it is automatically aborted by Pravega. This is also referred to as a \"lease\". maxExecutionTime The amount of time allowed between ping operations.","title":"Working with Transactions"},{"location":"transactions/#working-with-pravega-transactions","text":"This article explores how to write a set of Events to a Stream atomically using Pravega Transactions. Instructions for running the sample applications can be found in the Pravega Samples readme . You really should be familiar with Pravega Concepts (see Pravega Concepts ) before continuing reading this page.","title":"Working with Pravega: Transactions"},{"location":"transactions/#pravega-transactions-and-the-console-writer-and-console-reader-apps","text":"We have written a couple of applications, ConsoleReader and ConsoleWriter that help illustrate reading and writing data with Pravega and in particular to illustrate the Transaction facility in the Pravega programming model. You can find those applications here .","title":"Pravega Transactions and the Console Writer and Console Reader Apps"},{"location":"transactions/#consolereader","text":"The ConsoleReader app is very simple. It uses the Pravega Java Client Library to read from a Stream and output each Event onto the console. It runs indefinitely, so you have to kill the process to terminate the program.","title":"ConsoleReader"},{"location":"transactions/#consolewriter","text":"The ConsoleWriter app is a bit more sophisticated. It uses the Pravega Java Client Library to write Events to a Stream, including Events written in the context of a Pravega Transaction. To make manipulating Transactions a bit easier, we provide a console-based CLI. The help text for the CLI is shown below: ConsoleWriter Help text Enter one of the following commands at the command line prompt: If no command is entered, the line is treated as a parameter to the WRITE_EVENT command. WRITE_EVENT {event} - write the {event} out to the Stream or the current Transaction. WRITE_EVENT_RK {routingKey} , {event} - write the {event} out to the Stream or the current Transaction using {routingKey}. Note and around {routingKey}. BEGIN - begin a Transaction. Only one Transaction at a time is supported by the CLI. GET_TXN_ID - output the current Transaction s Id (if a Transaction is running) FLUSH - flush the current Transaction (if a Transaction is running) COMMIT - commit the Transaction (if a Transaction is running) ABORT - abort the Transaction (if a Transaction is running) STATUS - check the status of the Transaction(if a Transaction is running) HELP - print out a list of commands. QUIT - terminate the program. examples/someStream So writing a single Event is simple, just type some text (you don't even have to type the WRITE_EVENT command if you don't want to). But we really want to talk about Pravega Transactions, so lets dive into that.","title":"ConsoleWriter"},{"location":"transactions/#pravega-transactions","text":"The idea with a Pravega Transaction is that it allows an application to prepare a set of Events that can be written \"all at once\" to a Stream. This allows an application to \"commit\" a bunch of Events Atomically. This is done by writing them into the Transaction and calling commit to append them to the Stream. An application might want to do this in cases where it wants the Events to be durably stored and later decided whether or not those Events should be appended to the Stream. This allows the application to control when the set of Events are made visible to Readers. A Transaction is created via an EventStreamWriter. Recall that an EventStreamWriter itself is created through a ClientFactory and is constructed to operate against a Stream. Transactions are therefore bound to a Stream. Once a Transaction is created, it acts a lot like a Writer. Applications Write Events to the Transaction and once acknowledged, the data is considered durably persisted in the Transaction. Note that the data written to a Transaction will not be visible to Readers until the Transaction is committed. In addition to writeEvent and writeEvent using a routing key, there are several Transaction specific operations provided: Operation Discussion getTxnId() Retrieve the unique identifier for the Transaction. Pravega generates a unique UUID for each Transaction. flush() Ensure that all Writes have been persisted. ping() Extend the duration of a Transaction. Note that after a certain amount of idle time, the Transaction will automatically abort. This is to handle the case where the client has crashed and it is no longer appropriate to keep resources associated with the Transaction. checkStatus() Return the state of the Transaction. The Transaction can be in one of the following states: Open, Committing, Committed, Aborting or Aborted. commit() Append all of the Events written to the Transaction into the Stream. Either all of the Event data will be appended to the Stream or none of it will be. abort() Terminate the Transaction, the data written to the Transaction will be deleted.","title":"Pravega Transactions"},{"location":"transactions/#using-the-consolewriter-to-begin-and-commit-a-transaction","text":"All of the Transaction API is reflected in the ConsoleWriter's CLI command set. To begin a transaction, type BEGIN: Begin Transaction examples/someStream begin 346d8561-3fd8-40b6-8c15-9343eeea2992 When a Transaction is created, it returns a Transaction object parameterized to the type of Event supported by the Stream. In the case of the ConsoleWriter, the type of Event is a Java String. The command prompt changes to show the Transaction's id. Now any of the Transaction related commands can be issued (GET_TXN_ID, FLUSH, PING, COMMIT, ABORT and STATUS). Note that the BEGIN command won't work because the ConsoleWriter supports only one Transaction at a time (this is a limitation of the app, not a limitation of Pravega). When the ConsoleWriter is in a Transactional context, the WRITE_EVENT (remember if you don't type a command, ConsoleWriter assumes you want to write the text as an Event) or the WRITE_EVENT_RK will be written to the Transaction: Write Events to a Transaction 346d8561-3fd8-40b6-8c15-9343eeea2992 m1 **** Wrote m1 346d8561-3fd8-40b6-8c15-9343eeea2992 m2 **** Wrote m2 346d8561-3fd8-40b6-8c15-9343eeea2992 m3 **** Wrote m3 At this point, if you look at the Stream (by invoking the ConsoleReader app on the Stream, for example), you won't see those Events written to the Stream. Events not Written to the Stream (yet) $ bin/consoleReader ... ******** Reading events from examples/someStream But when a COMMIT command is given, causing the Transaction to commit: Do the Commit 346d8561-3fd8-40b6-8c15-9343eeea2992 commit **** Transaction commit completed. those Events are appended to the Stream and are now all available: After commit, the Events are Visible ******** Reading events from examples/someStream m1 m2 m3","title":"Using the ConsoleWriter to Begin and Commit a Transaction"},{"location":"transactions/#more-on-begin-transaction","text":"The Begin Transaction (beginTxn()) operation takes three parameters (ConsoleWriter chooses some reasonable defaults so in the CLI these are optional): Param Discussion transactionTimeout The amount of time a transaction should be allowed to run before it is automatically aborted by Pravega. This is also referred to as a \"lease\". maxExecutionTime The amount of time allowed between ping operations.","title":"More on Begin Transaction"},{"location":"wire-protocol/","text":"Streaming Service Wire Protocol This page describes the proposed Wire Protocol for the Streaming Service. See parent page for description of the service as a whole. Protocol Data is sent over the wire in self-contained \"messages\" that are either \"requests\" (messages from the client to the server) or \"replies\" (which are in response to a request and go back to the client). All requests and replies have an 8 byte header with two fields, (All data is written in BigEndian format): 1. Message Type - An integer (4 bytes) identifies the message type and this determines what fields will follow. (Note the protocol can be extended by adding new types) 2. Length - Unsigned integer 4 bytes (Messages should be 2^24, but the upper bits remain zero). How many bytes of data from this point forward are part of this message. (Possibly zero, indicating there is no data) The remainder of the fields are specific to the type of message. A few important messages are listed below. General Partial Message - Request/Reply Begin/Middle/End - Enum (1 byte) Data A partial message is one that was broken up when being sent over the wire. (For any reason). The whole message is reconstructed by reading the partial messages in sequence and assembling them into a whole. It is not valid to attempt to start a new partial message before completing the previous one. KeepAlive - Request/Reply Data - uninterpreted data of the length of the message. (Usually 0 bytes) Reading Read Segment - Request Segment to read - String (2 byte length, followed by that many bytes of Java's Modified UTF-8) Offset to read from - Long (8 bytes) Suggested Length of Reply - int (4 bytes) This is how much data the client wants. They won't necessarily get that much. Segment Read - Reply Segment that was read - String (2 byte length, followed by that many bytes of Java's Modified UTF-8) Offset that was read from - Long (8 bytes) Is at Tail - Boolean (1 bit) Is at EndOfSegment - (1 bit) Data - Binary (remaining length in message) The client requests to read from a particular stream at a particular offset, it then receives one or more replies in the form of SegmentRead messages. These contain the data they requested (assuming it exists). The server may decided to give the client more or less data than it asked for, in as many replies as it sees fit. Appending Setup Append - Request ConnectionId - UUID (16 bytes) Identifies this appender. Segment to append to. - String (2 byte length, followed by that many bytes of Java's Modified UTF-8) Append Setup - Reply Segment that can be appended to. - String (2 byte length, followed by that many bytes of Java's Modified UTF-8) ConnectionId - UUID (16 bytes) Identifies the requesting appender. ConnectionOffsetAckLevel - Long (8 bytes) What was the last offset received and stored on this segment for this connectionId (0 if new) BeginAppendBlock - Request Only valid after SetupAppend has already been done successfully. ConnectionId - UUID (16 bytes) ConnectionOffset - Long (8 bytes) Data written so far over this connection to this segment Length of data before EndAppendBlock message - Integer (4 bytes) EndAppendBlock- Request ConnectionId - UUID (16 bytes) ConnectionOffset - Long (8 bytes) Data written so far over this connection Block Length - (4 Bytes) Total size of the block that was written. (Note this may more or less than the number of bytes between the BeginAppendBlock and this message) Event - Request Only valid inside of a block Data Data Appended - Reply ConnectionId - UUID (16 bytes) ConnectionOffsetAckLevel - Long (8 bytes) The highest offset before which all data is successfully stored on this segment for this connectionId When appending a client Establishes a connection to what it thinks is the correct host. Sends a Setup Append request. Waits for the Append Setup reply. Then it can 1. Send a BeginEventBlock request 2. Send as many messages as can fit in the block 3. Send an EndEventBlock request While this is happening the server will be periodically sending it DataAppended replies acking messages. Note that there can be multiple \"Appends\" setup for a given TCP connection. This allows a client to share a connection when producing to multiple segments. A client can optimize its appending by specifying a large value in it's BeginAppendBlock message, as the events inside of the block do not need to be processed individually. The EndEventBlock message specifies the size of the append block rather than the BeginAppendBlock message. This means that the size of the data in the block need not be known in advance. This is useful if a client is producing a stream of small messages. It can begin a block, write many messages and then when it comes time to end the block, it can write a partial message followed the EndAppendBlock message, followed by the remaining partial message. This would avoid having headers on all of the messages in the block without having to buffer them in ram in its process.","title":"Wire Protocol"},{"location":"wire-protocol/#streaming-service-wire-protocol","text":"This page describes the proposed Wire Protocol for the Streaming Service. See parent page for description of the service as a whole.","title":"Streaming Service Wire Protocol"},{"location":"wire-protocol/#protocol","text":"Data is sent over the wire in self-contained \"messages\" that are either \"requests\" (messages from the client to the server) or \"replies\" (which are in response to a request and go back to the client). All requests and replies have an 8 byte header with two fields, (All data is written in BigEndian format): 1. Message Type - An integer (4 bytes) identifies the message type and this determines what fields will follow. (Note the protocol can be extended by adding new types) 2. Length - Unsigned integer 4 bytes (Messages should be 2^24, but the upper bits remain zero). How many bytes of data from this point forward are part of this message. (Possibly zero, indicating there is no data) The remainder of the fields are specific to the type of message. A few important messages are listed below.","title":"Protocol"},{"location":"wire-protocol/#general","text":"","title":"General"},{"location":"wire-protocol/#partial-message-requestreply","text":"Begin/Middle/End - Enum (1 byte) Data A partial message is one that was broken up when being sent over the wire. (For any reason). The whole message is reconstructed by reading the partial messages in sequence and assembling them into a whole. It is not valid to attempt to start a new partial message before completing the previous one.","title":"Partial Message - Request/Reply"},{"location":"wire-protocol/#keepalive-requestreply","text":"Data - uninterpreted data of the length of the message. (Usually 0 bytes)","title":"KeepAlive - Request/Reply"},{"location":"wire-protocol/#reading","text":"","title":"Reading"},{"location":"wire-protocol/#read-segment-request","text":"Segment to read - String (2 byte length, followed by that many bytes of Java's Modified UTF-8) Offset to read from - Long (8 bytes) Suggested Length of Reply - int (4 bytes) This is how much data the client wants. They won't necessarily get that much.","title":"Read Segment - Request"},{"location":"wire-protocol/#segment-read-reply","text":"Segment that was read - String (2 byte length, followed by that many bytes of Java's Modified UTF-8) Offset that was read from - Long (8 bytes) Is at Tail - Boolean (1 bit) Is at EndOfSegment - (1 bit) Data - Binary (remaining length in message) The client requests to read from a particular stream at a particular offset, it then receives one or more replies in the form of SegmentRead messages. These contain the data they requested (assuming it exists). The server may decided to give the client more or less data than it asked for, in as many replies as it sees fit.","title":"Segment Read - Reply"},{"location":"wire-protocol/#appending","text":"","title":"Appending"},{"location":"wire-protocol/#setup-append-request","text":"ConnectionId - UUID (16 bytes) Identifies this appender. Segment to append to. - String (2 byte length, followed by that many bytes of Java's Modified UTF-8)","title":"Setup Append - Request"},{"location":"wire-protocol/#append-setup-reply","text":"Segment that can be appended to. - String (2 byte length, followed by that many bytes of Java's Modified UTF-8) ConnectionId - UUID (16 bytes) Identifies the requesting appender. ConnectionOffsetAckLevel - Long (8 bytes) What was the last offset received and stored on this segment for this connectionId (0 if new)","title":"Append\u00a0Setup\u00a0- Reply"},{"location":"wire-protocol/#beginappendblock-request","text":"Only valid after SetupAppend has already been done successfully. ConnectionId - UUID (16 bytes) ConnectionOffset - Long (8 bytes) Data written so far over this connection to this segment Length of data before EndAppendBlock message - Integer (4 bytes)","title":"BeginAppendBlock - Request"},{"location":"wire-protocol/#endappendblock-request","text":"ConnectionId - UUID (16 bytes) ConnectionOffset - Long (8 bytes) Data written so far over this connection Block Length - (4 Bytes) Total size of the block that was written. (Note this may more or less than the number of bytes between the BeginAppendBlock and this message)","title":"EndAppendBlock- Request"},{"location":"wire-protocol/#event-request","text":"Only valid inside of a block Data","title":"Event - Request"},{"location":"wire-protocol/#data-appended-reply","text":"ConnectionId - UUID (16 bytes) ConnectionOffsetAckLevel - Long (8 bytes) The highest offset before which all data is successfully stored on this segment for this connectionId When appending a client Establishes a connection to what it thinks is the correct host. Sends a Setup Append request. Waits for the Append Setup reply. Then it can 1. Send a BeginEventBlock request 2. Send as many messages as can fit in the block 3. Send an EndEventBlock request While this is happening the server will be periodically sending it DataAppended replies acking messages. Note that there can be multiple \"Appends\" setup for a given TCP connection. This allows a client to share a connection when producing to multiple segments. A client can optimize its appending by specifying a large value in it's BeginAppendBlock message, as the events inside of the block do not need to be processed individually. The EndEventBlock message specifies the size of the append block rather than the BeginAppendBlock message. This means that the size of the data in the block need not be known in advance. This is useful if a client is producing a stream of small messages. It can begin a block, write many messages and then when it comes time to end the block, it can write a partial message followed the EndAppendBlock message, followed by the remaining partial message. This would avoid having headers on all of the messages in the block without having to buffer them in ram in its process.","title":"Data Appended - Reply"},{"location":"deployment/aws-install/","text":"Running on AWS Pre-reqs: Have an AWS account and have Terraform installed. To install and download Terraform, follow the instructions here: https://www.terraform.io/downloads.html Deploy Steps Run \"sudo terraform apply\" under the deployment/aws directory, and then follow prompt instruction, enter the AWS account credentials. There are four variables would be needed: AWS access key and AWS secret key, which can be obtained from AWS account cred_path, which is the absolute path of key pair file. It would be downloaded when key pair is created AWS region: Currently, we only support two regions: us-east-1 and us-west-1. We list below the instance types we recommend for them. Region us-east-1: Three m3.xlarge for EMR Three m3.2xlarge for Pravega One m3.medium for bootstrap, also as client Region us-west-1: Three m3.xlarge for EMR Three i3.4xlarge for Pravega One i3.xlarge for bootstrap, also as client Other instance types might present conflicts with the Linux Images used. How to customize the pravega cluster Change default value of \"pravega_num\" in variable.tf Define the your own nodes layout in installer/hosts-template, default hosts-template is under installer directory. There are three sections of hosts-template: 1. common-services is the section for zookeeper and bookkeeper 2. pravega-controller is the section for pravega controller node 3. pravega-hosts is the section for the pravega segment store node. How to destroy the pravega cluster Run \"sudo terraform destroy\", then enter \"yes\"","title":"Running in the Cloud (AWS)"},{"location":"deployment/aws-install/#running-on-aws","text":"Pre-reqs: Have an AWS account and have Terraform installed. To install and download Terraform, follow the instructions here: https://www.terraform.io/downloads.html","title":"Running on AWS"},{"location":"deployment/aws-install/#deploy-steps","text":"Run \"sudo terraform apply\" under the deployment/aws directory, and then follow prompt instruction, enter the AWS account credentials. There are four variables would be needed: AWS access key and AWS secret key, which can be obtained from AWS account cred_path, which is the absolute path of key pair file. It would be downloaded when key pair is created AWS region: Currently, we only support two regions: us-east-1 and us-west-1. We list below the instance types we recommend for them. Region us-east-1: Three m3.xlarge for EMR Three m3.2xlarge for Pravega One m3.medium for bootstrap, also as client Region us-west-1: Three m3.xlarge for EMR Three i3.4xlarge for Pravega One i3.xlarge for bootstrap, also as client Other instance types might present conflicts with the Linux Images used.","title":"Deploy Steps"},{"location":"deployment/aws-install/#how-to-customize-the-pravega-cluster","text":"Change default value of \"pravega_num\" in variable.tf Define the your own nodes layout in installer/hosts-template, default hosts-template is under installer directory. There are three sections of hosts-template: 1. common-services is the section for zookeeper and bookkeeper 2. pravega-controller is the section for pravega controller node 3. pravega-hosts is the section for the pravega segment store node.","title":"How to customize the pravega cluster"},{"location":"deployment/aws-install/#how-to-destroy-the-pravega-cluster","text":"Run \"sudo terraform destroy\", then enter \"yes\"","title":"How to destroy the pravega cluster"},{"location":"deployment/dcos-install/","text":"Deploying on DC/OS Prerequisities: DC/OS cli needs to be installed. To install the cli, follow the instructions here: https://docs.mesosphere.com/1.8/usage/cli/install/ Pravega can be run on DC/OS by leveraging Marathon. PravegaGroup.json defines the docker hub image locations and necessary application configration to start a simple Pravega cluster. Download PravegaGroup.json to your DC/OS cluster. For example: wget https://github.com/pravega/pravega/blob/master/PravegaGroup.json Add to Marathon using: dcos marathon group add PravegaGroup.json","title":"Deployment on DC/OS"},{"location":"deployment/dcos-install/#deploying-on-dcos","text":"Prerequisities: DC/OS cli needs to be installed. To install the cli, follow the instructions here: https://docs.mesosphere.com/1.8/usage/cli/install/ Pravega can be run on DC/OS by leveraging Marathon. PravegaGroup.json defines the docker hub image locations and necessary application configration to start a simple Pravega cluster. Download PravegaGroup.json to your DC/OS cluster. For example: wget https://github.com/pravega/pravega/blob/master/PravegaGroup.json Add to Marathon using: dcos marathon group add PravegaGroup.json","title":"Deploying on DC/OS"},{"location":"deployment/deployment/","text":"Pravega Deployment Overview This guide describes the options for running Pravega for development, testing and in production. Pravega Modes There are two modes for running Pravega. Standalone - Standalone mode is suitable for development and testing Pravega applications. It can either be run from the source code, from the distribution package or as a docker container. Distributed - Distributed mode runs each component separately on a single or multiple nodes. This is suitable for production in addition for development and testing. The deployment options in this mode include a manual installation, running in a docker swarm or DC/OS. Prerequisites The following prerequisites are required for running pravega in all modes. Java 8 The following prerequisites are required for running in production. These are only required for running in distributed mode. External HDFS 2.7 Zookeeper 3.5.1-alpha Bookkeeper 4.4.0 For more details on the prerequisites and recommended configuration options for bookkeeper see the Manual Install Guide . Installation There are multiple options provided for running Pravega in different environments. Most of these use the installation package from a Pravega release. You can find the latest Pravega release on the github releases page . Local - Running Pravega locally is suitable for development and testing. Running from source Local Standalone Mode Docker Compose (Distributed Mode) Production - Multi-node installation suitable for running in production. Manual Installation Docker Swarm DC/OS Cloud - AWS","title":"Deployment Overview"},{"location":"deployment/deployment/#pravega-deployment-overview","text":"This guide describes the options for running Pravega for development, testing and in production.","title":"Pravega Deployment Overview"},{"location":"deployment/deployment/#pravega-modes","text":"There are two modes for running Pravega. Standalone - Standalone mode is suitable for development and testing Pravega applications. It can either be run from the source code, from the distribution package or as a docker container. Distributed - Distributed mode runs each component separately on a single or multiple nodes. This is suitable for production in addition for development and testing. The deployment options in this mode include a manual installation, running in a docker swarm or DC/OS.","title":"Pravega Modes"},{"location":"deployment/deployment/#prerequisites","text":"The following prerequisites are required for running pravega in all modes. Java 8 The following prerequisites are required for running in production. These are only required for running in distributed mode. External HDFS 2.7 Zookeeper 3.5.1-alpha Bookkeeper 4.4.0 For more details on the prerequisites and recommended configuration options for bookkeeper see the Manual Install Guide .","title":"Prerequisites"},{"location":"deployment/deployment/#installation","text":"There are multiple options provided for running Pravega in different environments. Most of these use the installation package from a Pravega release. You can find the latest Pravega release on the github releases page . Local - Running Pravega locally is suitable for development and testing. Running from source Local Standalone Mode Docker Compose (Distributed Mode) Production - Multi-node installation suitable for running in production. Manual Installation Docker Swarm DC/OS Cloud - AWS","title":"Installation"},{"location":"deployment/docker-swarm/","text":"Deploying in a Docker Swarm Docker Swarm can be used to quickly spin up a distributed Pravega cluster that can easily scale up and down. Unlike docker-compose , this is useful for more than just testing and development, and in the future will be suitable for production workloads. Prerequisites A working single or multi-node Docker Swarm. See https://docs.docker.com/engine/swarm/swarm-tutorial . HDFS and ZooKeeper. We provide compose files for both of these, but both are single instance deploys that should only be used for testing/development. To deploy our HDFS and ZooKeeper: docker stack up --compose-file hdfs.yml pravega docker stack up --compose-file zookeeper.yml pravega This runs a single node HDFS container and single node ZooKeeper inside the pravega_default overlay network, and adds them to the pravega stack. HDFS is reachable inside the swarm as hdfs://hdfs:8020 , and ZooKeeper at tcp://zookeeper:2181 . You may run one or both of these to get up and running, but these shouldn't be used for serious workloads. Network Considerations Each Pravega Segment Store needs to be directly reachable by clients. Docker Swarm runs all traffic coming into its overlay network through a load balancer, which makes it more or less impossible to reach a specific instance of a scaled service from outside the cluster. This means that Pravega clients must either run inside the swarm, or we must run each Segment Store as a unique service on a distinct port. Both approaches will be demonstrated. Deploying (swarm only clients) The easiest way to deploy is to keep all traffic inside the swarm. This means your client apps must also run inside the swarm. ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega Note that ZK_URL and HDFS_URL don't include the protocol. They default to zookeeper:2181 and hdfs:8020 , so you can omit them if they're reachable at those addresses (which they will be if you've deployed zookeeper.yml / hdfs.yml ). Your clients must then be deployed into the swarm, with something like: docker service create --name=myapp --network=pravega_default mycompany/myapp The crucial bit being --network=pravega_default . Your client should talk to Pravega at tcp://controller:9090 . Deploying (external clients) If you intend to run clients outside the swarm, you must provide two additional environment variables, PUBLISHED_ADDRESS and LISTENING_ADDRESS . PUBLISHED_ADDRESS must be an IP or hostname that resolves to one or more swarm nodes (or a load balancer that sits in front of them). LISTENING_ADDRESS should always be 0 , or 0.0.0.0 . PUBLISHED_ADDRESS=1.2.3.4 LISTENING_ADDRESS=0 ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega As above, ZK_URL and HDFS_URL can be omitted if the services are at their default locations. Your client should talk to Pravega at tcp://${PUBLISHED_ADDRESS}:9090 . Scaling BookKeeper BookKeeper can be scaled up or down with: docker service scale pravega_bookkeeper=N As configured in this package, Pravega requires at least 3 BookKeeper nodes, so N must be = 3. Scaling Pravega Controller Pravega Controller can be scaled up or down with: docker service scale pravega_controller=N Scaling Pravega Segment Store (swarm only clients) If you app will run inside the swarm and you didn't run with PUBLISHED_ADDRESS , you can scale the Segment Store the usual way: docker service scale pravega_segmentstore=N Scaling Pravega Segment Store (external clients) If you require access to Pravega from outside the swarm and have deployed with PUBLISHED_ADDRESS , each instance of the Segment Store must be deployed as a unique service. This is a cumbersome process, but we've provided a helper script to make it fairly painless: ./scale_segmentstore N Tearing down All services (including HDFS and ZooKeeper if you've deployed our package) can be destroyed with: docker stack down pravega","title":"Deployment in Docker Swarm"},{"location":"deployment/docker-swarm/#deploying-in-a-docker-swarm","text":"Docker Swarm can be used to quickly spin up a distributed Pravega cluster that can easily scale up and down. Unlike docker-compose , this is useful for more than just testing and development, and in the future will be suitable for production workloads.","title":"Deploying in a Docker Swarm"},{"location":"deployment/docker-swarm/#prerequisites","text":"A working single or multi-node Docker Swarm. See https://docs.docker.com/engine/swarm/swarm-tutorial . HDFS and ZooKeeper. We provide compose files for both of these, but both are single instance deploys that should only be used for testing/development. To deploy our HDFS and ZooKeeper: docker stack up --compose-file hdfs.yml pravega docker stack up --compose-file zookeeper.yml pravega This runs a single node HDFS container and single node ZooKeeper inside the pravega_default overlay network, and adds them to the pravega stack. HDFS is reachable inside the swarm as hdfs://hdfs:8020 , and ZooKeeper at tcp://zookeeper:2181 . You may run one or both of these to get up and running, but these shouldn't be used for serious workloads.","title":"Prerequisites"},{"location":"deployment/docker-swarm/#network-considerations","text":"Each Pravega Segment Store needs to be directly reachable by clients. Docker Swarm runs all traffic coming into its overlay network through a load balancer, which makes it more or less impossible to reach a specific instance of a scaled service from outside the cluster. This means that Pravega clients must either run inside the swarm, or we must run each Segment Store as a unique service on a distinct port. Both approaches will be demonstrated.","title":"Network Considerations"},{"location":"deployment/docker-swarm/#deploying-swarm-only-clients","text":"The easiest way to deploy is to keep all traffic inside the swarm. This means your client apps must also run inside the swarm. ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega Note that ZK_URL and HDFS_URL don't include the protocol. They default to zookeeper:2181 and hdfs:8020 , so you can omit them if they're reachable at those addresses (which they will be if you've deployed zookeeper.yml / hdfs.yml ). Your clients must then be deployed into the swarm, with something like: docker service create --name=myapp --network=pravega_default mycompany/myapp The crucial bit being --network=pravega_default . Your client should talk to Pravega at tcp://controller:9090 .","title":"Deploying (swarm only clients)"},{"location":"deployment/docker-swarm/#deploying-external-clients","text":"If you intend to run clients outside the swarm, you must provide two additional environment variables, PUBLISHED_ADDRESS and LISTENING_ADDRESS . PUBLISHED_ADDRESS must be an IP or hostname that resolves to one or more swarm nodes (or a load balancer that sits in front of them). LISTENING_ADDRESS should always be 0 , or 0.0.0.0 . PUBLISHED_ADDRESS=1.2.3.4 LISTENING_ADDRESS=0 ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega As above, ZK_URL and HDFS_URL can be omitted if the services are at their default locations. Your client should talk to Pravega at tcp://${PUBLISHED_ADDRESS}:9090 .","title":"Deploying (external clients)"},{"location":"deployment/docker-swarm/#scaling-bookkeeper","text":"BookKeeper can be scaled up or down with: docker service scale pravega_bookkeeper=N As configured in this package, Pravega requires at least 3 BookKeeper nodes, so N must be = 3.","title":"Scaling BookKeeper"},{"location":"deployment/docker-swarm/#scaling-pravega-controller","text":"Pravega Controller can be scaled up or down with: docker service scale pravega_controller=N","title":"Scaling Pravega Controller"},{"location":"deployment/docker-swarm/#scaling-pravega-segment-store-swarm-only-clients","text":"If you app will run inside the swarm and you didn't run with PUBLISHED_ADDRESS , you can scale the Segment Store the usual way: docker service scale pravega_segmentstore=N","title":"Scaling Pravega Segment Store (swarm only clients)"},{"location":"deployment/docker-swarm/#scaling-pravega-segment-store-external-clients","text":"If you require access to Pravega from outside the swarm and have deployed with PUBLISHED_ADDRESS , each instance of the Segment Store must be deployed as a unique service. This is a cumbersome process, but we've provided a helper script to make it fairly painless: ./scale_segmentstore N","title":"Scaling Pravega Segment Store (external clients)"},{"location":"deployment/docker-swarm/#tearing-down","text":"All services (including HDFS and ZooKeeper if you've deployed our package) can be destroyed with: docker stack down pravega","title":"Tearing down"},{"location":"deployment/manual-install/","text":"Manual Installation This page describes the prerequisites and installation steps to deploy Pravega in a multi-node production environment. Prerequisites HDFS Setup an HDFS storage cluster running HDFS version 2.7+ . HDFS is used as Tier 2 storage and must have sufficient capacity to store contents of all streams. The storage cluster is recommended to be run alongside Pravega on separate nodes. Java Install the latest Java 8 from java.oracle.com . Packages are available for all major operating systems. Zookeeper Pravega requires Zookeeper 3.5.1-alpha+ . At least 3 Zookeeper nodes are recommended for a quorum. No special configuration is required for Zookeeper but it is recommended to use a dedicated cluster for Pravega. This specific version of Zookeeper can be downloaded from Apache at zookeeper-3.5.1-alpha.tar.gz . For installing Zookeeper see the Getting Started Guide . Bookkeeper Pravega requires Bookkeeper 4.4.0+ . At least 3 Bookkeeper servers are recommended for a quorum. This specific version of Bookkeeper can be downloaded from Apache at bookkeeper-server-4.4.0-bin.tar.gz . For installing Bookkeeper see the Getting Started Guide . Some specific Pravega instructions are shown below. All sets assuming being run from the bookkeeper-server-4.4.0 directory. Bookkeeper Configuration The following configuration options should be changed in the conf/bk_server.conf file. # Comma separated list of zp-ip : port for all ZK servers zkServers=localhost:2181 # Alternatively specify a different path to the storage for /bk journalDirectory=/bk/journal ledgerDirectories=/bk/ledgers indexDirectories=/bk/index zkLedgersRootPath=/pravega/bookkeeper/ledgers Initializing Zookeeper paths The following paths need to be created in Zookeeper. From the zookeeper-3.5.1-alpha directory on the Zookeeper servers run: bin/zkCli.sh -server $ZK_URL create /pravega bin/zkCli.sh -server $ZK_URL create /pravega/bookkeeper Replace $ZK_URL with the IP address of the Zookeeper nodes Running Bookkeeper Before starting the bookie, it needs to be formatted: bin/bookkeeper shell metaformat -nonInteractive Start the bookie: bin/bookkeeper bookie Installing Pravega For non-production systems, you can use the containers provided by the docker installation to run non-production HDFS, Zookeeper or Bookkeeper. There are two key components of Pravega that need to be run: - Controller - Control plane for Pravega. Installation requires at least one controller. Two or more are recommended for HA. - Segment Store - Storage node for Pravega. Installation requires at least one segment store. Before you start, you need to download the latest Pravega release. You can find the latest Pravega release on the github releases page . Recommendations If you are getting started with a simple 3 node cluster, you may want to layout your services like this: Node 1 Node 2 Node 3 Zookeeper X X X Bookkeeper X X X Pravega Controller X X Pravega Segment Store X X X All Nodes On each node extract the distribution package to your desired directory: tar xfvz pravega-0.1.0.tgz cd pravega-0.1.0 Installing the Controller The controller can simply be run using the following command. Replace zk-ip with the IP address of the Zookeeper nodes ZK_URL= zk-ip :2181 bin/pravega-controller Alternatively, instead of specifying this on startup each time, you can edit the conf/controller.conf file and change the zk url there: zk { url = zk-ip :2181 ... } Then you can run the controller with: bin/pravega-controller Installing the Segment Store Edit the conf/config.properties file. The following properies need to be changed. Replace zk-ip , controller-ip and hdfs-ip with the IPs of the respective services: pravegaservice.zkURL= zk-ip :2181 bookkeeper.zkAddress= zk-ip :2181 autoScale.controllerUri=tcp:// controller-ip :9090 # Settings required for HDFS hdfs.hdfsUrl= hdfs-ip :8020 Once the configuration changes have been made you can start the segment store with: bin/pravega-segmentstore","title":"Manual Install"},{"location":"deployment/manual-install/#manual-installation","text":"This page describes the prerequisites and installation steps to deploy Pravega in a multi-node production environment.","title":"Manual Installation"},{"location":"deployment/manual-install/#prerequisites","text":"","title":"Prerequisites"},{"location":"deployment/manual-install/#hdfs","text":"Setup an HDFS storage cluster running HDFS version 2.7+ . HDFS is used as Tier 2 storage and must have sufficient capacity to store contents of all streams. The storage cluster is recommended to be run alongside Pravega on separate nodes.","title":"HDFS"},{"location":"deployment/manual-install/#java","text":"Install the latest Java 8 from java.oracle.com . Packages are available for all major operating systems.","title":"Java"},{"location":"deployment/manual-install/#zookeeper","text":"Pravega requires Zookeeper 3.5.1-alpha+ . At least 3 Zookeeper nodes are recommended for a quorum. No special configuration is required for Zookeeper but it is recommended to use a dedicated cluster for Pravega. This specific version of Zookeeper can be downloaded from Apache at zookeeper-3.5.1-alpha.tar.gz . For installing Zookeeper see the Getting Started Guide .","title":"Zookeeper"},{"location":"deployment/manual-install/#bookkeeper","text":"Pravega requires Bookkeeper 4.4.0+ . At least 3 Bookkeeper servers are recommended for a quorum. This specific version of Bookkeeper can be downloaded from Apache at bookkeeper-server-4.4.0-bin.tar.gz . For installing Bookkeeper see the Getting Started Guide . Some specific Pravega instructions are shown below. All sets assuming being run from the bookkeeper-server-4.4.0 directory.","title":"Bookkeeper"},{"location":"deployment/manual-install/#bookkeeper-configuration","text":"The following configuration options should be changed in the conf/bk_server.conf file. # Comma separated list of zp-ip : port for all ZK servers zkServers=localhost:2181 # Alternatively specify a different path to the storage for /bk journalDirectory=/bk/journal ledgerDirectories=/bk/ledgers indexDirectories=/bk/index zkLedgersRootPath=/pravega/bookkeeper/ledgers","title":"Bookkeeper Configuration"},{"location":"deployment/manual-install/#initializing-zookeeper-paths","text":"The following paths need to be created in Zookeeper. From the zookeeper-3.5.1-alpha directory on the Zookeeper servers run: bin/zkCli.sh -server $ZK_URL create /pravega bin/zkCli.sh -server $ZK_URL create /pravega/bookkeeper Replace $ZK_URL with the IP address of the Zookeeper nodes","title":"Initializing Zookeeper paths"},{"location":"deployment/manual-install/#running-bookkeeper","text":"Before starting the bookie, it needs to be formatted: bin/bookkeeper shell metaformat -nonInteractive Start the bookie: bin/bookkeeper bookie","title":"Running Bookkeeper"},{"location":"deployment/manual-install/#installing-pravega","text":"For non-production systems, you can use the containers provided by the docker installation to run non-production HDFS, Zookeeper or Bookkeeper. There are two key components of Pravega that need to be run: - Controller - Control plane for Pravega. Installation requires at least one controller. Two or more are recommended for HA. - Segment Store - Storage node for Pravega. Installation requires at least one segment store. Before you start, you need to download the latest Pravega release. You can find the latest Pravega release on the github releases page .","title":"Installing Pravega"},{"location":"deployment/manual-install/#recommendations","text":"If you are getting started with a simple 3 node cluster, you may want to layout your services like this: Node 1 Node 2 Node 3 Zookeeper X X X Bookkeeper X X X Pravega Controller X X Pravega Segment Store X X X","title":"Recommendations"},{"location":"deployment/manual-install/#all-nodes","text":"On each node extract the distribution package to your desired directory: tar xfvz pravega-0.1.0.tgz cd pravega-0.1.0","title":"All Nodes"},{"location":"deployment/manual-install/#installing-the-controller","text":"The controller can simply be run using the following command. Replace zk-ip with the IP address of the Zookeeper nodes ZK_URL= zk-ip :2181 bin/pravega-controller Alternatively, instead of specifying this on startup each time, you can edit the conf/controller.conf file and change the zk url there: zk { url = zk-ip :2181 ... } Then you can run the controller with: bin/pravega-controller","title":"Installing the Controller"},{"location":"deployment/manual-install/#installing-the-segment-store","text":"Edit the conf/config.properties file. The following properies need to be changed. Replace zk-ip , controller-ip and hdfs-ip with the IPs of the respective services: pravegaservice.zkURL= zk-ip :2181 bookkeeper.zkAddress= zk-ip :2181 autoScale.controllerUri=tcp:// controller-ip :9090 # Settings required for HDFS hdfs.hdfsUrl= hdfs-ip :8020 Once the configuration changes have been made you can start the segment store with: bin/pravega-segmentstore","title":"Installing the Segment Store"},{"location":"deployment/run-local/","text":"Running Pravega Locally Running locally allows you to get started using pravega very quickly. Most of the options use the standalone mode which is suitable for most development and testing. All of the options for running locally start the required prerequisites so you can get started right away. Standalone Mode From Source First you need to have the Pravega source code checked out: git clone https://github.com/pravega/pravega.git cd pravega This one command will download dependencies, compile pravega and start the standalone deployment. ./gradlew startStandalone From Installation Package Download the Pravega release from the github releases page . The tarball and zip are identical. Instructions are provided using tar but the same steps work with the zip file. tar xfvz pravega-0.1.0.tgz Run pravega standalone: pravega-0.1.0/bin/pravega-standalone From Docker Container This will download and run Pravega from the container image on docker hub. Note, you must replace the ip with the IP of your machine. This is so that you can connect to Pravega from your local system. Optionally you can replace latest with the version of Pravega your desire docker run -it -e HOST_IP= ip -p 9090:9090 -p 12345:12345 pravega/pravega:latest standalone Docker Compose (Distributed Mode) Unlike other options for running locally, the docker compose option runs a full Pravega install in distributed mode. It contains containers for running Zookeeper, Bookkeeper and HDFS so Pravega operates as if it would in production. This is as easy to get started with as the standalone option but requires additional resources available. To use this you need to have Docker 1.12 or later. Download the docker-compose.yml from github. For example: wget https://github.com/pravega/pravega/tree/master/docker/compose/docker-compose.yml You need to set the IP address of your local machine as the value of HOST_IP in the following command. To run: HOST_IP=1.2.3.4 docker-compose up Clients can then connect to the controller at ${HOST_IP}:9090 .","title":"Running Locally"},{"location":"deployment/run-local/#running-pravega-locally","text":"Running locally allows you to get started using pravega very quickly. Most of the options use the standalone mode which is suitable for most development and testing. All of the options for running locally start the required prerequisites so you can get started right away.","title":"Running Pravega Locally"},{"location":"deployment/run-local/#standalone-mode","text":"","title":"Standalone Mode"},{"location":"deployment/run-local/#from-source","text":"First you need to have the Pravega source code checked out: git clone https://github.com/pravega/pravega.git cd pravega This one command will download dependencies, compile pravega and start the standalone deployment. ./gradlew startStandalone","title":"From Source"},{"location":"deployment/run-local/#from-installation-package","text":"Download the Pravega release from the github releases page . The tarball and zip are identical. Instructions are provided using tar but the same steps work with the zip file. tar xfvz pravega-0.1.0.tgz Run pravega standalone: pravega-0.1.0/bin/pravega-standalone","title":"From Installation Package"},{"location":"deployment/run-local/#from-docker-container","text":"This will download and run Pravega from the container image on docker hub. Note, you must replace the ip with the IP of your machine. This is so that you can connect to Pravega from your local system. Optionally you can replace latest with the version of Pravega your desire docker run -it -e HOST_IP= ip -p 9090:9090 -p 12345:12345 pravega/pravega:latest standalone","title":"From Docker Container"},{"location":"deployment/run-local/#docker-compose-distributed-mode","text":"Unlike other options for running locally, the docker compose option runs a full Pravega install in distributed mode. It contains containers for running Zookeeper, Bookkeeper and HDFS so Pravega operates as if it would in production. This is as easy to get started with as the standalone option but requires additional resources available. To use this you need to have Docker 1.12 or later. Download the docker-compose.yml from github. For example: wget https://github.com/pravega/pravega/tree/master/docker/compose/docker-compose.yml You need to set the IP address of your local machine as the value of HOST_IP in the following command. To run: HOST_IP=1.2.3.4 docker-compose up Clients can then connect to the controller at ${HOST_IP}:9090 .","title":"Docker Compose (Distributed Mode)"},{"location":"rest/restapis/","text":"Pravega Controller APIs Overview List of admin REST APIs for the pravega controller service. Version information Version : 0.0.1 License information License : Apache 2.0 License URL : http://www.apache.org/licenses/LICENSE-2.0 Terms of service : null URI scheme BasePath : /v1 Schemes : HTTP Tags ReaderGroups : Reader group related APIs Scopes : Scope related APIs Streams : Stream related APIs Paths POST /scopes Description Create a new scope Parameters Type Name Description Schema Body CreateScopeRequest required The scope configuration CreateScopeRequest CreateScopeRequest Name Description Schema scopeName optional Example : string string Responses HTTP Code Description Schema 201 Successfully created the scope ScopeProperty 409 Scope with the given name already exists No Content 500 Internal server error while creating a scope No Content Consumes application/json Produces application/json Tags Scopes Example HTTP request Request path /scopes Request body json : { scopeName : string } Example HTTP response Response 201 json : { scopeName : string } GET /scopes Description List all available scopes in pravega Responses HTTP Code Description Schema 200 List of currently available scopes ScopesList 500 Internal server error while fetching list of scopes No Content Produces application/json Tags Scopes Example HTTP request Request path /scopes Example HTTP response Response 200 json : { scopes : [ { scopeName : string } ] } GET /scopes/{scopeName} Description Retrieve details of an existing scope Parameters Type Name Description Schema Path scopeName required Scope name string Responses HTTP Code Description Schema 200 Successfully retrieved the scope details ScopeProperty 404 Scope with the given name not found No Content 500 Internal server error while fetching scope details No Content Produces application/json Tags Scopes Example HTTP request Request path /scopes/string Example HTTP response Response 200 json : { scopeName : string } DELETE /scopes/{scopeName} Description Delete a scope Parameters Type Name Description Schema Path scopeName required Scope name string Responses HTTP Code Description Schema 204 Successfully deleted the scope No Content 404 Scope not found No Content 412 Cannot delete scope since it has non-empty list of streams No Content 500 Internal server error while deleting a scope No Content Tags Scopes Example HTTP request Request path /scopes/string GET /scopes/{scopeName}/readergroups Description List reader groups within the given scope Parameters Type Name Description Schema Path scopeName required Scope name string Responses HTTP Code Description Schema 200 List of all reader groups configured for the given scope ReaderGroupsList 404 Scope not found No Content 500 Internal server error while fetching the list of reader groups for the given scope No Content Produces application/json Tags ReaderGroups Example HTTP request Request path /scopes/string/readergroups Example HTTP response Response 200 json : { readerGroups : [ object ] } GET /scopes/{scopeName}/readergroups/{readerGroupName} Description Fetch the properties of an existing reader group Parameters Type Name Description Schema Path readerGroupName required Reader group name string Path scopeName required Scope name string Responses HTTP Code Description Schema 200 Found reader group properties ReaderGroupProperty 404 Scope or reader group with given name not found No Content 500 Internal server error while fetching reader group details No Content Produces application/json Tags ReaderGroups Example HTTP request Request path /scopes/string/readergroups/string Example HTTP response Response 200 json : { scopeName : string , readerGroupName : string , streamList : [ string ], onlineReaderIds : [ string ] } POST /scopes/{scopeName}/streams Description Create a new stream Parameters Type Name Description Schema Path scopeName required Scope name string Body CreateStreamRequest required The stream configuration CreateStreamRequest CreateStreamRequest Name Description Schema retentionPolicy optional Example : [retentionconfig](#retentionconfig) RetentionConfig scalingPolicy optional Example : [scalingconfig](#scalingconfig) ScalingConfig streamName optional Example : string string Responses HTTP Code Description Schema 201 Successfully created the stream with the given configuration StreamProperty 404 Scope not found No Content 409 Stream with given name already exists No Content 500 Internal server error while creating a stream No Content Consumes application/json Produces application/json Tags Streams Example HTTP request Request path /scopes/string/streams Request body json : { streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } } Example HTTP response Response 201 json : { scopeName : string , streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } } GET /scopes/{scopeName}/streams Description List streams within the given scope Parameters Type Name Description Schema Path scopeName required Scope name string Query showInternalStreams optional Optional flag whether to display system created streams. If not specified only user created streams will be returned string Responses HTTP Code Description Schema 200 List of all streams configured for the given scope StreamsList 404 Scope not found No Content 500 Internal server error while fetching the list of streams for the given scope No Content Produces application/json Tags Streams Example HTTP request Request path /scopes/string/streams Request query json : { showInternalStreams : string } Example HTTP response Response 200 json : { streams : [ { scopeName : string , streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } } ] } GET /scopes/{scopeName}/streams/{streamName} Description Fetch the properties of an existing stream Parameters Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Responses HTTP Code Description Schema 200 Found stream properties StreamProperty 404 Scope or stream with given name not found No Content 500 Internal server error while fetching stream details No Content Produces application/json Tags Streams Example HTTP request Request path /scopes/string/streams/string Example HTTP response Response 200 json : { scopeName : string , streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } } PUT /scopes/{scopeName}/streams/{streamName} Description Update configuration of an existing stream Parameters Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Body UpdateStreamRequest required The new stream configuration UpdateStreamRequest UpdateStreamRequest Name Description Schema retentionPolicy optional Example : [retentionconfig](#retentionconfig) RetentionConfig scalingPolicy optional Example : [scalingconfig](#scalingconfig) ScalingConfig Responses HTTP Code Description Schema 200 Successfully updated the stream configuration StreamProperty 404 Scope or stream with given name not found No Content 500 Internal server error while updating the stream No Content Consumes application/json Produces application/json Tags Streams Example HTTP request Request path /scopes/string/streams/string Request body json : { scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } } Example HTTP response Response 200 json : { scopeName : string , streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } } DELETE /scopes/{scopeName}/streams/{streamName} Description Delete a stream Parameters Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Responses HTTP Code Description Schema 204 Successfully deleted the stream No Content 404 Stream not found No Content 412 Cannot delete stream since it is not sealed No Content 500 Internal server error while deleting the stream No Content Tags Streams Example HTTP request Request path /scopes/string/streams/string GET /scopes/{scopeName}/streams/{streamName}/scaling-events Description Get scaling events for a given datetime period. Parameters Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Query from required Parameter to display scaling events from that particular datetime. Input should be milliseconds from Jan 1 1970. integer (int64) Query to required Parameter to display scaling events to that particular datetime. Input should be milliseconds from Jan 1 1970. integer (int64) Responses HTTP Code Description Schema 200 Successfully fetched list of scaling events. ScalingEventList 404 Scope/Stream not found. No Content 500 Internal Server error while fetching scaling events. No Content Produces application/json Tags Streams Example HTTP request Request path /scopes/string/streams/string/scaling-events Request query json : { from : 0 , to : 0 } Example HTTP response Response 200 json : { scalingEvents : [ { timestamp : 0 , segmentList : [ { number : 0 , startTime : 0 , keyStart : 0 , keyEnd : 0 } ] } ] } PUT /scopes/{scopeName}/streams/{streamName}/state Description Updates the current state of the stream Parameters Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Body UpdateStreamStateRequest required The state info to be updated StreamState Responses HTTP Code Description Schema 200 Successfully updated the stream state StreamState 404 Scope or stream with given name not found No Content 500 Internal server error while updating the stream state No Content Consumes application/json Produces application/json Tags Streams Example HTTP request Request path /scopes/string/streams/string/state Request body json : { streamState : string } Example HTTP response Response 200 json : { streamState : string } Definitions ReaderGroupProperty Name Description Schema onlineReaderIds optional Example : [ string ] string array readerGroupName optional Example : string string scopeName optional Example : string string streamList optional Example : [ string ] string array ReaderGroupsList Name Description Schema readerGroups optional Example : [ object ] readerGroups array readerGroups Name Description Schema readerGroupName optional Example : string string RetentionConfig Name Description Schema type optional Example : string enum (LIMITED_DAYS, LIMITED_SIZE_MB) value optional Example : 0 integer (int64) ScaleMetadata Name Description Schema segmentList optional Example : [ [segment](#segment) ] Segment array timestamp optional Example : 0 integer (int64) ScalingConfig Name Description Schema minSegments optional Example : 0 integer (int32) scaleFactor optional Example : 0 integer (int32) targetRate optional Example : 0 integer (int32) type optional Example : string enum (FIXED_NUM_SEGMENTS, BY_RATE_IN_KBYTES_PER_SEC, BY_RATE_IN_EVENTS_PER_SEC) ScalingEventList Name Description Schema scalingEvents optional Example : [ [scalemetadata](#scalemetadata) ] ScaleMetadata array ScopeProperty Name Description Schema scopeName optional Example : string string ScopesList Name Description Schema scopes optional Example : [ [scopeproperty](#scopeproperty) ] ScopeProperty array Segment Name Description Schema keyEnd optional Example : 0 integer (double) keyStart optional Example : 0 integer (double) number optional Example : 0 integer (int32) startTime optional Example : 0 integer (int64) StreamProperty Name Description Schema retentionPolicy optional Example : [retentionconfig](#retentionconfig) RetentionConfig scalingPolicy optional Example : [scalingconfig](#scalingconfig) ScalingConfig scopeName optional Example : string string streamName optional Example : string string StreamState Name Description Schema streamState optional Example : string enum (SEALED) StreamsList Name Description Schema streams optional Example : [ [streamproperty](#streamproperty) ] StreamProperty array","title":"REST API - Controller"},{"location":"rest/restapis/#pravega-controller-apis","text":"","title":"Pravega Controller APIs"},{"location":"rest/restapis/#overview","text":"List of admin REST APIs for the pravega controller service.","title":"Overview"},{"location":"rest/restapis/#version-information","text":"Version : 0.0.1","title":"Version information"},{"location":"rest/restapis/#license-information","text":"License : Apache 2.0 License URL : http://www.apache.org/licenses/LICENSE-2.0 Terms of service : null","title":"License information"},{"location":"rest/restapis/#uri-scheme","text":"BasePath : /v1 Schemes : HTTP","title":"URI scheme"},{"location":"rest/restapis/#tags","text":"ReaderGroups : Reader group related APIs Scopes : Scope related APIs Streams : Stream related APIs","title":"Tags"},{"location":"rest/restapis/#paths","text":"","title":"Paths"},{"location":"rest/restapis/#post-scopes","text":"","title":"POST /scopes"},{"location":"rest/restapis/#description","text":"Create a new scope","title":"Description"},{"location":"rest/restapis/#parameters","text":"Type Name Description Schema Body CreateScopeRequest required The scope configuration CreateScopeRequest CreateScopeRequest Name Description Schema scopeName optional Example : string string","title":"Parameters"},{"location":"rest/restapis/#responses","text":"HTTP Code Description Schema 201 Successfully created the scope ScopeProperty 409 Scope with the given name already exists No Content 500 Internal server error while creating a scope No Content","title":"Responses"},{"location":"rest/restapis/#consumes","text":"application/json","title":"Consumes"},{"location":"rest/restapis/#produces","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_1","text":"Scopes","title":"Tags"},{"location":"rest/restapis/#example-http-request","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path","text":"/scopes","title":"Request path"},{"location":"rest/restapis/#request-body","text":"json : { scopeName : string }","title":"Request body"},{"location":"rest/restapis/#example-http-response","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-201","text":"json : { scopeName : string }","title":"Response 201"},{"location":"rest/restapis/#get-scopes","text":"","title":"GET /scopes"},{"location":"rest/restapis/#description_1","text":"List all available scopes in pravega","title":"Description"},{"location":"rest/restapis/#responses_1","text":"HTTP Code Description Schema 200 List of currently available scopes ScopesList 500 Internal server error while fetching list of scopes No Content","title":"Responses"},{"location":"rest/restapis/#produces_1","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_2","text":"Scopes","title":"Tags"},{"location":"rest/restapis/#example-http-request_1","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_1","text":"/scopes","title":"Request path"},{"location":"rest/restapis/#example-http-response_1","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200","text":"json : { scopes : [ { scopeName : string } ] }","title":"Response 200"},{"location":"rest/restapis/#get-scopesscopename","text":"","title":"GET /scopes/{scopeName}"},{"location":"rest/restapis/#description_2","text":"Retrieve details of an existing scope","title":"Description"},{"location":"rest/restapis/#parameters_1","text":"Type Name Description Schema Path scopeName required Scope name string","title":"Parameters"},{"location":"rest/restapis/#responses_2","text":"HTTP Code Description Schema 200 Successfully retrieved the scope details ScopeProperty 404 Scope with the given name not found No Content 500 Internal server error while fetching scope details No Content","title":"Responses"},{"location":"rest/restapis/#produces_2","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_3","text":"Scopes","title":"Tags"},{"location":"rest/restapis/#example-http-request_2","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_2","text":"/scopes/string","title":"Request path"},{"location":"rest/restapis/#example-http-response_2","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_1","text":"json : { scopeName : string }","title":"Response 200"},{"location":"rest/restapis/#delete-scopesscopename","text":"","title":"DELETE /scopes/{scopeName}"},{"location":"rest/restapis/#description_3","text":"Delete a scope","title":"Description"},{"location":"rest/restapis/#parameters_2","text":"Type Name Description Schema Path scopeName required Scope name string","title":"Parameters"},{"location":"rest/restapis/#responses_3","text":"HTTP Code Description Schema 204 Successfully deleted the scope No Content 404 Scope not found No Content 412 Cannot delete scope since it has non-empty list of streams No Content 500 Internal server error while deleting a scope No Content","title":"Responses"},{"location":"rest/restapis/#tags_4","text":"Scopes","title":"Tags"},{"location":"rest/restapis/#example-http-request_3","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_3","text":"/scopes/string","title":"Request path"},{"location":"rest/restapis/#get-scopesscopenamereadergroups","text":"","title":"GET /scopes/{scopeName}/readergroups"},{"location":"rest/restapis/#description_4","text":"List reader groups within the given scope","title":"Description"},{"location":"rest/restapis/#parameters_3","text":"Type Name Description Schema Path scopeName required Scope name string","title":"Parameters"},{"location":"rest/restapis/#responses_4","text":"HTTP Code Description Schema 200 List of all reader groups configured for the given scope ReaderGroupsList 404 Scope not found No Content 500 Internal server error while fetching the list of reader groups for the given scope No Content","title":"Responses"},{"location":"rest/restapis/#produces_3","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_5","text":"ReaderGroups","title":"Tags"},{"location":"rest/restapis/#example-http-request_4","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_4","text":"/scopes/string/readergroups","title":"Request path"},{"location":"rest/restapis/#example-http-response_3","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_2","text":"json : { readerGroups : [ object ] }","title":"Response 200"},{"location":"rest/restapis/#get-scopesscopenamereadergroupsreadergroupname","text":"","title":"GET /scopes/{scopeName}/readergroups/{readerGroupName}"},{"location":"rest/restapis/#description_5","text":"Fetch the properties of an existing reader group","title":"Description"},{"location":"rest/restapis/#parameters_4","text":"Type Name Description Schema Path readerGroupName required Reader group name string Path scopeName required Scope name string","title":"Parameters"},{"location":"rest/restapis/#responses_5","text":"HTTP Code Description Schema 200 Found reader group properties ReaderGroupProperty 404 Scope or reader group with given name not found No Content 500 Internal server error while fetching reader group details No Content","title":"Responses"},{"location":"rest/restapis/#produces_4","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_6","text":"ReaderGroups","title":"Tags"},{"location":"rest/restapis/#example-http-request_5","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_5","text":"/scopes/string/readergroups/string","title":"Request path"},{"location":"rest/restapis/#example-http-response_4","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_3","text":"json : { scopeName : string , readerGroupName : string , streamList : [ string ], onlineReaderIds : [ string ] }","title":"Response 200"},{"location":"rest/restapis/#post-scopesscopenamestreams","text":"","title":"POST /scopes/{scopeName}/streams"},{"location":"rest/restapis/#description_6","text":"Create a new stream","title":"Description"},{"location":"rest/restapis/#parameters_5","text":"Type Name Description Schema Path scopeName required Scope name string Body CreateStreamRequest required The stream configuration CreateStreamRequest CreateStreamRequest Name Description Schema retentionPolicy optional Example : [retentionconfig](#retentionconfig) RetentionConfig scalingPolicy optional Example : [scalingconfig](#scalingconfig) ScalingConfig streamName optional Example : string string","title":"Parameters"},{"location":"rest/restapis/#responses_6","text":"HTTP Code Description Schema 201 Successfully created the stream with the given configuration StreamProperty 404 Scope not found No Content 409 Stream with given name already exists No Content 500 Internal server error while creating a stream No Content","title":"Responses"},{"location":"rest/restapis/#consumes_1","text":"application/json","title":"Consumes"},{"location":"rest/restapis/#produces_5","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_7","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_6","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_6","text":"/scopes/string/streams","title":"Request path"},{"location":"rest/restapis/#request-body_1","text":"json : { streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } }","title":"Request body"},{"location":"rest/restapis/#example-http-response_5","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-201_1","text":"json : { scopeName : string , streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } }","title":"Response 201"},{"location":"rest/restapis/#get-scopesscopenamestreams","text":"","title":"GET /scopes/{scopeName}/streams"},{"location":"rest/restapis/#description_7","text":"List streams within the given scope","title":"Description"},{"location":"rest/restapis/#parameters_6","text":"Type Name Description Schema Path scopeName required Scope name string Query showInternalStreams optional Optional flag whether to display system created streams. If not specified only user created streams will be returned string","title":"Parameters"},{"location":"rest/restapis/#responses_7","text":"HTTP Code Description Schema 200 List of all streams configured for the given scope StreamsList 404 Scope not found No Content 500 Internal server error while fetching the list of streams for the given scope No Content","title":"Responses"},{"location":"rest/restapis/#produces_6","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_8","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_7","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_7","text":"/scopes/string/streams","title":"Request path"},{"location":"rest/restapis/#request-query","text":"json : { showInternalStreams : string }","title":"Request query"},{"location":"rest/restapis/#example-http-response_6","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_4","text":"json : { streams : [ { scopeName : string , streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } } ] }","title":"Response 200"},{"location":"rest/restapis/#get-scopesscopenamestreamsstreamname","text":"","title":"GET /scopes/{scopeName}/streams/{streamName}"},{"location":"rest/restapis/#description_8","text":"Fetch the properties of an existing stream","title":"Description"},{"location":"rest/restapis/#parameters_7","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string","title":"Parameters"},{"location":"rest/restapis/#responses_8","text":"HTTP Code Description Schema 200 Found stream properties StreamProperty 404 Scope or stream with given name not found No Content 500 Internal server error while fetching stream details No Content","title":"Responses"},{"location":"rest/restapis/#produces_7","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_9","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_8","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_8","text":"/scopes/string/streams/string","title":"Request path"},{"location":"rest/restapis/#example-http-response_7","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_5","text":"json : { scopeName : string , streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } }","title":"Response 200"},{"location":"rest/restapis/#put-scopesscopenamestreamsstreamname","text":"","title":"PUT /scopes/{scopeName}/streams/{streamName}"},{"location":"rest/restapis/#description_9","text":"Update configuration of an existing stream","title":"Description"},{"location":"rest/restapis/#parameters_8","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Body UpdateStreamRequest required The new stream configuration UpdateStreamRequest UpdateStreamRequest Name Description Schema retentionPolicy optional Example : [retentionconfig](#retentionconfig) RetentionConfig scalingPolicy optional Example : [scalingconfig](#scalingconfig) ScalingConfig","title":"Parameters"},{"location":"rest/restapis/#responses_9","text":"HTTP Code Description Schema 200 Successfully updated the stream configuration StreamProperty 404 Scope or stream with given name not found No Content 500 Internal server error while updating the stream No Content","title":"Responses"},{"location":"rest/restapis/#consumes_2","text":"application/json","title":"Consumes"},{"location":"rest/restapis/#produces_8","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_10","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_9","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_9","text":"/scopes/string/streams/string","title":"Request path"},{"location":"rest/restapis/#request-body_2","text":"json : { scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } }","title":"Request body"},{"location":"rest/restapis/#example-http-response_8","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_6","text":"json : { scopeName : string , streamName : string , scalingPolicy : { type : string , targetRate : 0 , scaleFactor : 0 , minSegments : 0 }, retentionPolicy : { type : string , value : 0 } }","title":"Response 200"},{"location":"rest/restapis/#delete-scopesscopenamestreamsstreamname","text":"","title":"DELETE /scopes/{scopeName}/streams/{streamName}"},{"location":"rest/restapis/#description_10","text":"Delete a stream","title":"Description"},{"location":"rest/restapis/#parameters_9","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string","title":"Parameters"},{"location":"rest/restapis/#responses_10","text":"HTTP Code Description Schema 204 Successfully deleted the stream No Content 404 Stream not found No Content 412 Cannot delete stream since it is not sealed No Content 500 Internal server error while deleting the stream No Content","title":"Responses"},{"location":"rest/restapis/#tags_11","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_10","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_10","text":"/scopes/string/streams/string","title":"Request path"},{"location":"rest/restapis/#get-scopesscopenamestreamsstreamnamescaling-events","text":"","title":"GET /scopes/{scopeName}/streams/{streamName}/scaling-events"},{"location":"rest/restapis/#description_11","text":"Get scaling events for a given datetime period.","title":"Description"},{"location":"rest/restapis/#parameters_10","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Query from required Parameter to display scaling events from that particular datetime. Input should be milliseconds from Jan 1 1970. integer (int64) Query to required Parameter to display scaling events to that particular datetime. Input should be milliseconds from Jan 1 1970. integer (int64)","title":"Parameters"},{"location":"rest/restapis/#responses_11","text":"HTTP Code Description Schema 200 Successfully fetched list of scaling events. ScalingEventList 404 Scope/Stream not found. No Content 500 Internal Server error while fetching scaling events. No Content","title":"Responses"},{"location":"rest/restapis/#produces_9","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_12","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_11","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_11","text":"/scopes/string/streams/string/scaling-events","title":"Request path"},{"location":"rest/restapis/#request-query_1","text":"json : { from : 0 , to : 0 }","title":"Request query"},{"location":"rest/restapis/#example-http-response_9","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_7","text":"json : { scalingEvents : [ { timestamp : 0 , segmentList : [ { number : 0 , startTime : 0 , keyStart : 0 , keyEnd : 0 } ] } ] }","title":"Response 200"},{"location":"rest/restapis/#put-scopesscopenamestreamsstreamnamestate","text":"","title":"PUT /scopes/{scopeName}/streams/{streamName}/state"},{"location":"rest/restapis/#description_12","text":"Updates the current state of the stream","title":"Description"},{"location":"rest/restapis/#parameters_11","text":"Type Name Description Schema Path scopeName required Scope name string Path streamName required Stream name string Body UpdateStreamStateRequest required The state info to be updated StreamState","title":"Parameters"},{"location":"rest/restapis/#responses_12","text":"HTTP Code Description Schema 200 Successfully updated the stream state StreamState 404 Scope or stream with given name not found No Content 500 Internal server error while updating the stream state No Content","title":"Responses"},{"location":"rest/restapis/#consumes_3","text":"application/json","title":"Consumes"},{"location":"rest/restapis/#produces_10","text":"application/json","title":"Produces"},{"location":"rest/restapis/#tags_13","text":"Streams","title":"Tags"},{"location":"rest/restapis/#example-http-request_12","text":"","title":"Example HTTP request"},{"location":"rest/restapis/#request-path_12","text":"/scopes/string/streams/string/state","title":"Request path"},{"location":"rest/restapis/#request-body_3","text":"json : { streamState : string }","title":"Request body"},{"location":"rest/restapis/#example-http-response_10","text":"","title":"Example HTTP response"},{"location":"rest/restapis/#response-200_8","text":"json : { streamState : string }","title":"Response 200"},{"location":"rest/restapis/#definitions","text":"","title":"Definitions"},{"location":"rest/restapis/#readergroupproperty","text":"Name Description Schema onlineReaderIds optional Example : [ string ] string array readerGroupName optional Example : string string scopeName optional Example : string string streamList optional Example : [ string ] string array","title":"ReaderGroupProperty"},{"location":"rest/restapis/#readergroupslist","text":"Name Description Schema readerGroups optional Example : [ object ] readerGroups array readerGroups Name Description Schema readerGroupName optional Example : string string","title":"ReaderGroupsList"},{"location":"rest/restapis/#retentionconfig","text":"Name Description Schema type optional Example : string enum (LIMITED_DAYS, LIMITED_SIZE_MB) value optional Example : 0 integer (int64)","title":"RetentionConfig"},{"location":"rest/restapis/#scalemetadata","text":"Name Description Schema segmentList optional Example : [ [segment](#segment) ] Segment array timestamp optional Example : 0 integer (int64)","title":"ScaleMetadata"},{"location":"rest/restapis/#scalingconfig","text":"Name Description Schema minSegments optional Example : 0 integer (int32) scaleFactor optional Example : 0 integer (int32) targetRate optional Example : 0 integer (int32) type optional Example : string enum (FIXED_NUM_SEGMENTS, BY_RATE_IN_KBYTES_PER_SEC, BY_RATE_IN_EVENTS_PER_SEC)","title":"ScalingConfig"},{"location":"rest/restapis/#scalingeventlist","text":"Name Description Schema scalingEvents optional Example : [ [scalemetadata](#scalemetadata) ] ScaleMetadata array","title":"ScalingEventList"},{"location":"rest/restapis/#scopeproperty","text":"Name Description Schema scopeName optional Example : string string","title":"ScopeProperty"},{"location":"rest/restapis/#scopeslist","text":"Name Description Schema scopes optional Example : [ [scopeproperty](#scopeproperty) ] ScopeProperty array","title":"ScopesList"},{"location":"rest/restapis/#segment","text":"Name Description Schema keyEnd optional Example : 0 integer (double) keyStart optional Example : 0 integer (double) number optional Example : 0 integer (int32) startTime optional Example : 0 integer (int64)","title":"Segment"},{"location":"rest/restapis/#streamproperty","text":"Name Description Schema retentionPolicy optional Example : [retentionconfig](#retentionconfig) RetentionConfig scalingPolicy optional Example : [scalingconfig](#scalingconfig) ScalingConfig scopeName optional Example : string string streamName optional Example : string string","title":"StreamProperty"},{"location":"rest/restapis/#streamstate","text":"Name Description Schema streamState optional Example : string enum (SEALED)","title":"StreamState"},{"location":"rest/restapis/#streamslist","text":"Name Description Schema streams optional Example : [ [streamproperty](#streamproperty) ] StreamProperty array","title":"StreamsList"}]}