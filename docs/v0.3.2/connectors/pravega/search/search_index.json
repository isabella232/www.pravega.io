{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Flink Connectors for Pravega This wiki describes the connectors API and it's usage to read and write Pravega streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. See the Pravega Concepts page for more information. Table of Contents Getting Started Quick Start Features Streaming Batch Table API/SQL Metrics Configurations Serialization Releasing Connectors How To Release Publishing Artifacts","title":"Overview"},{"location":"#apache-flink-connectors-for-pravega","text":"This wiki describes the connectors API and it's usage to read and write Pravega streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. See the Pravega Concepts page for more information.","title":"Apache Flink Connectors for Pravega"},{"location":"#table-of-contents","text":"Getting Started Quick Start Features Streaming Batch Table API/SQL Metrics Configurations Serialization Releasing Connectors How To Release Publishing Artifacts","title":"Table of Contents"},{"location":"batch/","text":"Batch Connector The Flink connector library for Pravega makes it possible to use a Pravega Stream as a data source and data sink in a batch program. See the below sections for details. Table of Contents FlinkPravegaInputFormat Parameters Input Stream(s) StreamCuts Parallelism FlinkPravegaOutputFormat Parameters Output Stream Parallelism Event Routing Serialization FlinkPravegaInputFormat A Pravega Stream may be used as a data source within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaInputFormat . The input format, reads a given Pravega Stream as a DataSet (the basic abstraction of the Flink Batch API). Note that the stream elements are considered to be unordered in the batch programming model. Use the ExecutionEnvironment::createInput method to open a Pravega Stream as a DataSet . Example // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < EventType > deserializer = ... // Define the input format based on a Pravega stream FlinkPravegaInputFormat < EventType > inputFormat = FlinkPravegaInputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataSource < EventType > dataSet = env . createInput ( inputFormat , TypeInformation . of ( EventType . class ) . setParallelism ( 2 ); Parameters A builder API is provided to construct an instance of FlinkPravegaInputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. Input Stream(s) Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The BatchClient is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified stream names, the scope is explicitly specified, e.g. my-scope/my-stream . In unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: 1. As a string containing a qualified name, in the form scope/stream . 2. As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. 3. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . StreamCuts A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The BatchClient accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . If stream cuts are not provided then the default start position requested is assumed to be the earliest available data in the stream and the default end position is assumed to be all available data in that stream as of when the job execution begins. Parallelism FlinkPravegaInputFormat supports parallelization. Use the setParallelism method of DataSet to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note : Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The synchronizer creates a backing stream that may be manually deleted after the job finishes. FlinkPravegaOutputFormat A Pravega Stream may be used as a data sink within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaOutputFormat . The FlinkPravegaOutputFormat can be supplied as a sink to the DataSet (the basic abstraction of the Flink Batch API). Example // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < EventType > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < EventType > router = ... // Define the input format based on a Pravega Stream FlinkPravegaOutputFormat < EventType > outputFormat = FlinkPravegaOutputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); Collection < EventType > inputData = Arrays . asList (...); env . fromCollection ( inputData ) . output ( outputFormat ); env . execute ( \"...\" ); Parameter A builder API is provided to construct an instance of FlinkPravegaOutputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. Output Stream Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Parallelism FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute. Event Routing Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. To establish the routing key for each event, provide an implementation of io.pravega.connectors.flink.PravegaEventRouter when constructing the writer. Serialization Please, see the serialization page for more information on how to use the serializer and deserializer .","title":"Batch"},{"location":"batch/#batch-connector","text":"The Flink connector library for Pravega makes it possible to use a Pravega Stream as a data source and data sink in a batch program. See the below sections for details.","title":"Batch Connector"},{"location":"batch/#table-of-contents","text":"FlinkPravegaInputFormat Parameters Input Stream(s) StreamCuts Parallelism FlinkPravegaOutputFormat Parameters Output Stream Parallelism Event Routing Serialization","title":"Table of Contents"},{"location":"batch/#flinkpravegainputformat","text":"A Pravega Stream may be used as a data source within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaInputFormat . The input format, reads a given Pravega Stream as a DataSet (the basic abstraction of the Flink Batch API). Note that the stream elements are considered to be unordered in the batch programming model. Use the ExecutionEnvironment::createInput method to open a Pravega Stream as a DataSet .","title":"FlinkPravegaInputFormat"},{"location":"batch/#example","text":"// Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < EventType > deserializer = ... // Define the input format based on a Pravega stream FlinkPravegaInputFormat < EventType > inputFormat = FlinkPravegaInputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataSource < EventType > dataSet = env . createInput ( inputFormat , TypeInformation . of ( EventType . class ) . setParallelism ( 2 );","title":"Example"},{"location":"batch/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaInputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events.","title":"Parameters"},{"location":"batch/#input-streams","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The BatchClient is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified stream names, the scope is explicitly specified, e.g. my-scope/my-stream . In unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: 1. As a string containing a qualified name, in the form scope/stream . 2. As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. 3. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Input Stream(s)"},{"location":"batch/#streamcuts","text":"A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The BatchClient accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . If stream cuts are not provided then the default start position requested is assumed to be the earliest available data in the stream and the default end position is assumed to be all available data in that stream as of when the job execution begins.","title":"StreamCuts"},{"location":"batch/#parallelism","text":"FlinkPravegaInputFormat supports parallelization. Use the setParallelism method of DataSet to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note : Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The synchronizer creates a backing stream that may be manually deleted after the job finishes.","title":"Parallelism"},{"location":"batch/#flinkpravegaoutputformat","text":"A Pravega Stream may be used as a data sink within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaOutputFormat . The FlinkPravegaOutputFormat can be supplied as a sink to the DataSet (the basic abstraction of the Flink Batch API).","title":"FlinkPravegaOutputFormat"},{"location":"batch/#example_1","text":"// Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < EventType > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < EventType > router = ... // Define the input format based on a Pravega Stream FlinkPravegaOutputFormat < EventType > outputFormat = FlinkPravegaOutputFormat .< EventType > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); Collection < EventType > inputData = Arrays . asList (...); env . fromCollection ( inputData ) . output ( outputFormat ); env . execute ( \"...\" );","title":"Example"},{"location":"batch/#parameter","text":"A builder API is provided to construct an instance of FlinkPravegaOutputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event.","title":"Parameter"},{"location":"batch/#output-stream","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Output Stream"},{"location":"batch/#parallelism_1","text":"FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute.","title":"Parallelism"},{"location":"batch/#event-routing","text":"Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. To establish the routing key for each event, provide an implementation of io.pravega.connectors.flink.PravegaEventRouter when constructing the writer.","title":"Event Routing"},{"location":"batch/#serialization","text":"Please, see the serialization page for more information on how to use the serializer and deserializer .","title":"Serialization"},{"location":"configurations/","text":"Configurations The Flink connector library for Pravega supports the Flink Streaming API , Table API and Batch API , using a common configuration class. Table of Contents Common Configuration PravegaConfig Class Creating PravegaConfig Using PravegaConfig Understanding the Default Scope Common Configuration PravegaConfig Class A top-level config object, PravegaConfig , is provided to establish a Pravega context for the Flink connector. The config object automatically configures itself from environment variables , system properties and program arguments . PravegaConfig information sources is given below: Setting Environment Variable / System Property / Program Argument Default Value Controller URI PRAVEGA_CONTROLLER_URI pravega.controller.uri --controller tcp://localhost:9090 Default Scope PRAVEGA_SCOPE pravega.scope --scope - Credentials - - Hostname Validation - true Creating PravegaConfig The recommended way to create an instance of PravegaConfig is to pass an instance of ParameterTool to fromParams : ParameterTool params = ParameterTool . fromArgs ( args ); PravegaConfig config = PravegaConfig . fromParams ( params ); If your application doesn't use the ParameterTool class that is provided by Flink, create the PravegaConfig using fromDefaults : PravegaConfig config = PravegaConfig . fromDefaults (); The PravegaConfig class provides a builder-style API to override the default configuration settings: PravegaConfig config = PravegaConfig . fromDefaults () . withControllerURI ( \"tcp://...\" ) . withDefaultScope ( \"SCOPE-NAME\" ) . withCredentials ( credentials ) . withHostnameValidation ( false ); Using PravegaConfig All of the various source and sink classes provided with the connector library have a builder-style API which accepts a PravegaConfig for common configuration. Pass a PravegaConfig object to the respective builder via withPravegaConfig . For example, see below code: PravegaConfig config = ...; FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . build (); Understanding the Default Scope Pravega organizes streams into scopes for the purposes of manageability. The PravegaConfig establishes a default scope name that is used in two scenarios: 1. For resolving unqualified stream names when constructing a source or sink. The sources and sinks accept stream names that may be qualified (e.g. my-scope/my-stream ) or unqualified (e.g. my-stream ). 2. For establishing the scope name for the coordination stream underlying a Pravega Reader Group. It is important to note that, the FlinkPravegaReader and the FlinkPravegaTableSource use the default scope configured on PravegaConfig as their Reader Group scope, and provide withReaderGroupScope as an override. The scope name of input stream(s) doesn't influence the Reader Group scope.","title":"Configurations"},{"location":"configurations/#configurations","text":"The Flink connector library for Pravega supports the Flink Streaming API , Table API and Batch API , using a common configuration class.","title":"Configurations"},{"location":"configurations/#table-of-contents","text":"Common Configuration PravegaConfig Class Creating PravegaConfig Using PravegaConfig Understanding the Default Scope","title":"Table of Contents"},{"location":"configurations/#common-configuration","text":"","title":"Common Configuration"},{"location":"configurations/#pravegaconfig-class","text":"A top-level config object, PravegaConfig , is provided to establish a Pravega context for the Flink connector. The config object automatically configures itself from environment variables , system properties and program arguments . PravegaConfig information sources is given below: Setting Environment Variable / System Property / Program Argument Default Value Controller URI PRAVEGA_CONTROLLER_URI pravega.controller.uri --controller tcp://localhost:9090 Default Scope PRAVEGA_SCOPE pravega.scope --scope - Credentials - - Hostname Validation - true","title":"PravegaConfig Class"},{"location":"configurations/#creating-pravegaconfig","text":"The recommended way to create an instance of PravegaConfig is to pass an instance of ParameterTool to fromParams : ParameterTool params = ParameterTool . fromArgs ( args ); PravegaConfig config = PravegaConfig . fromParams ( params ); If your application doesn't use the ParameterTool class that is provided by Flink, create the PravegaConfig using fromDefaults : PravegaConfig config = PravegaConfig . fromDefaults (); The PravegaConfig class provides a builder-style API to override the default configuration settings: PravegaConfig config = PravegaConfig . fromDefaults () . withControllerURI ( \"tcp://...\" ) . withDefaultScope ( \"SCOPE-NAME\" ) . withCredentials ( credentials ) . withHostnameValidation ( false );","title":"Creating PravegaConfig"},{"location":"configurations/#using-pravegaconfig","text":"All of the various source and sink classes provided with the connector library have a builder-style API which accepts a PravegaConfig for common configuration. Pass a PravegaConfig object to the respective builder via withPravegaConfig . For example, see below code: PravegaConfig config = ...; FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . build ();","title":"Using PravegaConfig"},{"location":"configurations/#understanding-the-default-scope","text":"Pravega organizes streams into scopes for the purposes of manageability. The PravegaConfig establishes a default scope name that is used in two scenarios: 1. For resolving unqualified stream names when constructing a source or sink. The sources and sinks accept stream names that may be qualified (e.g. my-scope/my-stream ) or unqualified (e.g. my-stream ). 2. For establishing the scope name for the coordination stream underlying a Pravega Reader Group. It is important to note that, the FlinkPravegaReader and the FlinkPravegaTableSource use the default scope configured on PravegaConfig as their Reader Group scope, and provide withReaderGroupScope as an override. The scope name of input stream(s) doesn't influence the Reader Group scope.","title":"Understanding the Default Scope"},{"location":"getting-started/","text":"Pravega Flink Connectors Connectors to read and write Pravega Streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. Features & Highlights Exactly-once processing guarantees for both Reader and Writer, supporting end-to-end exactly-once processing pipelines Seamless integration with Flink's checkpoints and savepoints. Parallel Readers and Writers supporting high throughput and low latency processing. Table API support to access Pravega Streams for both Batch and Streaming use case. Building Connectors Building the connectors from the source is only necessary when we want to use or contribute to the latest ( unreleased ) version of the Pravega Flink connectors. The connector project is linked to a specific version of Pravega, based on a git submodule pointing to a commit-id. By default the sub-module option is disabled and the build step will make use of the Pravega version defined in the gradle.properties file. You could override this option by enabling usePravegaVersionSubModule flag in gradle.properties to true . Checkout the source code repository by following below steps: git clone --recursive https://github.com/pravega/flink-connectors.git After cloning the repository, the project can be built by running the below command in the project root directory flink-connectors . ./gradlew clean build To install the artifacts in the local maven repository cache ~/.m2/repository , run the following command: ./gradlew clean install Customizing the Build Building against a custom Flink version We can check and change the Flink version that Pravega builds against via the flinkVersion variable in the gradle.properties file. Note : Only Flink versions that are compatible with the latest connector code can be chosen. Building against another Scala version This section is only relevant if you use Scala in the stream processing application in with Flink and Pravega. Parts of the Apache Flink use the language or depend on libraries written in Scala. Because Scala is not strictly compatible across versions, there exist different versions of Flink compiled for different Scala versions. If we use Scala code in the same application where we use the Apache Flink or the Flink connectors, we typically have to make sure we use a version of Flink that uses the same Scala version as our application. By default, the dependencies point to Flink for Scala 2.11 . To depend on released Flink artifacts for a different Scala version, you need to edit the build.gradle file and change all entries for the Flink dependencies to have a different Scala version suffix. For example, flink-streaming-java_2.11 would be replaced by flink-streaming-java_2.12 for Scala 2.12 . In order to build a new version of Flink for a different Scala version, please refer to the Flink documentation . Setting up your IDE Connector project uses Project Lombok , so we should ensure that we have our IDE setup with the required plugins. ( IntelliJ is recommended ). To import the source into IntelliJ: Import the project directory into IntelliJ IDE. It will automatically detect the gradle project and import things correctly. Enable Annotation Processing by going to Build, Execution, Deployment -> Compiler > Annotation Processors and checking Enable annotation processing . Install the Lombok Plugin . This can be found in Preferences -> Plugins . Restart your IDE. Connectors project compiles properly after applying the above steps. For eclipse, we can generate eclipse project files by running ./gradlew eclipse . Releases The latest releases can be found on the Github Release project page. Support Don\u2019t hesitate to ask! Contact the developers and community on the Slack if you need any help. Open an issue if you found a bug on Github Issues . Documentation See the Project Wiki for documentation on how to build and use the Flink Connector library. More examples on how to use the connectors with Flink application can be found in Pravega Samples repository. About Flink connectors for Pravega is 100% open source and community-driven. All components are available under Apache 2 License on GitHub.","title":"Getting Started"},{"location":"getting-started/#pravega-flink-connectors","text":"Connectors to read and write Pravega Streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams.","title":"Pravega Flink Connectors"},{"location":"getting-started/#features-highlights","text":"Exactly-once processing guarantees for both Reader and Writer, supporting end-to-end exactly-once processing pipelines Seamless integration with Flink's checkpoints and savepoints. Parallel Readers and Writers supporting high throughput and low latency processing. Table API support to access Pravega Streams for both Batch and Streaming use case.","title":"Features &amp; Highlights"},{"location":"getting-started/#building-connectors","text":"Building the connectors from the source is only necessary when we want to use or contribute to the latest ( unreleased ) version of the Pravega Flink connectors. The connector project is linked to a specific version of Pravega, based on a git submodule pointing to a commit-id. By default the sub-module option is disabled and the build step will make use of the Pravega version defined in the gradle.properties file. You could override this option by enabling usePravegaVersionSubModule flag in gradle.properties to true . Checkout the source code repository by following below steps: git clone --recursive https://github.com/pravega/flink-connectors.git After cloning the repository, the project can be built by running the below command in the project root directory flink-connectors . ./gradlew clean build To install the artifacts in the local maven repository cache ~/.m2/repository , run the following command: ./gradlew clean install","title":"Building Connectors"},{"location":"getting-started/#customizing-the-build","text":"","title":"Customizing the Build"},{"location":"getting-started/#building-against-a-custom-flink-version","text":"We can check and change the Flink version that Pravega builds against via the flinkVersion variable in the gradle.properties file. Note : Only Flink versions that are compatible with the latest connector code can be chosen.","title":"Building against a custom Flink version"},{"location":"getting-started/#building-against-another-scala-version","text":"This section is only relevant if you use Scala in the stream processing application in with Flink and Pravega. Parts of the Apache Flink use the language or depend on libraries written in Scala. Because Scala is not strictly compatible across versions, there exist different versions of Flink compiled for different Scala versions. If we use Scala code in the same application where we use the Apache Flink or the Flink connectors, we typically have to make sure we use a version of Flink that uses the same Scala version as our application. By default, the dependencies point to Flink for Scala 2.11 . To depend on released Flink artifacts for a different Scala version, you need to edit the build.gradle file and change all entries for the Flink dependencies to have a different Scala version suffix. For example, flink-streaming-java_2.11 would be replaced by flink-streaming-java_2.12 for Scala 2.12 . In order to build a new version of Flink for a different Scala version, please refer to the Flink documentation .","title":"Building against another Scala version"},{"location":"getting-started/#setting-up-your-ide","text":"Connector project uses Project Lombok , so we should ensure that we have our IDE setup with the required plugins. ( IntelliJ is recommended ). To import the source into IntelliJ: Import the project directory into IntelliJ IDE. It will automatically detect the gradle project and import things correctly. Enable Annotation Processing by going to Build, Execution, Deployment -> Compiler > Annotation Processors and checking Enable annotation processing . Install the Lombok Plugin . This can be found in Preferences -> Plugins . Restart your IDE. Connectors project compiles properly after applying the above steps. For eclipse, we can generate eclipse project files by running ./gradlew eclipse .","title":"Setting up your IDE"},{"location":"getting-started/#releases","text":"The latest releases can be found on the Github Release project page.","title":"Releases"},{"location":"getting-started/#support","text":"Don\u2019t hesitate to ask! Contact the developers and community on the Slack if you need any help. Open an issue if you found a bug on Github Issues .","title":"Support"},{"location":"getting-started/#documentation","text":"See the Project Wiki for documentation on how to build and use the Flink Connector library. More examples on how to use the connectors with Flink application can be found in Pravega Samples repository.","title":"Documentation"},{"location":"getting-started/#about","text":"Flink connectors for Pravega is 100% open source and community-driven. All components are available under Apache 2 License on GitHub.","title":"About"},{"location":"how-to-release/","text":"How to Release? If you are releasing a version of Pravega/Flink Connector, then you must read and follow the below instructions. The steps mentioned here are based on the experience we are building across releases, if you have any queries, please feel free to open an issue on Github Issues or contact us on Slack . Note : In case, if you are updating Flink version, please make sure, the new features or changes introduced in Flink are addressed in the connector. Preparing the Branch Preparing the branch consists of making the necessary changes to the branch we will be working on as part of releasing. Following are the possible two situations: Bug fix release : This is a minor release version over an existing release branch. Feature release or non-backward-compatible release : This is a change to either the first or the middle digit, and it requires a new release branch. Bug Fix Release For bug fix release, no new branch is required. First identify the branch we will be working on. It will be named rX.Y , (e.g., r0.2 ). The preparation consists of: Changing connectorVersion in gradle.properties from X.Y.Z-SNAPSHOT to X.Y.Z . For example, if the current value is connectorVersion=0.2.1-SNAPSHOT , then change it to connectorVersion=0.2.1 . Merge this change. Tag the commit with vX.Y.Z-rc0 . (For example, v0.2.1-rc0 ). Please note the following when performing step 3: 1. There are two ways to tag: - Via the command line : ``` > git checkout rX.Y > git tag vX.Y.Z-rc0 > git push upstream vX.Y.Z-rc0 ``` Ensure that your `upstream` is set up correctly. - **Via GitHub releases**: When creating a release candidate, Github automatically creates the tag if one doesn't exist. This is discussed in the [release candidate](#pushing-arelease-candidate) section. It is possible that a release candidate is problematic and we need to do a new release candidate. In this case, we need to repeat this tagging step as many times as needed. Note that when creating a new release candidate tag, we do not need to update the Connector version. Major Release In major release, either the middle or the most significant digit is changed. To perform major release, a new branch should be created. Please follow the below steps: For example, assume the new release to be 0.3.0 > git checkout master > git tag branch-0.3 > git push upstream branch-0.3 > git checkout -b r0.3 > git push upstream r0.3 After the above steps are performed, version changes needs to be updated to both master and r0.3 : In master , create an issue and corresponding pull request to change the connectorVersion in gradle.properties to 0.4.0-SNAPSHOT . Note that we are bumping up the middle digit because, in our example, we are releasing 0.3.0 . If we were releasing say 1.0.0 , then we would change connectorVersion to 1.1.0-SNAPSHOT . In r0.3 , create an issue and corresponding pull request to change the connectorVersion in gradle.properties to 0.3.0 . Once that change is merged, we need to tag the commit point in the same way we described for a bug fix release. See instructions in the previous section. Pushing a release candidate Step 1: Create a release on GitHub On the GitHub repository page, go to releases and create a new draft: Mark it as a \"pre-release\". Fill out the tag field and select the appropriate branch. Note that this is a release candidate, so the tag should look like vX.Y.Z-rcA Step 2: Build the distribution Run the following commands: For example, assume the branch to be r0.3 > git checkout r0.3 > ./gradlew clean install The files resulting from the build will be under ~/.m2/repository/io/pravega/pravega-connectors-flink_2.11/<RELEASE-VERSION> . For each one of the .jar files in that directory, generate checksums (currently md5 , sha1 , and sha256 ). Use the following bash script: #!/bin/bash for file in ./*.jar ; do md5sum $file > $file.md5 ; done for file in ./*.jar ; do shasum -a 1 $file > $file.sha1 ; done for file in ./*.jar ; do shasum -a 256 $file > $file.sha256 ; done Note :In the future, we might want to automate the generation of checksums via gradle. Step 3: Upload the files to the pre-release draft In the pre-release draft on GitHub, upload all the jar files (and its checksums) under ~/.m2/repository/io/pravega/pravega-connectors-flink_2.11/<RELEASE-VERSION> . Follow the instructions on that appears on the Github release page, it is straightforward. Step 4: Release Notes Create a release notes text file containing the following: 1. Related introductory text, highlighting the important changes featured in the release. 2. A complete list of commits and it can be obtained using the following command: > git log <commit-id-of-last-release>..<current-commit-id> <commit-id-of-last-release> depends on the kind of release we are doing. If it is a bug fix release, then we can use the tag of the last branch release. For new branches, we have been adding branch-X.Y tags at the branch point for convenience. <current-commit-id> is the commit point of the release candidate we are working on. If you have manually tagged the release candidate, then you can go ahead and use it in the above mentioned git log command. Add the list to the release notes file and attach it the notes box in the release draft. See previous releases for an example of how to put together notes. Step 5: Publishing The final step is to publish the release candidate by clicking on the button on the draft page. Once published, request the developers and community to validate the candidate. Releasing Once you are happy with the release candidate, we can start the release process. There are two main parts for a Connector release: Releasing on GitHub Publishing on Sonatype -> Maven Central Releasing on GitHub The process involved is similar to the creation of a release candidate as mentioned above. The following changes should be applied: 1. The tag should not have an rcA in it. If the successful rc is v0.3.0-rc0 , then the release tag is v0.3.0 . 2. Uncheck the pre-release box. Publishing on Sonatype -> Maven Central For this step, we need a Sonatype account. See this guide for how to create a Sonatype account. Your account also needs to be associated to Pravega to be able to publish the artifacts. Once you are ready, run the following steps: Perform the build using the following command: ./gradlew clean assemble publishToRepo -PdoSigning=true -Psigning.password=<signing-password> -PpublishUrl=mavenCentral -PpublishUsername=<sonatype-username> -PpublishPassword=<sonatype-password> Login to Nexus Repository Manager using Sonatype credentials with write access to io.pravega group. Under Build Promotion, choose the Staging Repositories, locate the staging repository that was created for the latest publish (format the iopravega-XXXX , like for example iopravega-1004 ). Select the repository and select the Close button in the top menu bar. This will perform validations to ensure that the contents meets the maven requirements (contains signatures, javadocs, sources, etc.). This operation takes minimal time to complete. (Press the Refresh button in the top menu bar occasionally until the operation completes). Once the operation completes, locate the URL field in the Summary tab of the newly closed repository (it will be something like https://oss.sonatype.org/content/repositories/iopravega-XXXX where XXXX is the number of the staging repository). This should be tested to ensure that all artifacts are present and functions as expected. To test, for example, use the pravega-samples and checkout develop branch to verify that it can locate and build with the staging artifacts. Concretely: 1. Change pravegaVersion and connectorVersion in gradle.properties to the staging version. 2. Run ./gradlew clean build After ensuring the correct working of the above mentioned procedures, Please click on the Release button in the top menu bar. Please do wait, until it shows up in Maven Central . Change the Connector version Once the release is done, create an issue and corresponding pull request to change the connectorVersion in gradle.properties to X.Y.(Z+1)-SNAPSHOT for the release branch.","title":"How To Release"},{"location":"how-to-release/#how-to-release","text":"If you are releasing a version of Pravega/Flink Connector, then you must read and follow the below instructions. The steps mentioned here are based on the experience we are building across releases, if you have any queries, please feel free to open an issue on Github Issues or contact us on Slack . Note : In case, if you are updating Flink version, please make sure, the new features or changes introduced in Flink are addressed in the connector.","title":"How to Release?"},{"location":"how-to-release/#preparing-the-branch","text":"Preparing the branch consists of making the necessary changes to the branch we will be working on as part of releasing. Following are the possible two situations: Bug fix release : This is a minor release version over an existing release branch. Feature release or non-backward-compatible release : This is a change to either the first or the middle digit, and it requires a new release branch.","title":"Preparing the Branch"},{"location":"how-to-release/#bug-fix-release","text":"For bug fix release, no new branch is required. First identify the branch we will be working on. It will be named rX.Y , (e.g., r0.2 ). The preparation consists of: Changing connectorVersion in gradle.properties from X.Y.Z-SNAPSHOT to X.Y.Z . For example, if the current value is connectorVersion=0.2.1-SNAPSHOT , then change it to connectorVersion=0.2.1 . Merge this change. Tag the commit with vX.Y.Z-rc0 . (For example, v0.2.1-rc0 ). Please note the following when performing step 3: 1. There are two ways to tag: - Via the command line : ``` > git checkout rX.Y > git tag vX.Y.Z-rc0 > git push upstream vX.Y.Z-rc0 ``` Ensure that your `upstream` is set up correctly. - **Via GitHub releases**: When creating a release candidate, Github automatically creates the tag if one doesn't exist. This is discussed in the [release candidate](#pushing-arelease-candidate) section. It is possible that a release candidate is problematic and we need to do a new release candidate. In this case, we need to repeat this tagging step as many times as needed. Note that when creating a new release candidate tag, we do not need to update the Connector version.","title":"Bug Fix Release"},{"location":"how-to-release/#major-release","text":"In major release, either the middle or the most significant digit is changed. To perform major release, a new branch should be created. Please follow the below steps: For example, assume the new release to be 0.3.0 > git checkout master > git tag branch-0.3 > git push upstream branch-0.3 > git checkout -b r0.3 > git push upstream r0.3 After the above steps are performed, version changes needs to be updated to both master and r0.3 : In master , create an issue and corresponding pull request to change the connectorVersion in gradle.properties to 0.4.0-SNAPSHOT . Note that we are bumping up the middle digit because, in our example, we are releasing 0.3.0 . If we were releasing say 1.0.0 , then we would change connectorVersion to 1.1.0-SNAPSHOT . In r0.3 , create an issue and corresponding pull request to change the connectorVersion in gradle.properties to 0.3.0 . Once that change is merged, we need to tag the commit point in the same way we described for a bug fix release. See instructions in the previous section.","title":"Major Release"},{"location":"how-to-release/#pushing-a-release-candidate","text":"","title":"Pushing a release candidate"},{"location":"how-to-release/#step-1-create-a-release-on-github","text":"On the GitHub repository page, go to releases and create a new draft: Mark it as a \"pre-release\". Fill out the tag field and select the appropriate branch. Note that this is a release candidate, so the tag should look like vX.Y.Z-rcA","title":"Step 1: Create a release on GitHub"},{"location":"how-to-release/#step-2-build-the-distribution","text":"Run the following commands: For example, assume the branch to be r0.3 > git checkout r0.3 > ./gradlew clean install The files resulting from the build will be under ~/.m2/repository/io/pravega/pravega-connectors-flink_2.11/<RELEASE-VERSION> . For each one of the .jar files in that directory, generate checksums (currently md5 , sha1 , and sha256 ). Use the following bash script: #!/bin/bash for file in ./*.jar ; do md5sum $file > $file.md5 ; done for file in ./*.jar ; do shasum -a 1 $file > $file.sha1 ; done for file in ./*.jar ; do shasum -a 256 $file > $file.sha256 ; done Note :In the future, we might want to automate the generation of checksums via gradle.","title":"Step 2: Build the distribution"},{"location":"how-to-release/#step-3-upload-the-files-to-the-pre-release-draft","text":"In the pre-release draft on GitHub, upload all the jar files (and its checksums) under ~/.m2/repository/io/pravega/pravega-connectors-flink_2.11/<RELEASE-VERSION> . Follow the instructions on that appears on the Github release page, it is straightforward.","title":"Step 3: Upload the files to the pre-release draft"},{"location":"how-to-release/#step-4-release-notes","text":"Create a release notes text file containing the following: 1. Related introductory text, highlighting the important changes featured in the release. 2. A complete list of commits and it can be obtained using the following command: > git log <commit-id-of-last-release>..<current-commit-id> <commit-id-of-last-release> depends on the kind of release we are doing. If it is a bug fix release, then we can use the tag of the last branch release. For new branches, we have been adding branch-X.Y tags at the branch point for convenience. <current-commit-id> is the commit point of the release candidate we are working on. If you have manually tagged the release candidate, then you can go ahead and use it in the above mentioned git log command. Add the list to the release notes file and attach it the notes box in the release draft. See previous releases for an example of how to put together notes.","title":"Step 4: Release Notes"},{"location":"how-to-release/#step-5-publishing","text":"The final step is to publish the release candidate by clicking on the button on the draft page. Once published, request the developers and community to validate the candidate.","title":"Step 5: Publishing"},{"location":"how-to-release/#releasing","text":"Once you are happy with the release candidate, we can start the release process. There are two main parts for a Connector release: Releasing on GitHub Publishing on Sonatype -> Maven Central","title":"Releasing"},{"location":"how-to-release/#releasing-on-github","text":"The process involved is similar to the creation of a release candidate as mentioned above. The following changes should be applied: 1. The tag should not have an rcA in it. If the successful rc is v0.3.0-rc0 , then the release tag is v0.3.0 . 2. Uncheck the pre-release box.","title":"Releasing on GitHub"},{"location":"how-to-release/#publishing-on-sonatype-maven-central","text":"For this step, we need a Sonatype account. See this guide for how to create a Sonatype account. Your account also needs to be associated to Pravega to be able to publish the artifacts. Once you are ready, run the following steps: Perform the build using the following command: ./gradlew clean assemble publishToRepo -PdoSigning=true -Psigning.password=<signing-password> -PpublishUrl=mavenCentral -PpublishUsername=<sonatype-username> -PpublishPassword=<sonatype-password> Login to Nexus Repository Manager using Sonatype credentials with write access to io.pravega group. Under Build Promotion, choose the Staging Repositories, locate the staging repository that was created for the latest publish (format the iopravega-XXXX , like for example iopravega-1004 ). Select the repository and select the Close button in the top menu bar. This will perform validations to ensure that the contents meets the maven requirements (contains signatures, javadocs, sources, etc.). This operation takes minimal time to complete. (Press the Refresh button in the top menu bar occasionally until the operation completes). Once the operation completes, locate the URL field in the Summary tab of the newly closed repository (it will be something like https://oss.sonatype.org/content/repositories/iopravega-XXXX where XXXX is the number of the staging repository). This should be tested to ensure that all artifacts are present and functions as expected. To test, for example, use the pravega-samples and checkout develop branch to verify that it can locate and build with the staging artifacts. Concretely: 1. Change pravegaVersion and connectorVersion in gradle.properties to the staging version. 2. Run ./gradlew clean build After ensuring the correct working of the above mentioned procedures, Please click on the Release button in the top menu bar. Please do wait, until it shows up in Maven Central .","title":"Publishing on Sonatype -&gt; Maven Central"},{"location":"how-to-release/#change-the-connector-version","text":"Once the release is done, create an issue and corresponding pull request to change the connectorVersion in gradle.properties to X.Y.(Z+1)-SNAPSHOT for the release branch.","title":"Change the Connector version"},{"location":"metrics/","text":"Metrics Pravega metrics are collected and exposed via Flink metrics framework when using FlinkPravegaReader or FlinkPravegaWriter . Reader Metrics The following metrics are exposed for FlinkPravegaReader related operations: Name Description readerGroupName The name of the Reader Group. scope The scope name of the Reader Group. streams The fully qualified name (i.e., scope/stream ) of the streams that are part of the Reader Group. onlineReaders The readers that are currently online/available. segmentPositions The StreamCut information that indicates where the readers have read so far. unreadBytes The total number of bytes that have not been read yet. Writer Metrics For FlinkPravegaWriter related operations, only the stream name is exposed: Name Description streams The fully qualified name of the stream i.e., scope/stream Querying Metrics The metrics can be viewed either from Flink UI or using the Flink REST API (like below): curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . readerGroupName curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . scope curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . streams curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . onlineReaders curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . stream . test . segmentPositions curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . unreadBytes","title":"Metrics"},{"location":"metrics/#metrics","text":"Pravega metrics are collected and exposed via Flink metrics framework when using FlinkPravegaReader or FlinkPravegaWriter .","title":"Metrics"},{"location":"metrics/#reader-metrics","text":"The following metrics are exposed for FlinkPravegaReader related operations: Name Description readerGroupName The name of the Reader Group. scope The scope name of the Reader Group. streams The fully qualified name (i.e., scope/stream ) of the streams that are part of the Reader Group. onlineReaders The readers that are currently online/available. segmentPositions The StreamCut information that indicates where the readers have read so far. unreadBytes The total number of bytes that have not been read yet.","title":"Reader Metrics"},{"location":"metrics/#writer-metrics","text":"For FlinkPravegaWriter related operations, only the stream name is exposed: Name Description streams The fully qualified name of the stream i.e., scope/stream","title":"Writer Metrics"},{"location":"metrics/#querying-metrics","text":"The metrics can be viewed either from Flink UI or using the Flink REST API (like below): curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . readerGroupName curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . scope curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . streams curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . onlineReaders curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . stream . test . segmentPositions curl - i - s - f / jobs /< JOB - ID >/ vertices /< SOURCE - TASK - ID >/ metrics ? get = 0. Source__ < SOURCE - OPERATOR - NAME >. PravegaReader . readerGroup . unreadBytes","title":"Querying Metrics"},{"location":"publishing-artifacts/","text":"Publishing Artifacts Pravega/Flink connector artifacts are published in the following repositories. Snapshot Artifacts can be found in jcenter -> OJO . Release Artifacts can be found in Sonatype -> Maven Central . Pravega/Flink connector artifacts are used by projects like pravega-samples which are used by external users. The master branch of the pravega-samples repository will always point to a stable release version of Pravega/Flink connector. However, the development branch ( develop ) of pravega-samples are likely to be unstable due to the Pravega/Flink connector snapshot artifact dependency. ( As the development branch of Pravega/Flink connector could possibly introduce a breaking change ). A typical version label of a snapshot artifact will have the label -SNAPSHOT associated with it (for e.g., 0.1.0-SNAPSHOT ). There could be more revisions for a snapshot version and the maven/gradle build scripts is responsible for fetching the most recent version from the available list of revisions. The snapshot repositories are usually configured to automatically discard the old revisions based on some retention policy settings. Any downstream projects that are depending on snapshots are faced with the challenges of keeping up with the snapshot changes since they could possibly introduce any breaking changes. To overcome this issue, the Pravega/Flink connector snapshot artifacts are published to jcenter repository with a stable version label which can be referred by any downstream projects. This will guarantee the build stability of the downstream projects that has a dependency on flink-connectors snapshot artifacts. However, unlike typical maven snapshots that are refreshed automatically upon any new revisions, the downstream projects are expected to synchronize with any latest snapshot revisions from flink-connectors by manually updating the build to make use of the new revisions. Publishing Snapshots The snapshots are the artifacts that are coming from master branch (development branch). The snapshot artifacts are published automatically to jcenter through Travis build setup. Any updates to master branch will trigger a build that will publish the artifacts upon successful completion of the build. We use bintray account to publish snapshot artifacts. Here is the link to the published snapshot artifacts. The gradle task that is used to publish the artifacts is provided below. ./gradlew clean assemble publishToRepo -PpublishUrl=jcenterSnapshot -PpublishUsername=<user> -PpublishPassword=<password> The bintray credentials are encrypted using travis encrypt tool and are created for the namespace pravega/pravega . /usr/local/bin/travis encrypt BINTRAY_USER=<BINTRAY_USER> --adapter net-http /usr/local/bin/travis encrypt BINTRAY_KEY=<BINTRAY_KEY> --adapter net-http Publishing Release Artifacts We use Sonatype -> Maven Central repository to manage the release artifacts. Please follow How-to-release page to understand the complete steps required to release a Pravega/Flink connector version. Here is the gradle task that is used to publish the artifacts. The published artifacts can be found here for more information. ./gradlew clean assemble publishToRepo -PdoSigning=true -Psigning.password=<signing-password> -PpublishUrl=mavenCentral -PpublishUsername=<sonatype-username> -PpublishPassword=<sonatype-password>","title":"Publishing Artifacts"},{"location":"publishing-artifacts/#publishing-artifacts","text":"Pravega/Flink connector artifacts are published in the following repositories. Snapshot Artifacts can be found in jcenter -> OJO . Release Artifacts can be found in Sonatype -> Maven Central . Pravega/Flink connector artifacts are used by projects like pravega-samples which are used by external users. The master branch of the pravega-samples repository will always point to a stable release version of Pravega/Flink connector. However, the development branch ( develop ) of pravega-samples are likely to be unstable due to the Pravega/Flink connector snapshot artifact dependency. ( As the development branch of Pravega/Flink connector could possibly introduce a breaking change ). A typical version label of a snapshot artifact will have the label -SNAPSHOT associated with it (for e.g., 0.1.0-SNAPSHOT ). There could be more revisions for a snapshot version and the maven/gradle build scripts is responsible for fetching the most recent version from the available list of revisions. The snapshot repositories are usually configured to automatically discard the old revisions based on some retention policy settings. Any downstream projects that are depending on snapshots are faced with the challenges of keeping up with the snapshot changes since they could possibly introduce any breaking changes. To overcome this issue, the Pravega/Flink connector snapshot artifacts are published to jcenter repository with a stable version label which can be referred by any downstream projects. This will guarantee the build stability of the downstream projects that has a dependency on flink-connectors snapshot artifacts. However, unlike typical maven snapshots that are refreshed automatically upon any new revisions, the downstream projects are expected to synchronize with any latest snapshot revisions from flink-connectors by manually updating the build to make use of the new revisions.","title":"Publishing Artifacts"},{"location":"publishing-artifacts/#publishing-snapshots","text":"The snapshots are the artifacts that are coming from master branch (development branch). The snapshot artifacts are published automatically to jcenter through Travis build setup. Any updates to master branch will trigger a build that will publish the artifacts upon successful completion of the build. We use bintray account to publish snapshot artifacts. Here is the link to the published snapshot artifacts. The gradle task that is used to publish the artifacts is provided below. ./gradlew clean assemble publishToRepo -PpublishUrl=jcenterSnapshot -PpublishUsername=<user> -PpublishPassword=<password> The bintray credentials are encrypted using travis encrypt tool and are created for the namespace pravega/pravega . /usr/local/bin/travis encrypt BINTRAY_USER=<BINTRAY_USER> --adapter net-http /usr/local/bin/travis encrypt BINTRAY_KEY=<BINTRAY_KEY> --adapter net-http","title":"Publishing Snapshots"},{"location":"publishing-artifacts/#publishing-release-artifacts","text":"We use Sonatype -> Maven Central repository to manage the release artifacts. Please follow How-to-release page to understand the complete steps required to release a Pravega/Flink connector version. Here is the gradle task that is used to publish the artifacts. The published artifacts can be found here for more information. ./gradlew clean assemble publishToRepo -PdoSigning=true -Psigning.password=<signing-password> -PpublishUrl=mavenCentral -PpublishUsername=<sonatype-username> -PpublishPassword=<sonatype-password>","title":"Publishing Release Artifacts"},{"location":"quickstart/","text":"Getting Started Creating a Flink Stream Processing Project Note : You can skip this step if you have a streaming project set up already. Please use the following project templates and setup guidelines, to set up a stream processing project with Apache Flink: Project template for Java Project template for Scala Once after the set up, please follow the below instructions to add the Flink Pravega connectors to the project. Add the Connector Dependencies To add the Pravega connector dependencies to your project, add the following entry to your project file: (For example, pom.xml for Maven) <dependency> <groupId>io.pravega</groupId> <artifactId>pravega-connectors-flink_2.11</artifactId> <version>0.3.2</version> </dependency> Use appropriate version as necessary. The snapshot versions are published to jcenter repository and the release artifacts are available in Maven Central repository. Alternatively, we could build and publish the connector project to local maven repository by following the below steps and make use of that version as your application dependency. ./gradlew clean install Running / Deploying the Application From Flink's perspective, the connector to Pravega is part of the streaming application (not part of Flink's core runtime), so the connector code must be part of the application's code artifact (JAR file). Typically, a Flink application is bundled as a fat-jar (also known as an uber-jar ) , such that all its dependencies are embedded. The project set up should have been a success, if you have used the above linked templates/guides . If you set up a application's project and dependencies manually, you need to make sure that it builds a jar with dependencies , to include both the application and the connector classes.","title":"Quick Start"},{"location":"quickstart/#getting-started","text":"","title":"Getting Started"},{"location":"quickstart/#creating-a-flink-stream-processing-project","text":"Note : You can skip this step if you have a streaming project set up already. Please use the following project templates and setup guidelines, to set up a stream processing project with Apache Flink: Project template for Java Project template for Scala Once after the set up, please follow the below instructions to add the Flink Pravega connectors to the project.","title":"Creating a Flink Stream Processing Project"},{"location":"quickstart/#add-the-connector-dependencies","text":"To add the Pravega connector dependencies to your project, add the following entry to your project file: (For example, pom.xml for Maven) <dependency> <groupId>io.pravega</groupId> <artifactId>pravega-connectors-flink_2.11</artifactId> <version>0.3.2</version> </dependency> Use appropriate version as necessary. The snapshot versions are published to jcenter repository and the release artifacts are available in Maven Central repository. Alternatively, we could build and publish the connector project to local maven repository by following the below steps and make use of that version as your application dependency. ./gradlew clean install","title":"Add the Connector Dependencies"},{"location":"quickstart/#running-deploying-the-application","text":"From Flink's perspective, the connector to Pravega is part of the streaming application (not part of Flink's core runtime), so the connector code must be part of the application's code artifact (JAR file). Typically, a Flink application is bundled as a fat-jar (also known as an uber-jar ) , such that all its dependencies are embedded. The project set up should have been a success, if you have used the above linked templates/guides . If you set up a application's project and dependencies manually, you need to make sure that it builds a jar with dependencies , to include both the application and the connector classes.","title":"Running / Deploying the Application"},{"location":"serialization/","text":"Serialization Serialization refers to converting a data element in your Flink program to/from a message in a Pravega stream. Flink defines a standard interface for data serialization to/from byte messages delivered by various connectors. The core interfaces are: - org.apache.flink.streaming.util.serialization.SerializationSchema - org.apache.flink.streaming.util.serialization.DeserializationSchema In-built serializers include: - org.apache.flink.streaming.util.serialization.SimpleStringSchema - org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema The Pravega connector is designed to use Flink's serialization interfaces. For example, to read each stream event as a UTF-8 string: DeserializationSchema < String > schema = new SimpleStringSchema (); FlinkPravegaReader < String > reader = new FlinkPravegaReader <>(..., schema ); DataStream < MyEvent > stream = env . addSource ( reader ); Interoperability with Other Applications A common scenario is using Flink to process Pravega stream data produced by a non-Flink application. The Pravega client library used by such applications defines the io.pravega.client.stream.Serializer interface for working with event data. The implementations of Serializer directly in a Flink program via built-in adapters can be used: - io.pravega.connectors.flink.serialization.PravegaSerializationSchema - io.pravega.connectors.flink.serialization.PravegaDeserializationSchema Below is an example, to pass an instance of the appropriate Pravega de/serializer class to the adapter's constructor: import io.pravega.client.stream.impl.JavaSerializer ; ... DeserializationSchema < MyEvent > adapter = new PravegaDeserializationSchema <>( MyEvent . class , new JavaSerializer < MyEvent >()); FlinkPravegaReader < MyEvent > reader = new FlinkPravegaReader <>(..., adapter ); DataStream < MyEvent > stream = env . addSource ( reader ); Note that the Pravega serializer must implement java.io.Serializable to be usable in a Flink program.","title":"Serialization"},{"location":"serialization/#serialization","text":"Serialization refers to converting a data element in your Flink program to/from a message in a Pravega stream. Flink defines a standard interface for data serialization to/from byte messages delivered by various connectors. The core interfaces are: - org.apache.flink.streaming.util.serialization.SerializationSchema - org.apache.flink.streaming.util.serialization.DeserializationSchema In-built serializers include: - org.apache.flink.streaming.util.serialization.SimpleStringSchema - org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema The Pravega connector is designed to use Flink's serialization interfaces. For example, to read each stream event as a UTF-8 string: DeserializationSchema < String > schema = new SimpleStringSchema (); FlinkPravegaReader < String > reader = new FlinkPravegaReader <>(..., schema ); DataStream < MyEvent > stream = env . addSource ( reader );","title":"Serialization"},{"location":"serialization/#interoperability-with-other-applications","text":"A common scenario is using Flink to process Pravega stream data produced by a non-Flink application. The Pravega client library used by such applications defines the io.pravega.client.stream.Serializer interface for working with event data. The implementations of Serializer directly in a Flink program via built-in adapters can be used: - io.pravega.connectors.flink.serialization.PravegaSerializationSchema - io.pravega.connectors.flink.serialization.PravegaDeserializationSchema Below is an example, to pass an instance of the appropriate Pravega de/serializer class to the adapter's constructor: import io.pravega.client.stream.impl.JavaSerializer ; ... DeserializationSchema < MyEvent > adapter = new PravegaDeserializationSchema <>( MyEvent . class , new JavaSerializer < MyEvent >()); FlinkPravegaReader < MyEvent > reader = new FlinkPravegaReader <>(..., adapter ); DataStream < MyEvent > stream = env . addSource ( reader ); Note that the Pravega serializer must implement java.io.Serializable to be usable in a Flink program.","title":"Interoperability with Other Applications"},{"location":"streaming/","text":"Streaming Connector The Flink connector library for Pravega provides a data source and data sink for use with the Flink Streaming API. See the below sections for details. Table of Contents FlinkPravegaReader Parameters Input Stream(s) Parallelism Checkpointing Timestamp Extraction / Watermark Emission Stream Cuts Historical Stream Processing FlinkPravegaWriter Parameters Parallelism Event Routing Event Time Ordering Writer Modes Metrics Data Serialization FlinkPravegaReader A Pravega Stream may be used as a data source within a Flink streaming program using an instance of io.pravega.connectors.flink.FlinkPravegaReader . The reader reads a given Pravega Stream (or multiple streams) as a DataStream (the basic abstraction of the Flink Streaming API). Open a Pravega Stream as a DataStream using the method StreamExecutionEnvironment::addSource . Example StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < MyClass > deserializer = ... // Define the data stream FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream < MyClass > stream = env . addSource ( pravegaSource ); Parameters A builder API is provided to construct an instance of FlinkPravegaReader . See the table below for a summary of builder properties. Note that, the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. withReaderGroupScope The scope to store the Reader Group synchronization stream into. withReaderGroupName The Reader Group name for display purposes. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default. Input Stream(s) Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The FlinkPravegaReader is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Parallelism The FlinkPravegaReader supports parallelization. Use the setParallelism method to of Datastream to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note: Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The Synchronizer creates a backing stream that may be manually deleted after the completion of the job. Checkpointing In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution. The reader is compatible with Flink checkpoints and savepoints. The reader automatically recovers from failure by rewinding to the checkpointed position in the stream. A savepoint is self-contained; it contains all information needed to resume from the correct position. The checkpoint mechanism works as a two-step process: The master hook handler from the job manager initiates the triggerCheckpoint request to the ReaderCheckpointHook that was registered with the Job Manager during FlinkPravegaReader source initialization. The ReaderCheckpointHook handler notifies Pravega to checkpoint the current reader state. This is a non-blocking call which returns a future once Pravega readers are done with the checkpointing. A CheckPoint event will be sent by Pravega as part of the data stream flow and on receiving the event, the FlinkPravegaReader will initiate triggerCheckpoint request to effectively let Flink continue and complete the checkpoint process. Timestamp Extraction / Watermark Emission Flink requires the events\u2019 timestamps (each element in the stream needs to have its event timestamp assigned). This is achieved by accessing/extracting the timestamp from some field in the element. These are used to tell the system about progress in event time. Pravega is not aware (or does not track) of event time and does not store event timestamps or watermarks. Nonetheless it is possible to use event time semantics via an application-specific timestamp assigner and watermark generator as described in Flink documentation . Specify an AssignerWithPeriodicWatermarks or AssignerWithPunctuatedWatermarks on the DataStream as normal. Each parallel instance of the source processes one or more stream segments in parallel. Each watermark generator instance will receive events multiplexed from numerous segments. Be aware that segments are processed in parallel, and that no effort is made to order the events across segments in terms of their event time. Also, a given segment may be reassigned to another parallel instance at any time, preserving exactly-once behavior but causing further spread in observed event times. StreamCuts A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The FlinkPravegaReader accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . Historical Stream Processing Historical processing refers to processing stream data from a specific position in the stream rather than from the stream's tail. The builder API provides an overloaded method forStream that accepts a StreamCut parameter for this purpose. FlinkPravegaWriter A Pravega Stream may be used as a data sink within a Flink program using an instance of io.pravega.connectors.flink.FlinkPravegaWriter . Add an instance of the writer to the dataflow program using the method DataStream::addSink . Example StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < MyClass > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < MyClass > router = ... // Define the sink function FlinkPravegaWriter < MyClass > pravegaSink = FlinkPravegaWriter .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . withWriterMode ( EXACTLY_ONCE ) . build (); DataStream < MyClass > stream = ... stream . addSink ( pravegaSink ); Parameters A builder API is provided to construct an instance of FlinkPravegaWriter . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort, _At-least-once , or Exactly-once guarantees. withTxnLeaseRenewalPeriod The Transaction lease renewal period that supports the Exactly-once writer mode. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default. Parallelism FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute. Event Routing Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. When constructing the FlinkPravegaWriter , please provide an implementation of io.pravega.connectors.flink.PravegaEventRouter which will guarantee the event ordering. In Pravega, events are guaranteed to be ordered at the segment level. Event Time Ordering For programs that use Flink's event time semantics, the connector library supports writing events in event time order. In combination with a Routing Key, this establishes a well-understood ordering for each key in the output stream. Use the method FlinkPravegaUtils::writeToPravegaInEventTimeOrder to write a given DataStream to a Pravega Stream such that events are automatically ordered by event time (on a per-key basis). Refer here for sample code. Writer Modes Writer modes relate to guarantees about the persistence of events emitted by the sink to a Pravega Stream. The writer supports three writer modes: 1. Best-effort - Any write failures will be ignored hence there could be data loss. 2. At-least-once - All events are persisted in Pravega. Duplicate events are possible, due to retries or in case of failure and subsequent recovery. 3. Exactly-once - All events are persisted in Pravega using a transactional approach integrated with the Flink checkpointing feature. By default, the At-least-once option is enabled and use .withWriterMode(...) option to override the value. See the Pravega documentation for details on transactional behavior. Metrics Metrics are reported by default unless it is explicitly disabled using enableMetrics(false) option. See Metrics page for more details on type of metrics that are reported._ Serialization See the serialization page for more information on how to use the serializer and deserializer .","title":"Streaming"},{"location":"streaming/#streaming-connector","text":"The Flink connector library for Pravega provides a data source and data sink for use with the Flink Streaming API. See the below sections for details.","title":"Streaming Connector"},{"location":"streaming/#table-of-contents","text":"FlinkPravegaReader Parameters Input Stream(s) Parallelism Checkpointing Timestamp Extraction / Watermark Emission Stream Cuts Historical Stream Processing FlinkPravegaWriter Parameters Parallelism Event Routing Event Time Ordering Writer Modes Metrics Data Serialization","title":"Table of Contents"},{"location":"streaming/#flinkpravegareader","text":"A Pravega Stream may be used as a data source within a Flink streaming program using an instance of io.pravega.connectors.flink.FlinkPravegaReader . The reader reads a given Pravega Stream (or multiple streams) as a DataStream (the basic abstraction of the Flink Streaming API). Open a Pravega Stream as a DataStream using the method StreamExecutionEnvironment::addSource .","title":"FlinkPravegaReader"},{"location":"streaming/#example","text":"StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema < MyClass > deserializer = ... // Define the data stream FlinkPravegaReader < MyClass > pravegaSource = FlinkPravegaReader .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream < MyClass > stream = env . addSource ( pravegaSource );","title":"Example"},{"location":"streaming/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaReader . See the table below for a summary of builder properties. Note that, the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. withReaderGroupScope The scope to store the Reader Group synchronization stream into. withReaderGroupName The Reader Group name for display purposes. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default.","title":"Parameters"},{"location":"streaming/#input-streams","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The FlinkPravegaReader is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Input Stream(s)"},{"location":"streaming/#parallelism","text":"The FlinkPravegaReader supports parallelization. Use the setParallelism method to of Datastream to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note: Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The Synchronizer creates a backing stream that may be manually deleted after the completion of the job.","title":"Parallelism"},{"location":"streaming/#checkpointing","text":"In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution. The reader is compatible with Flink checkpoints and savepoints. The reader automatically recovers from failure by rewinding to the checkpointed position in the stream. A savepoint is self-contained; it contains all information needed to resume from the correct position. The checkpoint mechanism works as a two-step process: The master hook handler from the job manager initiates the triggerCheckpoint request to the ReaderCheckpointHook that was registered with the Job Manager during FlinkPravegaReader source initialization. The ReaderCheckpointHook handler notifies Pravega to checkpoint the current reader state. This is a non-blocking call which returns a future once Pravega readers are done with the checkpointing. A CheckPoint event will be sent by Pravega as part of the data stream flow and on receiving the event, the FlinkPravegaReader will initiate triggerCheckpoint request to effectively let Flink continue and complete the checkpoint process.","title":"Checkpointing"},{"location":"streaming/#timestamp-extraction-watermark-emission","text":"Flink requires the events\u2019 timestamps (each element in the stream needs to have its event timestamp assigned). This is achieved by accessing/extracting the timestamp from some field in the element. These are used to tell the system about progress in event time. Pravega is not aware (or does not track) of event time and does not store event timestamps or watermarks. Nonetheless it is possible to use event time semantics via an application-specific timestamp assigner and watermark generator as described in Flink documentation . Specify an AssignerWithPeriodicWatermarks or AssignerWithPunctuatedWatermarks on the DataStream as normal. Each parallel instance of the source processes one or more stream segments in parallel. Each watermark generator instance will receive events multiplexed from numerous segments. Be aware that segments are processed in parallel, and that no effort is made to order the events across segments in terms of their event time. Also, a given segment may be reassigned to another parallel instance at any time, preserving exactly-once behavior but causing further spread in observed event times.","title":"Timestamp Extraction / Watermark Emission"},{"location":"streaming/#streamcuts","text":"A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The FlinkPravegaReader accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code .","title":"StreamCuts"},{"location":"streaming/#historical-stream-processing","text":"Historical processing refers to processing stream data from a specific position in the stream rather than from the stream's tail. The builder API provides an overloaded method forStream that accepts a StreamCut parameter for this purpose.","title":"Historical Stream Processing"},{"location":"streaming/#flinkpravegawriter","text":"A Pravega Stream may be used as a data sink within a Flink program using an instance of io.pravega.connectors.flink.FlinkPravegaWriter . Add an instance of the writer to the dataflow program using the method DataStream::addSink .","title":"FlinkPravegaWriter"},{"location":"streaming/#example_1","text":"StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema < MyClass > serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter < MyClass > router = ... // Define the sink function FlinkPravegaWriter < MyClass > pravegaSink = FlinkPravegaWriter .< MyClass > builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . withWriterMode ( EXACTLY_ONCE ) . build (); DataStream < MyClass > stream = ... stream . addSink ( pravegaSink );","title":"Example"},{"location":"streaming/#parameters_1","text":"A builder API is provided to construct an instance of FlinkPravegaWriter . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort, _At-least-once , or Exactly-once guarantees. withTxnLeaseRenewalPeriod The Transaction lease renewal period that supports the Exactly-once writer mode. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default.","title":"Parameters"},{"location":"streaming/#parallelism_1","text":"FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute.","title":"Parallelism"},{"location":"streaming/#event-routing","text":"Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. When constructing the FlinkPravegaWriter , please provide an implementation of io.pravega.connectors.flink.PravegaEventRouter which will guarantee the event ordering. In Pravega, events are guaranteed to be ordered at the segment level.","title":"Event Routing"},{"location":"streaming/#event-time-ordering","text":"For programs that use Flink's event time semantics, the connector library supports writing events in event time order. In combination with a Routing Key, this establishes a well-understood ordering for each key in the output stream. Use the method FlinkPravegaUtils::writeToPravegaInEventTimeOrder to write a given DataStream to a Pravega Stream such that events are automatically ordered by event time (on a per-key basis). Refer here for sample code.","title":"Event Time Ordering"},{"location":"streaming/#writer-modes","text":"Writer modes relate to guarantees about the persistence of events emitted by the sink to a Pravega Stream. The writer supports three writer modes: 1. Best-effort - Any write failures will be ignored hence there could be data loss. 2. At-least-once - All events are persisted in Pravega. Duplicate events are possible, due to retries or in case of failure and subsequent recovery. 3. Exactly-once - All events are persisted in Pravega using a transactional approach integrated with the Flink checkpointing feature. By default, the At-least-once option is enabled and use .withWriterMode(...) option to override the value. See the Pravega documentation for details on transactional behavior.","title":"Writer Modes"},{"location":"streaming/#metrics","text":"Metrics are reported by default unless it is explicitly disabled using enableMetrics(false) option. See Metrics page for more details on type of metrics that are reported._","title":"Metrics"},{"location":"streaming/#serialization","text":"See the serialization page for more information on how to use the serializer and deserializer .","title":"Serialization"},{"location":"table-api/","text":"Table Connector The Flink connector library for Pravega provides a table source and table sink for use with the Flink Table API. The Table API provides a unified API for both the Flink streaming and batch environment. See the below sections for details. Table of Contents Table Source Parameters Custom Formats Time Attribute Support Table Sink Parameters Custom Formats Table Source A Pravega Stream may be used as a table source within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSource is then used to parse raw stream data as Row objects that conform to the table schema. The connector library provides out-of-box support for JSON-formatted data with FlinkPravegaJsonTableSource , and may be extended to support other formats. Example The following example uses the provided table source to read JSON-formatted events from a Pravega Stream: // Create a Flink Table environment ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); String [] fieldNames = { \"user\" , \"uri\" , \"accessTime\" }; // Read data from the stream using Table reader TableSchema tableSchema = TableSchema . builder () . field ( \"user\" , Types . STRING ()) . field ( \"uri\" , Types . STRING ()) . field ( \"accessTime\" , Types . SQL_TIMESTAMP ()) . build (); FlinkPravegaJsonTableSource source = FlinkPravegaJsonTableSource . builder () . forStream ( stream ) . withPravegaConfig ( pravegaConfig ) . failOnMissingField ( true ) . withRowtimeAttribute ( \"accessTime\" , new ExistingField ( \"accessTime\" ), new BoundedOutOfOrderTimestamps ( 30000L )) . withSchema ( tableSchema ) . withReaderGroupScope ( stream . getScope ()) . build (); // (Option-1) Read table as stream data StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( \"MyTableRow\" , source ); String sqlQuery = \"SELECT user, count(uri) from MyTableRow GROUP BY user\" ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... // (Option-2) Read table as batch data (use tumbling window as part of the query) BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( \"MyTableRow\" , source ); String sqlQuery = \"SELECT user, \" + \"TUMBLE_END(accessTime, INTERVAL '5' MINUTE) AS accessTime, \" + \"COUNT(uri) AS cnt \" + \"from MyTableRow GROUP BY \" + \"user, TUMBLE(accessTime, INTERVAL '5' MINUTE)\" ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... Parameters A builder API is provided to construct an instance of FlinkPravegaJsonTableSource . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table source supports both the Flink streaming and batch environments . In the streaming environment, the table source uses a FlinkPravegaReader connector. In the batch environment, the table source uses a FlinkPravegaInputFormat connectors. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. Applies only to streaming API. withReaderGroupScope The scope to store the Reader Group synchronization stream into. Applies only to streaming API. withReaderGroupName The Reader Group name for display purposes. Applies only to streaming API. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. Applies only to streaming API. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. Applies only to streaming API. withSchema The table schema which describes which JSON fields to expect. withProctimeAttribute The name of the processing time attribute in the supplied table schema. withRowTimeAttribute supply the name of the rowtime attribute in the table schema, a TimeStampExtractor instance to extract the rowtime attribute value from the event and a WaterMarkStratergy to generate watermarks for the rowtime attribute. failOnMissingField A flag indicating whether to fail if a JSON field is missing. Custom Formats To work with stream events in a format other than JSON, extend FlinkPravegaTableSource . Please see the implementation of FlinkPravegaJsonTableSource for more details. Time Attribute Support With the use of withProctimeAttribute or withRowTimeAttribute builder method, one could supply the time attribute information of the event. The configured field must be present in the table schema and of type Types.SQL_TIMESTAMP() . Table Sink A Pravega Stream may be used as an append-only table within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSink is then used to write table rows to a Pravega Stream in a particular format. The connector library provides out-of-box support for JSON-formatted data with FlinkPravegaJsonTableSource , and may be extended to support other formats. Example The following example uses the provided table sink to write JSON-formatted events to a Pravega Stream: // Create a Flink Table environment StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( ParameterTool . fromArgs ( args )); // Define a table (see Flink documentation) Table table = ... // Write the table to a Pravega Stream FlinkPravegaJsonTableSink sink = FlinkPravegaJsonTableSink . builder () . forStream ( \"sensor_stream\" ) . withPravegaConfig ( config ) . withRoutingKeyField ( \"sensor_id\" ) . withWriterMode ( EXACTLY_ONCE ) . build (); table . writeToSink ( sink ); Parameters A builder API is provided to construct an instance of FlinkPravegaJsonTableSink . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations wiki page for more information. Note that the table sink supports both the Flink streaming and batch environments. In the streaming environment, the table sink uses a FlinkPravegaWriter connector. In the batch environment, the table sink uses a FlinkPravegaOutputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort , At-least-once , or Exactly-once guarantees. withTxnTimeout The timeout for the Pravega Tansaction that supports the exactly-once writer mode. withSchema The table schema which describes which JSON fields to expect. withRoutingKeyField The table field to use as the Routing Key for written events. Custom Formats To work with stream events in a format other than JSON, extend FlinkPravegaTableSink . Please see the implementation of FlinkPravegaJsonTableSink for more details.","title":"Table API"},{"location":"table-api/#table-connector","text":"The Flink connector library for Pravega provides a table source and table sink for use with the Flink Table API. The Table API provides a unified API for both the Flink streaming and batch environment. See the below sections for details.","title":"Table Connector"},{"location":"table-api/#table-of-contents","text":"Table Source Parameters Custom Formats Time Attribute Support Table Sink Parameters Custom Formats","title":"Table of Contents"},{"location":"table-api/#table-source","text":"A Pravega Stream may be used as a table source within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSource is then used to parse raw stream data as Row objects that conform to the table schema. The connector library provides out-of-box support for JSON-formatted data with FlinkPravegaJsonTableSource , and may be extended to support other formats.","title":"Table Source"},{"location":"table-api/#example","text":"The following example uses the provided table source to read JSON-formatted events from a Pravega Stream: // Create a Flink Table environment ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); String [] fieldNames = { \"user\" , \"uri\" , \"accessTime\" }; // Read data from the stream using Table reader TableSchema tableSchema = TableSchema . builder () . field ( \"user\" , Types . STRING ()) . field ( \"uri\" , Types . STRING ()) . field ( \"accessTime\" , Types . SQL_TIMESTAMP ()) . build (); FlinkPravegaJsonTableSource source = FlinkPravegaJsonTableSource . builder () . forStream ( stream ) . withPravegaConfig ( pravegaConfig ) . failOnMissingField ( true ) . withRowtimeAttribute ( \"accessTime\" , new ExistingField ( \"accessTime\" ), new BoundedOutOfOrderTimestamps ( 30000L )) . withSchema ( tableSchema ) . withReaderGroupScope ( stream . getScope ()) . build (); // (Option-1) Read table as stream data StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( \"MyTableRow\" , source ); String sqlQuery = \"SELECT user, count(uri) from MyTableRow GROUP BY user\" ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... // (Option-2) Read table as batch data (use tumbling window as part of the query) BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( \"MyTableRow\" , source ); String sqlQuery = \"SELECT user, \" + \"TUMBLE_END(accessTime, INTERVAL '5' MINUTE) AS accessTime, \" + \"COUNT(uri) AS cnt \" + \"from MyTableRow GROUP BY \" + \"user, TUMBLE(accessTime, INTERVAL '5' MINUTE)\" ; Table result = tableEnv . sqlQuery ( sqlQuery ); ...","title":"Example"},{"location":"table-api/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaJsonTableSource . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table source supports both the Flink streaming and batch environments . In the streaming environment, the table source uses a FlinkPravegaReader connector. In the batch environment, the table source uses a FlinkPravegaInputFormat connectors. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. Applies only to streaming API. withReaderGroupScope The scope to store the Reader Group synchronization stream into. Applies only to streaming API. withReaderGroupName The Reader Group name for display purposes. Applies only to streaming API. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. Applies only to streaming API. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. Applies only to streaming API. withSchema The table schema which describes which JSON fields to expect. withProctimeAttribute The name of the processing time attribute in the supplied table schema. withRowTimeAttribute supply the name of the rowtime attribute in the table schema, a TimeStampExtractor instance to extract the rowtime attribute value from the event and a WaterMarkStratergy to generate watermarks for the rowtime attribute. failOnMissingField A flag indicating whether to fail if a JSON field is missing.","title":"Parameters"},{"location":"table-api/#custom-formats","text":"To work with stream events in a format other than JSON, extend FlinkPravegaTableSource . Please see the implementation of FlinkPravegaJsonTableSource for more details.","title":"Custom Formats"},{"location":"table-api/#time-attribute-support","text":"With the use of withProctimeAttribute or withRowTimeAttribute builder method, one could supply the time attribute information of the event. The configured field must be present in the table schema and of type Types.SQL_TIMESTAMP() .","title":"Time Attribute Support"},{"location":"table-api/#table-sink","text":"A Pravega Stream may be used as an append-only table within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSink is then used to write table rows to a Pravega Stream in a particular format. The connector library provides out-of-box support for JSON-formatted data with FlinkPravegaJsonTableSource , and may be extended to support other formats.","title":"Table Sink"},{"location":"table-api/#example_1","text":"The following example uses the provided table sink to write JSON-formatted events to a Pravega Stream: // Create a Flink Table environment StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( ParameterTool . fromArgs ( args )); // Define a table (see Flink documentation) Table table = ... // Write the table to a Pravega Stream FlinkPravegaJsonTableSink sink = FlinkPravegaJsonTableSink . builder () . forStream ( \"sensor_stream\" ) . withPravegaConfig ( config ) . withRoutingKeyField ( \"sensor_id\" ) . withWriterMode ( EXACTLY_ONCE ) . build (); table . writeToSink ( sink );","title":"Example"},{"location":"table-api/#parameters_1","text":"A builder API is provided to construct an instance of FlinkPravegaJsonTableSink . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations wiki page for more information. Note that the table sink supports both the Flink streaming and batch environments. In the streaming environment, the table sink uses a FlinkPravegaWriter connector. In the batch environment, the table sink uses a FlinkPravegaOutputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort , At-least-once , or Exactly-once guarantees. withTxnTimeout The timeout for the Pravega Tansaction that supports the exactly-once writer mode. withSchema The table schema which describes which JSON fields to expect. withRoutingKeyField The table field to use as the Routing Key for written events.","title":"Parameters"},{"location":"table-api/#custom-formats_1","text":"To work with stream events in a format other than JSON, extend FlinkPravegaTableSink . Please see the implementation of FlinkPravegaJsonTableSink for more details.","title":"Custom Formats"}]}