{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Flink Connectors for Pravega This documentation describes the connectors API and it's usage to read and write Pravega streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. See the Pravega Concepts page for more information. Table of Contents Getting Started Quick Start Features Streaming Batch Table API/SQL Metrics Configurations Serialization","title":"Overview"},{"location":"#apache-flink-connectors-for-pravega","text":"This documentation describes the connectors API and it's usage to read and write Pravega streams with Apache Flink stream processing framework. Build end-to-end stream processing pipelines that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. See the Pravega Concepts page for more information.","title":"Apache Flink Connectors for Pravega"},{"location":"#table-of-contents","text":"Getting Started Quick Start Features Streaming Batch Table API/SQL Metrics Configurations Serialization","title":"Table of Contents"},{"location":"batch/","text":"Batch Connector The Flink Connector library for Pravega makes it possible to use a Pravega Stream as a data source and data sink in a batch program. See the below sections for details. Table of Contents FlinkPravegaInputFormat Parameters Input Stream(s) StreamCuts Parallelism FlinkPravegaOutputFormat Parameters Output Stream Parallelism Event Routing Serialization FlinkPravegaInputFormat A Pravega Stream may be used as a data source within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaInputFormat . The input format reads events of a stream as a DataSet (the basic abstraction of the Flink Batch API). This input format opens the stream for batch reading, which processes stream segments in parallel and does not follow routing key order. Use the ExecutionEnvironment::createInput method to open a Pravega Stream as a DataSet . Example // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema EventType deserializer = ... // Define the input format based on a Pravega stream FlinkPravegaInputFormat EventType inputFormat = FlinkPravegaInputFormat . EventType builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataSource EventType dataSet = env . createInput ( inputFormat , TypeInformation . of ( EventType . class ) . setParallelism ( 2 ); Parameters A builder API is provided to construct an instance of FlinkPravegaInputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. Input Stream(s) Each Pravega stream exists within a scope. A scope defines a namespace for streams such that names are unique. Across scopes, streams can have the same name. For example, if we have scopes A and B , then we can have a stream called myStream in each one of them. We cannot have a stream with the same name in the same scope. The builder API accepts both qualified and unqualified stream names. In qualified stream names, the scope is explicitly specified, e.g. my-scope/my-stream . In unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . See the configurations page for more information on default scope. A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Multiple streams can be passed as parameter option (using the builder API). The BatchClient implementation is capable of reading from numerous streams in parallel, even across scopes. StreamCuts A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The BatchClient accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . If stream cuts are not provided then the default start position requested is assumed to be the earliest available data in the stream and the default end position is assumed to be all available data in that stream as of when the job execution begins. Parallelism FlinkPravegaInputFormat supports parallelization. Use the setParallelism method of DataSet to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. FlinkPravegaOutputFormat A Pravega Stream may be used as a data sink within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaOutputFormat . The FlinkPravegaOutputFormat can be supplied as a sink to the DataSet (the basic abstraction of the Flink Batch API). Example // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema EventType serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter EventType router = ... // Define the input format based on a Pravega Stream FlinkPravegaOutputFormat EventType outputFormat = FlinkPravegaOutputFormat . EventType builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); Collection EventType inputData = Arrays . asList (...); env . fromCollection ( inputData ) . output ( outputFormat ); env . execute ( ... ); Parameter A builder API is provided to construct an instance of FlinkPravegaOutputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. Output Stream Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Parallelism FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute. Event Routing Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. To establish the routing key for each event, provide an implementation of io.pravega.connectors.flink.PravegaEventRouter when constructing the writer. Serialization Please, see the serialization page for more information on how to use the serializer and deserializer .","title":"Batch"},{"location":"batch/#batch-connector","text":"The Flink Connector library for Pravega makes it possible to use a Pravega Stream as a data source and data sink in a batch program. See the below sections for details.","title":"Batch Connector"},{"location":"batch/#table-of-contents","text":"FlinkPravegaInputFormat Parameters Input Stream(s) StreamCuts Parallelism FlinkPravegaOutputFormat Parameters Output Stream Parallelism Event Routing Serialization","title":"Table of Contents"},{"location":"batch/#flinkpravegainputformat","text":"A Pravega Stream may be used as a data source within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaInputFormat . The input format reads events of a stream as a DataSet (the basic abstraction of the Flink Batch API). This input format opens the stream for batch reading, which processes stream segments in parallel and does not follow routing key order. Use the ExecutionEnvironment::createInput method to open a Pravega Stream as a DataSet .","title":"FlinkPravegaInputFormat"},{"location":"batch/#example","text":"// Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema EventType deserializer = ... // Define the input format based on a Pravega stream FlinkPravegaInputFormat EventType inputFormat = FlinkPravegaInputFormat . EventType builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataSource EventType dataSet = env . createInput ( inputFormat , TypeInformation . of ( EventType . class ) . setParallelism ( 2 );","title":"Example"},{"location":"batch/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaInputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events.","title":"Parameters"},{"location":"batch/#input-streams","text":"Each Pravega stream exists within a scope. A scope defines a namespace for streams such that names are unique. Across scopes, streams can have the same name. For example, if we have scopes A and B , then we can have a stream called myStream in each one of them. We cannot have a stream with the same name in the same scope. The builder API accepts both qualified and unqualified stream names. In qualified stream names, the scope is explicitly specified, e.g. my-scope/my-stream . In unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . See the configurations page for more information on default scope. A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Multiple streams can be passed as parameter option (using the builder API). The BatchClient implementation is capable of reading from numerous streams in parallel, even across scopes.","title":"Input Stream(s)"},{"location":"batch/#streamcuts","text":"A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The BatchClient accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . If stream cuts are not provided then the default start position requested is assumed to be the earliest available data in the stream and the default end position is assumed to be all available data in that stream as of when the job execution begins.","title":"StreamCuts"},{"location":"batch/#parallelism","text":"FlinkPravegaInputFormat supports parallelization. Use the setParallelism method of DataSet to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments.","title":"Parallelism"},{"location":"batch/#flinkpravegaoutputformat","text":"A Pravega Stream may be used as a data sink within a Flink batch program using an instance of io.pravega.connectors.flink.FlinkPravegaOutputFormat . The FlinkPravegaOutputFormat can be supplied as a sink to the DataSet (the basic abstraction of the Flink Batch API).","title":"FlinkPravegaOutputFormat"},{"location":"batch/#example_1","text":"// Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema EventType serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter EventType router = ... // Define the input format based on a Pravega Stream FlinkPravegaOutputFormat EventType outputFormat = FlinkPravegaOutputFormat . EventType builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . build (); ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); Collection EventType inputData = Arrays . asList (...); env . fromCollection ( inputData ) . output ( outputFormat ); env . execute ( ... );","title":"Example"},{"location":"batch/#parameter","text":"A builder API is provided to construct an instance of FlinkPravegaOutputFormat . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event.","title":"Parameter"},{"location":"batch/#output-stream","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Output Stream"},{"location":"batch/#parallelism_1","text":"FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute.","title":"Parallelism"},{"location":"batch/#event-routing","text":"Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. To establish the routing key for each event, provide an implementation of io.pravega.connectors.flink.PravegaEventRouter when constructing the writer.","title":"Event Routing"},{"location":"batch/#serialization","text":"Please, see the serialization page for more information on how to use the serializer and deserializer .","title":"Serialization"},{"location":"configurations/","text":"Configurations The Flink connector library for Pravega supports the Flink Streaming API , Table API and Batch API , using a common configuration class. Table of Contents Common Configuration PravegaConfig Class Creating PravegaConfig Using PravegaConfig Understanding the Default Scope Common Configuration PravegaConfig Class A top-level config object, PravegaConfig , is provided to establish a Pravega context for the Flink connector. The config object automatically configures itself from environment variables , system properties and program arguments . PravegaConfig information sources is given below: Setting Environment Variable / System Property / Program Argument Default Value Controller URI PRAVEGA_CONTROLLER_URI pravega.controller.uri --controller tcp://localhost:9090 Default Scope PRAVEGA_SCOPE pravega.scope --scope - Credentials - - Hostname Validation - true Creating PravegaConfig The recommended way to create an instance of PravegaConfig is to pass an instance of ParameterTool to fromParams : ParameterTool params = ParameterTool . fromArgs ( args ); PravegaConfig config = PravegaConfig . fromParams ( params ); If your application doesn't use the ParameterTool class that is provided by Flink, create the PravegaConfig using fromDefaults : PravegaConfig config = PravegaConfig . fromDefaults (); The PravegaConfig class provides a builder-style API to override the default configuration settings: PravegaConfig config = PravegaConfig . fromDefaults () . withControllerURI ( tcp://... ) . withDefaultScope ( SCOPE-NAME ) . withCredentials ( credentials ) . withHostnameValidation ( false ); Using PravegaConfig All of the various source and sink classes provided with the connector library have a builder-style API which accepts a PravegaConfig for common configuration. Pass a PravegaConfig object to the respective builder via withPravegaConfig . For example, see below code: PravegaConfig config = ...; FlinkPravegaReader MyClass pravegaSource = FlinkPravegaReader . MyClass builder () . forStream (...) . withPravegaConfig ( config ) . build (); Understanding the Default Scope Pravega organizes streams into scopes for the purposes of manageability. The PravegaConfig establishes a default scope name that is used in two scenarios: For resolving unqualified stream names when constructing a source or sink. The sources and sinks accept stream names that may be qualified (e.g. my-scope/my-stream ) or unqualified (e.g. my-stream ). For establishing the scope name for the coordination stream underlying a Pravega Reader Group. It is important to note that, the FlinkPravegaReader and the FlinkPravegaTableSource use the default scope configured on PravegaConfig as their Reader Group scope, and provide withReaderGroupScope as an override. The scope name of input stream(s) doesn't influence the Reader Group scope.","title":"Configurations"},{"location":"configurations/#configurations","text":"The Flink connector library for Pravega supports the Flink Streaming API , Table API and Batch API , using a common configuration class.","title":"Configurations"},{"location":"configurations/#table-of-contents","text":"Common Configuration PravegaConfig Class Creating PravegaConfig Using PravegaConfig Understanding the Default Scope","title":"Table of Contents"},{"location":"configurations/#common-configuration","text":"","title":"Common Configuration"},{"location":"configurations/#pravegaconfig-class","text":"A top-level config object, PravegaConfig , is provided to establish a Pravega context for the Flink connector. The config object automatically configures itself from environment variables , system properties and program arguments . PravegaConfig information sources is given below: Setting Environment Variable / System Property / Program Argument Default Value Controller URI PRAVEGA_CONTROLLER_URI pravega.controller.uri --controller tcp://localhost:9090 Default Scope PRAVEGA_SCOPE pravega.scope --scope - Credentials - - Hostname Validation - true","title":"PravegaConfig Class"},{"location":"configurations/#creating-pravegaconfig","text":"The recommended way to create an instance of PravegaConfig is to pass an instance of ParameterTool to fromParams : ParameterTool params = ParameterTool . fromArgs ( args ); PravegaConfig config = PravegaConfig . fromParams ( params ); If your application doesn't use the ParameterTool class that is provided by Flink, create the PravegaConfig using fromDefaults : PravegaConfig config = PravegaConfig . fromDefaults (); The PravegaConfig class provides a builder-style API to override the default configuration settings: PravegaConfig config = PravegaConfig . fromDefaults () . withControllerURI ( tcp://... ) . withDefaultScope ( SCOPE-NAME ) . withCredentials ( credentials ) . withHostnameValidation ( false );","title":"Creating PravegaConfig"},{"location":"configurations/#using-pravegaconfig","text":"All of the various source and sink classes provided with the connector library have a builder-style API which accepts a PravegaConfig for common configuration. Pass a PravegaConfig object to the respective builder via withPravegaConfig . For example, see below code: PravegaConfig config = ...; FlinkPravegaReader MyClass pravegaSource = FlinkPravegaReader . MyClass builder () . forStream (...) . withPravegaConfig ( config ) . build ();","title":"Using PravegaConfig"},{"location":"configurations/#understanding-the-default-scope","text":"Pravega organizes streams into scopes for the purposes of manageability. The PravegaConfig establishes a default scope name that is used in two scenarios: For resolving unqualified stream names when constructing a source or sink. The sources and sinks accept stream names that may be qualified (e.g. my-scope/my-stream ) or unqualified (e.g. my-stream ). For establishing the scope name for the coordination stream underlying a Pravega Reader Group. It is important to note that, the FlinkPravegaReader and the FlinkPravegaTableSource use the default scope configured on PravegaConfig as their Reader Group scope, and provide withReaderGroupScope as an override. The scope name of input stream(s) doesn't influence the Reader Group scope.","title":"Understanding the Default Scope"},{"location":"getting-started/","text":"Pravega Flink Connectors This repository implements connectors to read and write Pravega Streams with Apache Flink stream processing framework. The connectors can be used to build end-to-end stream processing pipelines (see Samples ) that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams. Features Highlights Exactly-once processing guarantees for both Reader and Writer, supporting end-to-end exactly-once processing pipelines Seamless integration with Flink's checkpoints and savepoints. Parallel Readers and Writers supporting high throughput and low latency processing. Table API support to access Pravega Streams for both Batch and Streaming use case. Building Connectors Building the connectors from the source is only necessary when we want to use or contribute to the latest ( unreleased ) version of the Pravega Flink connectors. The connector project is linked to a specific version of Pravega, based on a git submodule pointing to a commit-id. By default the sub-module option is disabled and the build step will make use of the Pravega version defined in the gradle.properties file. You could override this option by enabling usePravegaVersionSubModule flag in gradle.properties to true . Checkout the source code repository by following below steps: git clone --recursive https://github.com/pravega/flink-connectors.git After cloning the repository, the project can be built by running the below command in the project root directory flink-connectors . ./gradlew clean build To install the artifacts in the local maven repository cache ~/.m2/repository , run the following command: ./gradlew clean install Customizing the Build Building against a custom Flink version We can check and change the Flink version that Pravega builds against via the flinkVersion variable in the gradle.properties file. Note : Only Flink versions that are compatible with the latest connector code can be chosen. Building against another Scala version This section is only relevant if you use Scala in the stream processing application with Flink and Pravega. Parts of the Apache Flink use the language or depend on libraries written in Scala. Because Scala is not strictly compatible across versions, there exist different versions of Flink compiled for different Scala versions. If we use Scala code in the same application where we use the Apache Flink or the Flink connectors, we typically have to make sure we use a version of Flink that uses the same Scala version as our application. Each version of Flink has a preferred Scala version as determined by the official Flink docker image. We use the preferred version by default. To depend on released Flink artifacts for a different Scala version, you need to edit the build.gradle file and change all entries for the Flink dependencies to have a different Scala version suffix. For example, flink-streaming-java_2.11 would be replaced by flink-streaming-java_2.12 for Scala 2.12 . In order to build a new version of Flink for a different Scala version, please refer to the Flink documentation . Setting up your IDE Connector project uses Project Lombok , so we should ensure that we have our IDE setup with the required plugins. ( IntelliJ is recommended ). To import the source into IntelliJ: Import the project directory into IntelliJ IDE. It will automatically detect the gradle project and import things correctly. Enable Annotation Processing by going to Build, Execution, Deployment - Compiler Annotation Processors and checking Enable annotation processing . Install the Lombok Plugin . This can be found in Preferences - Plugins . Restart your IDE. Connectors project compiles properly after applying the above steps. For eclipse, we can generate eclipse project files by running ./gradlew eclipse . Releases The latest releases can be found on the Github Release project page. Support Don\u2019t hesitate to ask! Contact the developers and community on the Slack if you need any help. Open an issue if you found a bug on Github Issues . Samples Follow the Pravega Samples repository to learn more about how to build and use the Flink Connector library.","title":"Getting Started"},{"location":"getting-started/#pravega-flink-connectors","text":"This repository implements connectors to read and write Pravega Streams with Apache Flink stream processing framework. The connectors can be used to build end-to-end stream processing pipelines (see Samples ) that use Pravega as the stream storage and message bus, and Apache Flink for computation over the streams.","title":"Pravega Flink Connectors"},{"location":"getting-started/#features-highlights","text":"Exactly-once processing guarantees for both Reader and Writer, supporting end-to-end exactly-once processing pipelines Seamless integration with Flink's checkpoints and savepoints. Parallel Readers and Writers supporting high throughput and low latency processing. Table API support to access Pravega Streams for both Batch and Streaming use case.","title":"Features &amp; Highlights"},{"location":"getting-started/#building-connectors","text":"Building the connectors from the source is only necessary when we want to use or contribute to the latest ( unreleased ) version of the Pravega Flink connectors. The connector project is linked to a specific version of Pravega, based on a git submodule pointing to a commit-id. By default the sub-module option is disabled and the build step will make use of the Pravega version defined in the gradle.properties file. You could override this option by enabling usePravegaVersionSubModule flag in gradle.properties to true . Checkout the source code repository by following below steps: git clone --recursive https://github.com/pravega/flink-connectors.git After cloning the repository, the project can be built by running the below command in the project root directory flink-connectors . ./gradlew clean build To install the artifacts in the local maven repository cache ~/.m2/repository , run the following command: ./gradlew clean install","title":"Building Connectors"},{"location":"getting-started/#customizing-the-build","text":"","title":"Customizing the Build"},{"location":"getting-started/#building-against-a-custom-flink-version","text":"We can check and change the Flink version that Pravega builds against via the flinkVersion variable in the gradle.properties file. Note : Only Flink versions that are compatible with the latest connector code can be chosen.","title":"Building against a custom Flink version"},{"location":"getting-started/#building-against-another-scala-version","text":"This section is only relevant if you use Scala in the stream processing application with Flink and Pravega. Parts of the Apache Flink use the language or depend on libraries written in Scala. Because Scala is not strictly compatible across versions, there exist different versions of Flink compiled for different Scala versions. If we use Scala code in the same application where we use the Apache Flink or the Flink connectors, we typically have to make sure we use a version of Flink that uses the same Scala version as our application. Each version of Flink has a preferred Scala version as determined by the official Flink docker image. We use the preferred version by default. To depend on released Flink artifacts for a different Scala version, you need to edit the build.gradle file and change all entries for the Flink dependencies to have a different Scala version suffix. For example, flink-streaming-java_2.11 would be replaced by flink-streaming-java_2.12 for Scala 2.12 . In order to build a new version of Flink for a different Scala version, please refer to the Flink documentation .","title":"Building against another Scala version"},{"location":"getting-started/#setting-up-your-ide","text":"Connector project uses Project Lombok , so we should ensure that we have our IDE setup with the required plugins. ( IntelliJ is recommended ). To import the source into IntelliJ: Import the project directory into IntelliJ IDE. It will automatically detect the gradle project and import things correctly. Enable Annotation Processing by going to Build, Execution, Deployment - Compiler Annotation Processors and checking Enable annotation processing . Install the Lombok Plugin . This can be found in Preferences - Plugins . Restart your IDE. Connectors project compiles properly after applying the above steps. For eclipse, we can generate eclipse project files by running ./gradlew eclipse .","title":"Setting up your IDE"},{"location":"getting-started/#releases","text":"The latest releases can be found on the Github Release project page.","title":"Releases"},{"location":"getting-started/#support","text":"Don\u2019t hesitate to ask! Contact the developers and community on the Slack if you need any help. Open an issue if you found a bug on Github Issues .","title":"Support"},{"location":"getting-started/#samples","text":"Follow the Pravega Samples repository to learn more about how to build and use the Flink Connector library.","title":"Samples"},{"location":"metrics/","text":"Metrics Pravega metrics are collected and exposed via Flink metrics framework when using FlinkPravegaReader or FlinkPravegaWriter . Reader Metrics The following metrics are exposed for FlinkPravegaReader related operations: Name Description readerGroupName The name of the Reader Group. scope The scope name of the Reader Group. streams The fully qualified name (i.e., scope/stream ) of the streams that are part of the Reader Group. onlineReaders The readers that are currently online/available. segmentPositions The StreamCut information that indicates where the readers have read so far. unreadBytes The total number of bytes that have not been read yet. Writer Metrics For FlinkPravegaWriter related operations, only the stream name is exposed: Name Description streams The fully qualified name of the stream i.e., scope/stream Querying Metrics The metrics can be viewed either from Flink UI or using the Flink REST API (like below): curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . readerGroupName curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . scope curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . streams curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . onlineReaders curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . stream . test . segmentPositions curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . unreadBytes","title":"Metrics"},{"location":"metrics/#metrics","text":"Pravega metrics are collected and exposed via Flink metrics framework when using FlinkPravegaReader or FlinkPravegaWriter .","title":"Metrics"},{"location":"metrics/#reader-metrics","text":"The following metrics are exposed for FlinkPravegaReader related operations: Name Description readerGroupName The name of the Reader Group. scope The scope name of the Reader Group. streams The fully qualified name (i.e., scope/stream ) of the streams that are part of the Reader Group. onlineReaders The readers that are currently online/available. segmentPositions The StreamCut information that indicates where the readers have read so far. unreadBytes The total number of bytes that have not been read yet.","title":"Reader Metrics"},{"location":"metrics/#writer-metrics","text":"For FlinkPravegaWriter related operations, only the stream name is exposed: Name Description streams The fully qualified name of the stream i.e., scope/stream","title":"Writer Metrics"},{"location":"metrics/#querying-metrics","text":"The metrics can be viewed either from Flink UI or using the Flink REST API (like below): curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . readerGroupName curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . scope curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . streams curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . onlineReaders curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . stream . test . segmentPositions curl - i - s - f / jobs / JOB - ID / vertices / SOURCE - TASK - ID / metrics ? get = 0. Source__ SOURCE - OPERATOR - NAME . PravegaReader . readerGroup . unreadBytes","title":"Querying Metrics"},{"location":"quickstart/","text":"Getting Started Creating a Flink Stream Processing Project Note : You can skip this step if you have a streaming project set up already. Please use the following project templates and setup guidelines, to set up a stream processing project with Apache Flink using Connectors Project template for Java Project template for Scala Once after the set up, please follow the below instructions to add the Flink Pravega connectors to the project. Add the Connector Dependencies To add the Pravega connector dependencies to your project, add the following entry to your project file: (For example, pom.xml for Maven) !-- Before Pravega 0.6 -- dependency groupId io.pravega /groupId artifactId pravega-connectors-flink_2.12 /artifactId version 0.5.1 /version /dependency !-- Pravega 0.6 -- dependency groupId io.pravega /groupId artifactId pravega-connectors-flink-1.9_2.12 /artifactId version 0.6.0 /version /dependency Use appropriate version as necessary. 1.9 is the Flink Major-Minor version. 2.12 is the Scala version. 0.6.0 is the Pravega version. The snapshot versions are published to jcenter repository and the release artifacts are available in Maven Central repository. Alternatively, we could build and publish the connector artifacts to local maven repository by executing the following command and make use of that version as your application dependency. ./gradlew clean install Running / Deploying the Application From Flink's perspective, the connector to Pravega is part of the streaming application (not part of Flink's core runtime), so the connector code must be part of the application's code artifact (JAR file). Typically, a Flink application is bundled as a fat-jar (also known as an uber-jar ) , such that all its dependencies are embedded. The project set up should have been a success, if you have used the above linked templates/guides . If you set up a application's project and dependencies manually, you need to make sure that it builds a jar with dependencies , to include both the application and the connector classes.","title":"Quick Start"},{"location":"quickstart/#getting-started","text":"","title":"Getting Started"},{"location":"quickstart/#creating-a-flink-stream-processing-project","text":"Note : You can skip this step if you have a streaming project set up already. Please use the following project templates and setup guidelines, to set up a stream processing project with Apache Flink using Connectors Project template for Java Project template for Scala Once after the set up, please follow the below instructions to add the Flink Pravega connectors to the project.","title":"Creating a Flink Stream Processing Project"},{"location":"quickstart/#add-the-connector-dependencies","text":"To add the Pravega connector dependencies to your project, add the following entry to your project file: (For example, pom.xml for Maven) !-- Before Pravega 0.6 -- dependency groupId io.pravega /groupId artifactId pravega-connectors-flink_2.12 /artifactId version 0.5.1 /version /dependency !-- Pravega 0.6 -- dependency groupId io.pravega /groupId artifactId pravega-connectors-flink-1.9_2.12 /artifactId version 0.6.0 /version /dependency Use appropriate version as necessary. 1.9 is the Flink Major-Minor version. 2.12 is the Scala version. 0.6.0 is the Pravega version. The snapshot versions are published to jcenter repository and the release artifacts are available in Maven Central repository. Alternatively, we could build and publish the connector artifacts to local maven repository by executing the following command and make use of that version as your application dependency. ./gradlew clean install","title":"Add the Connector Dependencies"},{"location":"quickstart/#running-deploying-the-application","text":"From Flink's perspective, the connector to Pravega is part of the streaming application (not part of Flink's core runtime), so the connector code must be part of the application's code artifact (JAR file). Typically, a Flink application is bundled as a fat-jar (also known as an uber-jar ) , such that all its dependencies are embedded. The project set up should have been a success, if you have used the above linked templates/guides . If you set up a application's project and dependencies manually, you need to make sure that it builds a jar with dependencies , to include both the application and the connector classes.","title":"Running / Deploying the Application"},{"location":"serialization/","text":"Serialization Serialization refers to converting a data element in your Flink program to/from a message in a Pravega stream. Flink defines a standard interface for data serialization to/from byte messages delivered by various connectors. The core interfaces are: - org.apache.flink.streaming.util.serialization.SerializationSchema - org.apache.flink.streaming.util.serialization.DeserializationSchema Built-in serializers include: - org.apache.flink.streaming.util.serialization.SimpleStringSchema - org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema The Pravega connector is designed to use Flink's serialization interfaces. For example, to read each stream event as a UTF-8 string: DeserializationSchema String schema = new SimpleStringSchema (); FlinkPravegaReader String reader = new FlinkPravegaReader (..., schema ); DataStream MyEvent stream = env . addSource ( reader ); Interoperability with Other Applications A common scenario is using Flink to process Pravega stream data produced by a non-Flink application. The Pravega client library used by such applications defines the io.pravega.client.stream.Serializer interface for working with event data. The implementations of Serializer directly in a Flink program via built-in adapters can be used: - io.pravega.connectors.flink.serialization.PravegaSerializationSchema - io.pravega.connectors.flink.serialization.PravegaDeserializationSchema Below is an example, to pass an instance of the appropriate Pravega de/serializer class to the adapter's constructor: import io.pravega.client.stream.impl.JavaSerializer ; ... DeserializationSchema MyEvent adapter = new PravegaDeserializationSchema ( MyEvent . class , new JavaSerializer MyEvent ()); FlinkPravegaReader MyEvent reader = new FlinkPravegaReader (..., adapter ); DataStream MyEvent stream = env . addSource ( reader ); Note that the Pravega serializer must implement java.io.Serializable to be usable in a Flink program.","title":"Serialization"},{"location":"serialization/#serialization","text":"Serialization refers to converting a data element in your Flink program to/from a message in a Pravega stream. Flink defines a standard interface for data serialization to/from byte messages delivered by various connectors. The core interfaces are: - org.apache.flink.streaming.util.serialization.SerializationSchema - org.apache.flink.streaming.util.serialization.DeserializationSchema Built-in serializers include: - org.apache.flink.streaming.util.serialization.SimpleStringSchema - org.apache.flink.streaming.util.serialization.TypeInformationSerializationSchema The Pravega connector is designed to use Flink's serialization interfaces. For example, to read each stream event as a UTF-8 string: DeserializationSchema String schema = new SimpleStringSchema (); FlinkPravegaReader String reader = new FlinkPravegaReader (..., schema ); DataStream MyEvent stream = env . addSource ( reader );","title":"Serialization"},{"location":"serialization/#interoperability-with-other-applications","text":"A common scenario is using Flink to process Pravega stream data produced by a non-Flink application. The Pravega client library used by such applications defines the io.pravega.client.stream.Serializer interface for working with event data. The implementations of Serializer directly in a Flink program via built-in adapters can be used: - io.pravega.connectors.flink.serialization.PravegaSerializationSchema - io.pravega.connectors.flink.serialization.PravegaDeserializationSchema Below is an example, to pass an instance of the appropriate Pravega de/serializer class to the adapter's constructor: import io.pravega.client.stream.impl.JavaSerializer ; ... DeserializationSchema MyEvent adapter = new PravegaDeserializationSchema ( MyEvent . class , new JavaSerializer MyEvent ()); FlinkPravegaReader MyEvent reader = new FlinkPravegaReader (..., adapter ); DataStream MyEvent stream = env . addSource ( reader ); Note that the Pravega serializer must implement java.io.Serializable to be usable in a Flink program.","title":"Interoperability with Other Applications"},{"location":"streaming/","text":"Streaming Connector The Flink Connector library for Pravega provides a data source and data sink for use with the Flink Streaming API. See the below sections for details. Table of Contents FlinkPravegaReader Parameters Input Stream(s) Parallelism Checkpointing Timestamp Extraction / Watermark Emission Stream Cuts Historical Stream Processing FlinkPravegaWriter Parameters Parallelism Event Routing Event Time Ordering Watermark Writer Modes Metrics Data Serialization FlinkPravegaReader A Pravega Stream may be used as a data source within a Flink streaming program using an instance of io.pravega.connectors.flink.FlinkPravegaReader . The reader reads a given Pravega Stream (or multiple streams) as a DataStream (the basic abstraction of the Flink Streaming API). Open a Pravega Stream as a DataStream using the method StreamExecutionEnvironment::addSource . Example StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema MyClass deserializer = ... // Define the data stream FlinkPravegaReader MyClass pravegaSource = FlinkPravegaReader . MyClass builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream MyClass stream = env . addSource ( pravegaSource ); Parameters A builder API is provided to construct an instance of FlinkPravegaReader . See the table below for a summary of builder properties. Note that, the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. withReaderGroupScope The scope to store the Reader Group synchronization stream into. withReaderGroupName The Reader Group name for display purposes. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. withTimestampAssigner The AssignerWithTimeWindows implementation which describes the event timestamp and Pravega watermark strategy in event time semantics. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default. Input Stream(s) Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The FlinkPravegaReader is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") . Parallelism The FlinkPravegaReader supports parallelization. Use the setParallelism method to of Datastream to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note: Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The Synchronizer creates a backing stream that may be manually deleted after the completion of the job. Checkpointing In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution. The reader is compatible with Flink checkpoints and savepoints. The reader automatically recovers from failure by rewinding to the checkpointed position in the stream. A savepoint is self-contained; it contains all information needed to resume from the correct position. The checkpoint mechanism works as a two-step process: The master hook handler from the job manager initiates the triggerCheckpoint request to the ReaderCheckpointHook that was registered with the Job Manager during FlinkPravegaReader source initialization. The ReaderCheckpointHook handler notifies Pravega to checkpoint the current reader state. This is a non-blocking call which returns a future once Pravega readers are done with the checkpointing. A CheckPoint event will be sent by Pravega as part of the data stream flow and on receiving the event, the FlinkPravegaReader will initiate triggerCheckpoint request to effectively let Flink continue and complete the checkpoint process. Timestamp Extraction / Watermark Emission Flink requires the events\u2019 timestamps (each element in the stream needs to have its event timestamp assigned). This is achieved by accessing/extracting the timestamp from some field in the element. These are used to tell the system about progress in event time. Since Pravega 0.6, Pravega has proposed a new watermarking API to enable the writer to provide time information. On the reader side, a new concept TimeWindow is proposed to represent a time window for the events which are currently being read by a reader. It is possible to use event time semantics with either pravega watermark (after 0.6) or normal watermark. To use Pravega watermark, an interface called AssignerWithTimeWindows should be implemented in the application via an application-specific timestamp assigner and a watermark generator with TimeWindow . Different applications can choose to be more or less conservative with the given TimeWindow . LowerBoundAssigner is provided as a default implementation of the most conservative watermark. To use normal watermark, you can follow Flink documentation . Simply, specify an AssignerWithPeriodicWatermarks or AssignerWithPunctuatedWatermarks on the DataStream as normal. Each parallel instance of the source processes one or more stream segments in parallel. Each watermark generator instance will receive events multiplexed from numerous segments. Be aware that segments are processed in parallel, and that no effort is made to order the events across segments in terms of their event time. Also, a given segment may be reassigned to another parallel instance at any time, preserving exactly-once behavior but causing further spread in observed event times. StreamCuts A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The FlinkPravegaReader accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code . Historical Stream Processing Historical processing refers to processing stream data from a specific position in the stream rather than from the stream's tail. The builder API provides an overloaded method forStream that accepts a StreamCut parameter for this purpose. One such example is re-processing a stream, where we may have to process the data from the beginning (or from a certain point in the stream) to re-derive the output. For instance, in situations where the computation logic has been changed to address new additional criteria, or we fixed a bug or doing a typical A/B testing etc., where the ability to consume historical data as a stream is critical. FlinkPravegaWriter A Pravega Stream may be used as a data sink within a Flink program using an instance of io.pravega.connectors.flink.FlinkPravegaWriter . Add an instance of the writer to the dataflow program using the method DataStream::addSink . Example StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema MyClass serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter MyClass router = ... // Define the sink function FlinkPravegaWriter MyClass pravegaSink = FlinkPravegaWriter . MyClass builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . withWriterMode ( EXACTLY_ONCE ) . build (); DataStream MyClass stream = ... stream . addSink ( pravegaSink ); Parameters A builder API is provided to construct an instance of FlinkPravegaWriter . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort, _At-least-once , or Exactly-once guarantees. withTxnLeaseRenewalPeriod The Transaction lease renewal period that supports the Exactly-once writer mode. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. enableWatermark true or false to enable/disable emitting Flink watermark in event-time semantics to Pravega streams. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default. Parallelism FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute. Event Routing Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. When constructing the FlinkPravegaWriter , please provide an implementation of io.pravega.connectors.flink.PravegaEventRouter which will guarantee the event ordering. In Pravega, events are guaranteed to be ordered at the segment level. For example, to guarantee write order specific to sensor id, you could provide a router implementation like below. private static class SensorEventRouter SensorEvent implements PravegaEventRouter SensorEvent { @Override public String getRoutingKey(SensorEvent event) { return event.getId(); } } Event Time Ordering For programs that use Flink's event time semantics, the connector library supports writing events in event time order. In combination with a Routing Key, this establishes a well-understood ordering for each key in the output stream. Use the method FlinkPravegaUtils::writeToPravegaInEventTimeOrder to write a given DataStream to a Pravega Stream such that events are automatically ordered by event time (on a per-key basis). Refer here for sample code. Watermark Flink applications in event time semantics are carrying watermarks within each operator. Both Pravega transactional and non-transactional writers provide watermark API to indicate the event-time watermark for a stream. With enableWatermark(true) , each watermark in Flink will be emitted into a Pravega stream. Writer Modes Writer modes relate to guarantees about the persistence of events emitted by the sink to a Pravega Stream. The writer supports three writer modes: Best-effort - Any write failures will be ignored hence there could be data loss. At-least-once - All events are persisted in Pravega. Duplicate events are possible, due to retries or in case of failure and subsequent recovery. Exactly-once - All events are persisted in Pravega using a transactional approach integrated with the Flink checkpointing feature. By default, the At-least-once option is enabled and use .withWriterMode(...) option to override the value. See the Pravega documentation for details on transactional behavior. Metrics Metrics are reported by default unless it is explicitly disabled using enableMetrics(false) option. See Metrics page for more details on type of metrics that are reported. Serialization See the serialization page for more information on how to use the serializer and deserializer .","title":"Streaming"},{"location":"streaming/#streaming-connector","text":"The Flink Connector library for Pravega provides a data source and data sink for use with the Flink Streaming API. See the below sections for details.","title":"Streaming Connector"},{"location":"streaming/#table-of-contents","text":"FlinkPravegaReader Parameters Input Stream(s) Parallelism Checkpointing Timestamp Extraction / Watermark Emission Stream Cuts Historical Stream Processing FlinkPravegaWriter Parameters Parallelism Event Routing Event Time Ordering Watermark Writer Modes Metrics Data Serialization","title":"Table of Contents"},{"location":"streaming/#flinkpravegareader","text":"A Pravega Stream may be used as a data source within a Flink streaming program using an instance of io.pravega.connectors.flink.FlinkPravegaReader . The reader reads a given Pravega Stream (or multiple streams) as a DataStream (the basic abstraction of the Flink Streaming API). Open a Pravega Stream as a DataStream using the method StreamExecutionEnvironment::addSource .","title":"FlinkPravegaReader"},{"location":"streaming/#example","text":"StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event deserializer DeserializationSchema MyClass deserializer = ... // Define the data stream FlinkPravegaReader MyClass pravegaSource = FlinkPravegaReader . MyClass builder () . forStream (...) . withPravegaConfig ( config ) . withDeserializationSchema ( deserializer ) . build (); DataStream MyClass stream = env . addSource ( pravegaSource );","title":"Example"},{"location":"streaming/#parameters","text":"A builder API is provided to construct an instance of FlinkPravegaReader . See the table below for a summary of builder properties. Note that, the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. withReaderGroupScope The scope to store the Reader Group synchronization stream into. withReaderGroupName The Reader Group name for display purposes. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. withDeserializationSchema The deserialization schema which describes how to turn byte messages into events. withTimestampAssigner The AssignerWithTimeWindows implementation which describes the event timestamp and Pravega watermark strategy in event time semantics. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default.","title":"Parameters"},{"location":"streaming/#input-streams","text":"Each stream in Pravega is contained by a scope. A scope acts as a namespace for one or more streams. The FlinkPravegaReader is able to read from numerous streams in parallel, even across scopes. The builder API accepts both qualified and unqualified stream names. In qualified, the scope is explicitly specified, e.g. my-scope/my-stream . In Unqualified stream names are assumed to refer to the default scope as set in the PravegaConfig . A stream may be specified in one of three ways: As a string containing a qualified name, in the form scope/stream . As a string containing an unqualified name, in the form stream . Such streams are resolved to the default scope. As an instance of io.pravega.client.stream.Stream , e.g. Stream.of(\"my-scope\", \"my-stream\") .","title":"Input Stream(s)"},{"location":"streaming/#parallelism","text":"The FlinkPravegaReader supports parallelization. Use the setParallelism method to of Datastream to configure the number of parallel instances to execute. The parallel instances consume the stream in a coordinated manner, each consuming one or more stream segments. Note: Coordination is achieved with the use of a Pravega Reader Group, which is based on a State Synchronizer . The Synchronizer creates a backing stream that may be manually deleted after the completion of the job.","title":"Parallelism"},{"location":"streaming/#checkpointing","text":"In order to make state fault tolerant, Flink needs to checkpoint the state. Checkpoints allow Flink to recover state and positions in the streams to give the application the same semantics as a failure-free execution. The reader is compatible with Flink checkpoints and savepoints. The reader automatically recovers from failure by rewinding to the checkpointed position in the stream. A savepoint is self-contained; it contains all information needed to resume from the correct position. The checkpoint mechanism works as a two-step process: The master hook handler from the job manager initiates the triggerCheckpoint request to the ReaderCheckpointHook that was registered with the Job Manager during FlinkPravegaReader source initialization. The ReaderCheckpointHook handler notifies Pravega to checkpoint the current reader state. This is a non-blocking call which returns a future once Pravega readers are done with the checkpointing. A CheckPoint event will be sent by Pravega as part of the data stream flow and on receiving the event, the FlinkPravegaReader will initiate triggerCheckpoint request to effectively let Flink continue and complete the checkpoint process.","title":"Checkpointing"},{"location":"streaming/#timestamp-extraction-watermark-emission","text":"Flink requires the events\u2019 timestamps (each element in the stream needs to have its event timestamp assigned). This is achieved by accessing/extracting the timestamp from some field in the element. These are used to tell the system about progress in event time. Since Pravega 0.6, Pravega has proposed a new watermarking API to enable the writer to provide time information. On the reader side, a new concept TimeWindow is proposed to represent a time window for the events which are currently being read by a reader. It is possible to use event time semantics with either pravega watermark (after 0.6) or normal watermark. To use Pravega watermark, an interface called AssignerWithTimeWindows should be implemented in the application via an application-specific timestamp assigner and a watermark generator with TimeWindow . Different applications can choose to be more or less conservative with the given TimeWindow . LowerBoundAssigner is provided as a default implementation of the most conservative watermark. To use normal watermark, you can follow Flink documentation . Simply, specify an AssignerWithPeriodicWatermarks or AssignerWithPunctuatedWatermarks on the DataStream as normal. Each parallel instance of the source processes one or more stream segments in parallel. Each watermark generator instance will receive events multiplexed from numerous segments. Be aware that segments are processed in parallel, and that no effort is made to order the events across segments in terms of their event time. Also, a given segment may be reassigned to another parallel instance at any time, preserving exactly-once behavior but causing further spread in observed event times.","title":"Timestamp Extraction / Watermark Emission"},{"location":"streaming/#streamcuts","text":"A StreamCut represents a specific position in a Pravega Stream, which may be obtained from various API interactions with the Pravega client. The FlinkPravegaReader accepts a StreamCut as the start and/or end position of a given stream. For further reading on StreamCuts, please refer to documentation on StreamCut and sample code .","title":"StreamCuts"},{"location":"streaming/#historical-stream-processing","text":"Historical processing refers to processing stream data from a specific position in the stream rather than from the stream's tail. The builder API provides an overloaded method forStream that accepts a StreamCut parameter for this purpose. One such example is re-processing a stream, where we may have to process the data from the beginning (or from a certain point in the stream) to re-derive the output. For instance, in situations where the computation logic has been changed to address new additional criteria, or we fixed a bug or doing a typical A/B testing etc., where the ability to consume historical data as a stream is critical.","title":"Historical Stream Processing"},{"location":"streaming/#flinkpravegawriter","text":"A Pravega Stream may be used as a data sink within a Flink program using an instance of io.pravega.connectors.flink.FlinkPravegaWriter . Add an instance of the writer to the dataflow program using the method DataStream::addSink .","title":"FlinkPravegaWriter"},{"location":"streaming/#example_1","text":"StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); // Define the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); // Define the event serializer SerializationSchema MyClass serializer = ... // Define the event router for selecting the Routing Key PravegaEventRouter MyClass router = ... // Define the sink function FlinkPravegaWriter MyClass pravegaSink = FlinkPravegaWriter . MyClass builder () . forStream (...) . withPravegaConfig ( config ) . withSerializationSchema ( serializer ) . withEventRouter ( router ) . withWriterMode ( EXACTLY_ONCE ) . build (); DataStream MyClass stream = ... stream . addSink ( pravegaSink );","title":"Example"},{"location":"streaming/#parameters_1","text":"A builder API is provided to construct an instance of FlinkPravegaWriter . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort, _At-least-once , or Exactly-once guarantees. withTxnLeaseRenewalPeriod The Transaction lease renewal period that supports the Exactly-once writer mode. withSerializationSchema The serialization schema which describes how to turn events into byte messages. withEventRouter The router function which determines the Routing Key for a given event. enableWatermark true or false to enable/disable emitting Flink watermark in event-time semantics to Pravega streams. enableMetrics true or false to enable/disable reporting Pravega metrics. Metrics is enabled by default.","title":"Parameters"},{"location":"streaming/#parallelism_1","text":"FlinkPravegaWriter supports parallelization. Use the setParallelism method to configure the number of parallel instances to execute.","title":"Parallelism"},{"location":"streaming/#event-routing","text":"Every event written to a Pravega Stream has an associated Routing Key. The Routing Key is the basis for event ordering. See the Pravega Concepts for details. When constructing the FlinkPravegaWriter , please provide an implementation of io.pravega.connectors.flink.PravegaEventRouter which will guarantee the event ordering. In Pravega, events are guaranteed to be ordered at the segment level. For example, to guarantee write order specific to sensor id, you could provide a router implementation like below. private static class SensorEventRouter SensorEvent implements PravegaEventRouter SensorEvent { @Override public String getRoutingKey(SensorEvent event) { return event.getId(); } }","title":"Event Routing"},{"location":"streaming/#event-time-ordering","text":"For programs that use Flink's event time semantics, the connector library supports writing events in event time order. In combination with a Routing Key, this establishes a well-understood ordering for each key in the output stream. Use the method FlinkPravegaUtils::writeToPravegaInEventTimeOrder to write a given DataStream to a Pravega Stream such that events are automatically ordered by event time (on a per-key basis). Refer here for sample code.","title":"Event Time Ordering"},{"location":"streaming/#watermark","text":"Flink applications in event time semantics are carrying watermarks within each operator. Both Pravega transactional and non-transactional writers provide watermark API to indicate the event-time watermark for a stream. With enableWatermark(true) , each watermark in Flink will be emitted into a Pravega stream.","title":"Watermark"},{"location":"streaming/#writer-modes","text":"Writer modes relate to guarantees about the persistence of events emitted by the sink to a Pravega Stream. The writer supports three writer modes: Best-effort - Any write failures will be ignored hence there could be data loss. At-least-once - All events are persisted in Pravega. Duplicate events are possible, due to retries or in case of failure and subsequent recovery. Exactly-once - All events are persisted in Pravega using a transactional approach integrated with the Flink checkpointing feature. By default, the At-least-once option is enabled and use .withWriterMode(...) option to override the value. See the Pravega documentation for details on transactional behavior.","title":"Writer Modes"},{"location":"streaming/#metrics","text":"Metrics are reported by default unless it is explicitly disabled using enableMetrics(false) option. See Metrics page for more details on type of metrics that are reported.","title":"Metrics"},{"location":"streaming/#serialization","text":"See the serialization page for more information on how to use the serializer and deserializer .","title":"Serialization"},{"location":"table-api/","text":"Table Connector The Flink connector library for Pravega provides a table source and table sink for use with the Flink Table API. The Table API provides a unified API for both the Flink streaming and batch environment. See the below sections for details. FlinkPravegaJsonTableSource and FlinkPravegaJsonTableSink implementation has been deprecated and replaced with ConnectorDescriptor / TableFactory based implementation introduced in Flink 1.6. With these changes, it is possible to use the Pravega Table API either programmatically (using Pravega Descriptor) or declaratively through YAML configuration files for the SQL client. Table of Contents Table Source Parameters Custom Formats Time Attribute Support Pravega watermark (Evolving) ) Table Sink Parameters Custom Formats Using SQL Client Environment File Table Source A Pravega Stream may be used as a table source within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSource is then used to parse raw stream data as Row objects that conform to the table schema. Example The following example uses the provided table source to read JSON-formatted events from a Pravega Stream: // define table schema definition Schema schema = new Schema () . field ( user , Types . STRING ()) . field ( uri , Types . STRING ()) . field ( accessTime , Types . SQL_TIMESTAMP ()). rowtime ( new Rowtime (). timestampsFromField ( accessTime ) . watermarksPeriodicBounded ( 30000L )); // define pravega reader configurations using Pravega descriptor Pravega pravega = new Pravega (); pravega . tableSourceReaderBuilder () . withReaderGroupScope ( stream . getScope ()) . forStream ( stream ) . withPravegaConfig ( pravegaConfig ); // (Option-1) Streaming Source StreamExecutionEnvironment execEnvRead = StreamExecutionEnvironment . getExecutionEnvironment (); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( execEnvRead ); StreamTableDescriptor desc = tableEnv . connect ( pravega ) . withFormat ( new Json (). failOnMissingField ( true ). deriveSchema ()) . withSchema ( schema ) . inAppendMode (); final Map String , String propertiesMap = DescriptorProperties . toJavaMap ( desc ); final TableSource ? source = TableFactoryService . find ( StreamTableSourceFactory . class , propertiesMap ) . createStreamTableSource ( propertiesMap ); tableEnv . registerTableSource ( MyTableRow , source ); String sqlQuery = SELECT user, count(uri) from MyTableRow GROUP BY user ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... // (Option-2) Batch Source ExecutionEnvironment execEnvRead = ExecutionEnvironment . getExecutionEnvironment (); BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( execEnvRead ); execEnvRead . setParallelism ( 1 ); BatchTableDescriptor desc = tableEnv . connect ( pravega ) . withFormat ( new Json (). failOnMissingField ( true ). deriveSchema ()) . withSchema ( schema ); final Map String , String propertiesMap = DescriptorProperties . toJavaMap ( desc ); final TableSource ? source = TableFactoryService . find ( BatchTableSourceFactory . class , propertiesMap ) . createBatchTableSource ( propertiesMap ); tableEnv . registerTableSource ( MyTableRow , source ); String sqlQuery = SELECT ... ; Table result = tableEnv . sqlQuery ( sqlQuery ); DataSet Row resultSet = tableEnv . toDataSet ( result , Row . class ); ... @deprecated // Create a Flink Table environment ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); String [] fieldNames = { user , uri , accessTime }; // Read data from the stream using Table reader TableSchema tableSchema = TableSchema . builder () . field ( user , Types . STRING ()) . field ( uri , Types . STRING ()) . field ( accessTime , Types . SQL_TIMESTAMP ()) . build (); FlinkPravegaJsonTableSource source = FlinkPravegaJsonTableSource . builder () . forStream ( stream ) . withPravegaConfig ( pravegaConfig ) . failOnMissingField ( true ) . withRowtimeAttribute ( accessTime , new ExistingField ( accessTime ), new BoundedOutOfOrderTimestamps ( 30000L )) . withSchema ( tableSchema ) . withReaderGroupScope ( stream . getScope ()) . build (); // (Option-1) Read table as stream data StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( MyTableRow , source ); String sqlQuery = SELECT user, count(uri) from MyTableRow GROUP BY user ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... // (Option-2) Read table as batch data (use tumbling window as part of the query) BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( MyTableRow , source ); String sqlQuery = SELECT user, + TUMBLE_END(accessTime, INTERVAL 5 MINUTE) AS accessTime, + COUNT(uri) AS cnt + from MyTableRow GROUP BY + user, TUMBLE(accessTime, INTERVAL 5 MINUTE) ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... Parameters A builder API is provided to construct an concrete subclass of FlinkPravegaTableSource . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table source supports both the Flink streaming and batch environments . In the streaming environment, the table source uses a FlinkPravegaReader connector. In the batch environment, the table source uses a FlinkPravegaInputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. Applies only to streaming API. withReaderGroupScope The scope to store the Reader Group synchronization stream into. Applies only to streaming API. withReaderGroupName The Reader Group name for display purposes. Applies only to streaming API. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. Applies only to streaming API. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. Applies only to streaming API. withTimestampAssigner (Evolving) The AssignerWithTimeWindows implementation to implementation which describes the event timestamp and Pravega watermark strategy in event time semantics. Applies only to streaming API. The below configurations are applicable only for the deprecated FlinkPravegaJsonTableSource implementation. Method Description withSchema The table schema which describes which JSON fields to expect. withProctimeAttribute The name of the processing time attribute in the supplied table schema. withRowTimeAttribute supply the name of the rowtime attribute in the table schema, a TimeStampExtractor instance to extract the rowtime attribute value from the event and a WaterMarkStratergy to generate watermarks for the rowtime attribute. failOnMissingField A flag indicating whether to fail if a JSON field is missing. Custom Formats @deprecated and the steps outlined in this section is applicable only for FlinkPravegaJsonTableSource based implementation. Please use Pravega descriptor instead. To work with stream events in a format other than JSON, extend FlinkPravegaTableSource . Please see the implementation of FlinkPravegaJsonTableSource for more details. Time Attribute Support @deprecated and the steps outlined in this section is applicable only for FlinkPravegaJsonTableSource based implementation. Please use Pravega descriptor instead. With the use of withProctimeAttribute or withRowTimeAttribute builder method, one could supply the time attribute information of the event. The configured field must be present in the table schema and of type Types.SQL_TIMESTAMP() . Pravega watermark (Evolving) Pravega watermark for Table API Reader depends on the underlying DataStream settings. The following example shows how to read data with watermark by a table source. // A user-defined implementation of `AssignerWithTimeWindows`, the event type should be `Row` public static class MyAssigner extends LowerBoundAssigner Row { public MyAssigner () {} @Override public long extractTimestamp ( Row element , long previousElementTimestamp ) { // The third attribute of the element is the event timestamp return ( long ) element . getField ( 2 ); } } Pravega pravega = new Pravega (); pravega . tableSourceReaderBuilder () // Assign the watermark in the source . withTimestampAssigner ( new MyAssigner ()) . withReaderGroupScope ( stream . getScope ()) . forStream ( stream ) . withPravegaConfig ( pravegaConfig ); final ConnectTableDescriptor tableDesc = new TestTableDescriptor ( pravega ) . withFormat (...) . withSchema ( new Schema () . field (...) // Use the timestamp and Pravega watermark defined in the source . rowtime ( new Rowtime () . timestampsFromSource () . watermarksFromSource () )) . inAppendMode (); Table Sink A Pravega Stream may be used as an append-only table within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSink is then used to write table rows to a Pravega Stream in a particular format. Example The following example uses the provided table sink to write JSON-formatted events to a Pravega Stream: // (Option-1) Streaming Sink StreamExecutionEnvironment env = StreamExecutionEnvironment . createLocalEnvironment (). setParallelism ( 1 ); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); Table table = tableEnv . fromDataStream ( env . fromCollection ( Arrays . asList (...)); Pravega pravega = new Pravega (); pravega . tableSinkWriterBuilder () . withRoutingKeyField ( category ) . forStream ( stream ) . withPravegaConfig ( setupUtils . getPravegaConfig ()); StreamTableDescriptor desc = tableEnv . connect ( pravega ) . withFormat ( new Json (). failOnMissingField ( true ). deriveSchema ()) . withSchema ( new Schema (). field ( category , Types . STRING ()). field ( value , Types . INT ())) . inAppendMode (); desc . registerTableSink ( test ); final Map String , String propertiesMap = DescriptorProperties . toJavaMap ( desc ); final TableSink ? sink = TableFactoryService . find ( StreamTableSinkFactory . class , propertiesMap ) . createStreamTableSink ( propertiesMap ); table . writeToSink ( sink ); env . execute (); // (Option-2) Batch Sink ExecutionEnvironment env = ExecutionEnvironment . createLocalEnvironment (); BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); Table table = tableEnv . fromDataSet ( env . fromCollection ( Arrays . asList (...)); Pravega pravega = new Pravega (); pravega . tableSinkWriterBuilder () . withRoutingKeyField ( category ) . forStream ( stream ) . withPravegaConfig ( setupUtils . getPravegaConfig ()); BatchTableDescriptor desc = tableEnv . connect ( pravega ) . withFormat ( new Json (). failOnMissingField ( true ). deriveSchema ()) . withSchema ( new Schema (). field ( category , Types . STRING ()). field ( value , Types . INT ())); desc . registerTableSink ( test ); final Map String , String propertiesMap = DescriptorProperties . toJavaMap ( desc ); final TableSink ? sink = TableFactoryService . find ( BatchTableSinkFactory . class , propertiesMap ) . createBatchTableSink ( propertiesMap ); table . writeToSink ( sink ); env . execute (); @deprecated // Create a Flink Table environment StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( ParameterTool . fromArgs ( args )); // Define a table (see Flink documentation) Table table = ... // Write the table to a Pravega Stream FlinkPravegaJsonTableSink sink = FlinkPravegaJsonTableSink . builder () . forStream ( sensor_stream ) . withPravegaConfig ( config ) . withRoutingKeyField ( sensor_id ) . withWriterMode ( EXACTLY_ONCE ) . build (); table . writeToSink ( sink ); Parameters A builder API is provided to construct a concrete subclass of FlinkPravegaTableSink . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table sink supports both the Flink streaming and batch environments. In the streaming environment, the table sink uses a FlinkPravegaWriter connector. In the batch environment, the table sink uses a FlinkPravegaOutputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort , At-least-once , or Exactly-once guarantees. withTxnTimeout The timeout for the Pravega Tansaction that supports the exactly-once writer mode. withRoutingKeyField The table field to use as the Routing Key for written events. enableWatermark true or false to enable/disable the event-time watermark emitting into Pravega stream. The below configurations are applicable only for the deprecated FlinkPravegaJsonTableSink implementation. Method Description withSchema The table schema which describes which JSON fields to expect. Custom Formats @deprecated and the steps outlined in this section is applicable only for FlinkPravegaJsonTableSink based implementation. Please use Pravega descriptor instead. To work with stream events in a format other than JSON, extend FlinkPravegaTableSink . Please see the implementation of FlinkPravegaJsonTableSink for more details. Using SQL Client Flink Sql Client was introduced in Flink 1.6 which aims at providing an easy way of writing, debugging, and submitting table programs to a Flink cluster without a single line of Java or Scala code. The SQL Client CLI allows for retrieving and visualizing real-time results from the running distributed application on the command line. It is now possible to access Pravega streams using standard SQL commands through Flink's SQL client. To do so, the following files have to copied to Flink cluster library $FLINK_HOME/lib path - Pravega connector jar - Flink JSON jar (to serialize/deserialize data in json format) - Flink Avro jar (to serialize/deserialize data in avro format) Flink format jars can be downloaded from maven central repository . In a nutshell, here is what we need to do to use Flink SQL client with Pravega. 1. Download Flink binary version supported by the connector. 2. Make sure to copy flink-table .jar and flink-sql-client .jar from $FLINK-HOME/opt/ to $FLINK-HOME/lib/ location. 3. Copy Flink format jars (json, avro) from maven central to $FLINK-HOME/lib/ location. 4. Copy Flink Pravega connector jar file to $FLINK-HOME/lib/ location. 5. Prepare SQL client configuration file (that contains Pravega connector descriptor configurations). Make sure to create any Pravega streams that you will be accessing from SQL client shell ahead of time. 6. Run SQL client shell in embedded mode using the command $FLINK-HOME/bin/sql-client.sh embedded -d SQL_configuration_file 7. Run SELECT 'Hello World' from SQL client shell and make sure it does not throw any errors. It should show an empty results screen if there are no errors. 8. After these steps, you could run SQL commands from the SQL client shell prompt to interact with Pravega. For more details on how to setup, configure and access the SQL client shell, please follow the getting started documentation. Environment File The YAML configuration file schema for providing Pravega table API specific connector configuration is provided below. tables : - name : sample # name the new table type : source # declare if the table should be source , sink , or both . If both provide both reader and writer configurations update-mode : append # specify the update-mode *only* for streaming tables # declare the external system to connect to connector : type : pravega version : 1 metrics : # optional (true|false) connection-config : controller-uri : # mandatory default-scope : # optional (assuming reader or writer provides scope) security : # optional auth-type : # optional (base64 encoded string) auth-token : # optional (base64 encoded string) validate-hostname : # optional (true|false) trust-store : # optional (truststore filename) reader : # required only if type: source stream-info : - scope : test # optional (uses default-scope value or else throws error) stream : stream1 # mandatory start-streamcut : # optional (base64 encoded string) end-streamcut : # optional (base64 encoded string) - scope : test # repeating info to provide multiple stream configurations stream : stream2 start-streamcut : end-streamcut : reader-group : # optional uid : # optional scope : # optional (uses default-scope or else throws error) name : # optional refresh-interval : # optional (long milliseconds) event-read-timeout-interval : # optional (long milliseconds) checkpoint-initiate-timeout-interval : # optional (long milliseconds) writer : # required only if type: sink scope : foo # optional (uses default-scope value) stream : bar # mandatory mode : # optional (exactly_once | atleast_once) txn-lease-renewal-interval : # optional (long milliseconds) routingkey-field-name : # mandatory (provide field name from schema that has to be used as routing key) # declare a format for this system (refer flink documentation for details) format : # declare the schema of the table (refer flink documentation for details) schema : Sample Environment File Here is a sample environment file for reference which can be used as a source as well as sink to read from and write data into Pravega as table records tables : - name : sample type : both update-mode : append # declare the external system to connect to connector : type : pravega version : 1 metrics : true connection-config : controller-uri : tcp://localhost:9090 default-scope : wVamQsOSaCxvYiHQVhRl reader : stream-info : - stream : streamX writer : stream : streamX mode : atleast_once txn-lease-renewal-interval : 10000 routingkey-field-name : category format : type : json fail-on-missing-field : true derive-schema : true schema : - name : category type : VARCHAR - name : value type : INT functions : [] execution : # batch or streaming execution type : streaming # allow event-time or only processing-time in sources time-characteristic : event-time # interval in ms for emitting periodic watermarks periodic-watermarks-interval : 200 # changelog or table presentation of results result-mode : table # parallelism of the program parallelism : 1 # maximum parallelism max-parallelism : 128 # minimum idle state retention in ms min-idle-state-retention : 0 # maximum idle state retention in ms max-idle-state-retention : 0 deployment : # general cluster communication timeout in ms response-timeout : 5000 # (optional) address from cluster to gateway gateway-address : # (optional) port from cluster to gateway gateway-port : 0","title":"Table API"},{"location":"table-api/#table-connector","text":"The Flink connector library for Pravega provides a table source and table sink for use with the Flink Table API. The Table API provides a unified API for both the Flink streaming and batch environment. See the below sections for details. FlinkPravegaJsonTableSource and FlinkPravegaJsonTableSink implementation has been deprecated and replaced with ConnectorDescriptor / TableFactory based implementation introduced in Flink 1.6. With these changes, it is possible to use the Pravega Table API either programmatically (using Pravega Descriptor) or declaratively through YAML configuration files for the SQL client.","title":"Table Connector"},{"location":"table-api/#table-of-contents","text":"Table Source Parameters Custom Formats Time Attribute Support Pravega watermark (Evolving) ) Table Sink Parameters Custom Formats Using SQL Client Environment File","title":"Table of Contents"},{"location":"table-api/#table-source","text":"A Pravega Stream may be used as a table source within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSource is then used to parse raw stream data as Row objects that conform to the table schema.","title":"Table Source"},{"location":"table-api/#example","text":"The following example uses the provided table source to read JSON-formatted events from a Pravega Stream: // define table schema definition Schema schema = new Schema () . field ( user , Types . STRING ()) . field ( uri , Types . STRING ()) . field ( accessTime , Types . SQL_TIMESTAMP ()). rowtime ( new Rowtime (). timestampsFromField ( accessTime ) . watermarksPeriodicBounded ( 30000L )); // define pravega reader configurations using Pravega descriptor Pravega pravega = new Pravega (); pravega . tableSourceReaderBuilder () . withReaderGroupScope ( stream . getScope ()) . forStream ( stream ) . withPravegaConfig ( pravegaConfig ); // (Option-1) Streaming Source StreamExecutionEnvironment execEnvRead = StreamExecutionEnvironment . getExecutionEnvironment (); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( execEnvRead ); StreamTableDescriptor desc = tableEnv . connect ( pravega ) . withFormat ( new Json (). failOnMissingField ( true ). deriveSchema ()) . withSchema ( schema ) . inAppendMode (); final Map String , String propertiesMap = DescriptorProperties . toJavaMap ( desc ); final TableSource ? source = TableFactoryService . find ( StreamTableSourceFactory . class , propertiesMap ) . createStreamTableSource ( propertiesMap ); tableEnv . registerTableSource ( MyTableRow , source ); String sqlQuery = SELECT user, count(uri) from MyTableRow GROUP BY user ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... // (Option-2) Batch Source ExecutionEnvironment execEnvRead = ExecutionEnvironment . getExecutionEnvironment (); BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( execEnvRead ); execEnvRead . setParallelism ( 1 ); BatchTableDescriptor desc = tableEnv . connect ( pravega ) . withFormat ( new Json (). failOnMissingField ( true ). deriveSchema ()) . withSchema ( schema ); final Map String , String propertiesMap = DescriptorProperties . toJavaMap ( desc ); final TableSource ? source = TableFactoryService . find ( BatchTableSourceFactory . class , propertiesMap ) . createBatchTableSource ( propertiesMap ); tableEnv . registerTableSource ( MyTableRow , source ); String sqlQuery = SELECT ... ; Table result = tableEnv . sqlQuery ( sqlQuery ); DataSet Row resultSet = tableEnv . toDataSet ( result , Row . class ); ... @deprecated // Create a Flink Table environment ExecutionEnvironment env = ExecutionEnvironment . getExecutionEnvironment (); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( params ); String [] fieldNames = { user , uri , accessTime }; // Read data from the stream using Table reader TableSchema tableSchema = TableSchema . builder () . field ( user , Types . STRING ()) . field ( uri , Types . STRING ()) . field ( accessTime , Types . SQL_TIMESTAMP ()) . build (); FlinkPravegaJsonTableSource source = FlinkPravegaJsonTableSource . builder () . forStream ( stream ) . withPravegaConfig ( pravegaConfig ) . failOnMissingField ( true ) . withRowtimeAttribute ( accessTime , new ExistingField ( accessTime ), new BoundedOutOfOrderTimestamps ( 30000L )) . withSchema ( tableSchema ) . withReaderGroupScope ( stream . getScope ()) . build (); // (Option-1) Read table as stream data StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( MyTableRow , source ); String sqlQuery = SELECT user, count(uri) from MyTableRow GROUP BY user ; Table result = tableEnv . sqlQuery ( sqlQuery ); ... // (Option-2) Read table as batch data (use tumbling window as part of the query) BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); tableEnv . registerTableSource ( MyTableRow , source ); String sqlQuery = SELECT user, + TUMBLE_END(accessTime, INTERVAL 5 MINUTE) AS accessTime, + COUNT(uri) AS cnt + from MyTableRow GROUP BY + user, TUMBLE(accessTime, INTERVAL 5 MINUTE) ; Table result = tableEnv . sqlQuery ( sqlQuery ); ...","title":"Example"},{"location":"table-api/#parameters","text":"A builder API is provided to construct an concrete subclass of FlinkPravegaTableSource . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table source supports both the Flink streaming and batch environments . In the streaming environment, the table source uses a FlinkPravegaReader connector. In the batch environment, the table source uses a FlinkPravegaInputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be read from, with optional start and/or end position. May be called repeatedly to read numerous streams in parallel. uid The uid to identify the checkpoint state of this source. Applies only to streaming API. withReaderGroupScope The scope to store the Reader Group synchronization stream into. Applies only to streaming API. withReaderGroupName The Reader Group name for display purposes. Applies only to streaming API. withReaderGroupRefreshTime The interval for synchronizing the Reader Group state across parallel source instances. Applies only to streaming API. withCheckpointInitiateTimeout The timeout for executing a checkpoint of the Reader Group state. Applies only to streaming API. withTimestampAssigner (Evolving) The AssignerWithTimeWindows implementation to implementation which describes the event timestamp and Pravega watermark strategy in event time semantics. Applies only to streaming API. The below configurations are applicable only for the deprecated FlinkPravegaJsonTableSource implementation. Method Description withSchema The table schema which describes which JSON fields to expect. withProctimeAttribute The name of the processing time attribute in the supplied table schema. withRowTimeAttribute supply the name of the rowtime attribute in the table schema, a TimeStampExtractor instance to extract the rowtime attribute value from the event and a WaterMarkStratergy to generate watermarks for the rowtime attribute. failOnMissingField A flag indicating whether to fail if a JSON field is missing.","title":"Parameters"},{"location":"table-api/#custom-formats","text":"@deprecated and the steps outlined in this section is applicable only for FlinkPravegaJsonTableSource based implementation. Please use Pravega descriptor instead. To work with stream events in a format other than JSON, extend FlinkPravegaTableSource . Please see the implementation of FlinkPravegaJsonTableSource for more details.","title":"Custom Formats"},{"location":"table-api/#time-attribute-support","text":"@deprecated and the steps outlined in this section is applicable only for FlinkPravegaJsonTableSource based implementation. Please use Pravega descriptor instead. With the use of withProctimeAttribute or withRowTimeAttribute builder method, one could supply the time attribute information of the event. The configured field must be present in the table schema and of type Types.SQL_TIMESTAMP() .","title":"Time Attribute Support"},{"location":"table-api/#pravega-watermark-evolving","text":"Pravega watermark for Table API Reader depends on the underlying DataStream settings. The following example shows how to read data with watermark by a table source. // A user-defined implementation of `AssignerWithTimeWindows`, the event type should be `Row` public static class MyAssigner extends LowerBoundAssigner Row { public MyAssigner () {} @Override public long extractTimestamp ( Row element , long previousElementTimestamp ) { // The third attribute of the element is the event timestamp return ( long ) element . getField ( 2 ); } } Pravega pravega = new Pravega (); pravega . tableSourceReaderBuilder () // Assign the watermark in the source . withTimestampAssigner ( new MyAssigner ()) . withReaderGroupScope ( stream . getScope ()) . forStream ( stream ) . withPravegaConfig ( pravegaConfig ); final ConnectTableDescriptor tableDesc = new TestTableDescriptor ( pravega ) . withFormat (...) . withSchema ( new Schema () . field (...) // Use the timestamp and Pravega watermark defined in the source . rowtime ( new Rowtime () . timestampsFromSource () . watermarksFromSource () )) . inAppendMode ();","title":"Pravega watermark (Evolving)"},{"location":"table-api/#table-sink","text":"A Pravega Stream may be used as an append-only table within a Flink table program. The Flink Table API is oriented around Flink's TableSchema classes which describe the table fields. A concrete subclass of FlinkPravegaTableSink is then used to write table rows to a Pravega Stream in a particular format.","title":"Table Sink"},{"location":"table-api/#example_1","text":"The following example uses the provided table sink to write JSON-formatted events to a Pravega Stream: // (Option-1) Streaming Sink StreamExecutionEnvironment env = StreamExecutionEnvironment . createLocalEnvironment (). setParallelism ( 1 ); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); Table table = tableEnv . fromDataStream ( env . fromCollection ( Arrays . asList (...)); Pravega pravega = new Pravega (); pravega . tableSinkWriterBuilder () . withRoutingKeyField ( category ) . forStream ( stream ) . withPravegaConfig ( setupUtils . getPravegaConfig ()); StreamTableDescriptor desc = tableEnv . connect ( pravega ) . withFormat ( new Json (). failOnMissingField ( true ). deriveSchema ()) . withSchema ( new Schema (). field ( category , Types . STRING ()). field ( value , Types . INT ())) . inAppendMode (); desc . registerTableSink ( test ); final Map String , String propertiesMap = DescriptorProperties . toJavaMap ( desc ); final TableSink ? sink = TableFactoryService . find ( StreamTableSinkFactory . class , propertiesMap ) . createStreamTableSink ( propertiesMap ); table . writeToSink ( sink ); env . execute (); // (Option-2) Batch Sink ExecutionEnvironment env = ExecutionEnvironment . createLocalEnvironment (); BatchTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); Table table = tableEnv . fromDataSet ( env . fromCollection ( Arrays . asList (...)); Pravega pravega = new Pravega (); pravega . tableSinkWriterBuilder () . withRoutingKeyField ( category ) . forStream ( stream ) . withPravegaConfig ( setupUtils . getPravegaConfig ()); BatchTableDescriptor desc = tableEnv . connect ( pravega ) . withFormat ( new Json (). failOnMissingField ( true ). deriveSchema ()) . withSchema ( new Schema (). field ( category , Types . STRING ()). field ( value , Types . INT ())); desc . registerTableSink ( test ); final Map String , String propertiesMap = DescriptorProperties . toJavaMap ( desc ); final TableSink ? sink = TableFactoryService . find ( BatchTableSinkFactory . class , propertiesMap ) . createBatchTableSink ( propertiesMap ); table . writeToSink ( sink ); env . execute (); @deprecated // Create a Flink Table environment StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); StreamTableEnvironment tableEnv = TableEnvironment . getTableEnvironment ( env ); // Load the Pravega configuration PravegaConfig config = PravegaConfig . fromParams ( ParameterTool . fromArgs ( args )); // Define a table (see Flink documentation) Table table = ... // Write the table to a Pravega Stream FlinkPravegaJsonTableSink sink = FlinkPravegaJsonTableSink . builder () . forStream ( sensor_stream ) . withPravegaConfig ( config ) . withRoutingKeyField ( sensor_id ) . withWriterMode ( EXACTLY_ONCE ) . build (); table . writeToSink ( sink );","title":"Example"},{"location":"table-api/#parameters_1","text":"A builder API is provided to construct a concrete subclass of FlinkPravegaTableSink . See the table below for a summary of builder properties. Note that the builder accepts an instance of PravegaConfig for common configuration properties. See the configurations page for more information. Note that the table sink supports both the Flink streaming and batch environments. In the streaming environment, the table sink uses a FlinkPravegaWriter connector. In the batch environment, the table sink uses a FlinkPravegaOutputFormat connector. Please see the documentation of Streaming Connector and Batch Connector to have a better understanding on the below mentioned parameter list. Method Description withPravegaConfig The Pravega client configuration, which includes connection info, security info, and a default scope. forStream The stream to be written to. withWriterMode The writer mode to provide Best-effort , At-least-once , or Exactly-once guarantees. withTxnTimeout The timeout for the Pravega Tansaction that supports the exactly-once writer mode. withRoutingKeyField The table field to use as the Routing Key for written events. enableWatermark true or false to enable/disable the event-time watermark emitting into Pravega stream. The below configurations are applicable only for the deprecated FlinkPravegaJsonTableSink implementation. Method Description withSchema The table schema which describes which JSON fields to expect.","title":"Parameters"},{"location":"table-api/#custom-formats_1","text":"@deprecated and the steps outlined in this section is applicable only for FlinkPravegaJsonTableSink based implementation. Please use Pravega descriptor instead. To work with stream events in a format other than JSON, extend FlinkPravegaTableSink . Please see the implementation of FlinkPravegaJsonTableSink for more details.","title":"Custom Formats"},{"location":"table-api/#using-sql-client","text":"Flink Sql Client was introduced in Flink 1.6 which aims at providing an easy way of writing, debugging, and submitting table programs to a Flink cluster without a single line of Java or Scala code. The SQL Client CLI allows for retrieving and visualizing real-time results from the running distributed application on the command line. It is now possible to access Pravega streams using standard SQL commands through Flink's SQL client. To do so, the following files have to copied to Flink cluster library $FLINK_HOME/lib path - Pravega connector jar - Flink JSON jar (to serialize/deserialize data in json format) - Flink Avro jar (to serialize/deserialize data in avro format) Flink format jars can be downloaded from maven central repository . In a nutshell, here is what we need to do to use Flink SQL client with Pravega. 1. Download Flink binary version supported by the connector. 2. Make sure to copy flink-table .jar and flink-sql-client .jar from $FLINK-HOME/opt/ to $FLINK-HOME/lib/ location. 3. Copy Flink format jars (json, avro) from maven central to $FLINK-HOME/lib/ location. 4. Copy Flink Pravega connector jar file to $FLINK-HOME/lib/ location. 5. Prepare SQL client configuration file (that contains Pravega connector descriptor configurations). Make sure to create any Pravega streams that you will be accessing from SQL client shell ahead of time. 6. Run SQL client shell in embedded mode using the command $FLINK-HOME/bin/sql-client.sh embedded -d SQL_configuration_file 7. Run SELECT 'Hello World' from SQL client shell and make sure it does not throw any errors. It should show an empty results screen if there are no errors. 8. After these steps, you could run SQL commands from the SQL client shell prompt to interact with Pravega. For more details on how to setup, configure and access the SQL client shell, please follow the getting started documentation.","title":"Using SQL Client"},{"location":"table-api/#environment-file","text":"The YAML configuration file schema for providing Pravega table API specific connector configuration is provided below. tables : - name : sample # name the new table type : source # declare if the table should be source , sink , or both . If both provide both reader and writer configurations update-mode : append # specify the update-mode *only* for streaming tables # declare the external system to connect to connector : type : pravega version : 1 metrics : # optional (true|false) connection-config : controller-uri : # mandatory default-scope : # optional (assuming reader or writer provides scope) security : # optional auth-type : # optional (base64 encoded string) auth-token : # optional (base64 encoded string) validate-hostname : # optional (true|false) trust-store : # optional (truststore filename) reader : # required only if type: source stream-info : - scope : test # optional (uses default-scope value or else throws error) stream : stream1 # mandatory start-streamcut : # optional (base64 encoded string) end-streamcut : # optional (base64 encoded string) - scope : test # repeating info to provide multiple stream configurations stream : stream2 start-streamcut : end-streamcut : reader-group : # optional uid : # optional scope : # optional (uses default-scope or else throws error) name : # optional refresh-interval : # optional (long milliseconds) event-read-timeout-interval : # optional (long milliseconds) checkpoint-initiate-timeout-interval : # optional (long milliseconds) writer : # required only if type: sink scope : foo # optional (uses default-scope value) stream : bar # mandatory mode : # optional (exactly_once | atleast_once) txn-lease-renewal-interval : # optional (long milliseconds) routingkey-field-name : # mandatory (provide field name from schema that has to be used as routing key) # declare a format for this system (refer flink documentation for details) format : # declare the schema of the table (refer flink documentation for details) schema :","title":"Environment File"},{"location":"table-api/#sample-environment-file","text":"Here is a sample environment file for reference which can be used as a source as well as sink to read from and write data into Pravega as table records tables : - name : sample type : both update-mode : append # declare the external system to connect to connector : type : pravega version : 1 metrics : true connection-config : controller-uri : tcp://localhost:9090 default-scope : wVamQsOSaCxvYiHQVhRl reader : stream-info : - stream : streamX writer : stream : streamX mode : atleast_once txn-lease-renewal-interval : 10000 routingkey-field-name : category format : type : json fail-on-missing-field : true derive-schema : true schema : - name : category type : VARCHAR - name : value type : INT functions : [] execution : # batch or streaming execution type : streaming # allow event-time or only processing-time in sources time-characteristic : event-time # interval in ms for emitting periodic watermarks periodic-watermarks-interval : 200 # changelog or table presentation of results result-mode : table # parallelism of the program parallelism : 1 # maximum parallelism max-parallelism : 128 # minimum idle state retention in ms min-idle-state-retention : 0 # maximum idle state retention in ms max-idle-state-retention : 0 deployment : # general cluster communication timeout in ms response-timeout : 5000 # (optional) address from cluster to gateway gateway-address : # (optional) port from cluster to gateway gateway-port : 0","title":"Sample Environment File"}]}